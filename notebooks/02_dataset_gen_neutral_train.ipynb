{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b6131e",
   "metadata": {},
   "source": [
    "# Neutral Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b752a224",
   "metadata": {},
   "source": [
    "## Load Style Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7e370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muice Responses: 3402\n",
      "Ayaka Responses: 1416\n",
      "Zhongli Responses: 514\n",
      "Hutao Responses: 845\n",
      "Haruhi Responses: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "MUICE_PATH = Path(\"../Dataset/Muice/train.jsonl\")\n",
    "HARUHI_PATH = Path(r\"..\\Dataset\\Haruhi\\Haruhi_clean.jsonl\")\n",
    "\n",
    "# Check if Dataset exist\n",
    "assert MUICE_PATH.is_file(), \"请确保 Muice Dataset 的路径正确！\"\n",
    "assert HARUHI_PATH.is_file(), \"请确保 Haruhi Dataset 的路径正确！\"\n",
    "\n",
    "# Load Muice Dataset\n",
    "dataset_lines = MUICE_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "muice_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "    muice_responses.append(item[\"Response\"])\n",
    "\n",
    "# Load Haruhi Dataset\n",
    "dataset_lines = HARUHI_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "ayaka_responses: List[str] = []\n",
    "zhongli_responses: List[str] = []\n",
    "hutao_responses: List[str] = []\n",
    "haruhi_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "\n",
    "    # 只提取单一角色的训练集\n",
    "    if item[\"agent_role\"] == \"神里绫华\":\n",
    "        ayaka_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"钟离\":\n",
    "        zhongli_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"胡桃\":\n",
    "        hutao_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role_name_en\"] == \"haruhi\":\n",
    "        haruhi_responses.append(item[\"agent_response\"])\n",
    "\n",
    "# 输出所有训练集的长度\n",
    "\n",
    "print(f\"Muice Responses: {len(muice_responses)}\")\n",
    "print(f\"Ayaka Responses: {len(ayaka_responses)}\")\n",
    "print(f\"Zhongli Responses: {len(zhongli_responses)}\")\n",
    "print(f\"Hutao Responses: {len(hutao_responses)}\")\n",
    "print(f\"Haruhi Responses: {len(haruhi_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189fdb9",
   "metadata": {},
   "source": [
    "## Initialize LLM Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d2232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, Sequence, Tuple, Iterable\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletionMessageParam as ChatMsgParam\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # 从 .env 文件加载环境变量\n",
    "\n",
    "# === 配置项 ===\n",
    "DEFAULT_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"sk-PLACEHOLDER\")\n",
    "DEFAULT_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"qwen3-max-2025-09-23\")\n",
    "DEFAULT_SYSTEM_PROMPT = \"你是一个语言风格分析专家，你需要根据通过一个带有特定角色风格的句子，改写为语义相同但风格中性的版本。要求不使用任何语气词、感叹词或修辞；不体现说话者的性格、情绪或身份；使用普通的书面语或口语表达\"\n",
    "\n",
    "ConversationTurn = Tuple[str, Optional[str]]\n",
    "\"\"\"表示一次对话轮次：(user_message, assistant_reply)。assistant_reply 可为 None。\"\"\"\n",
    "\n",
    "\n",
    "def _build_messages(\n",
    "    prompt: str,\n",
    "    history: Optional[Sequence[ConversationTurn]] = None,\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> list[ChatMsgParam]:\n",
    "    messages: list[ChatMsgParam] = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"极简 LLM 封装：初始化固定模型，提供 ask() 返回字符串。\"\"\"\n",
    "\n",
    "    model: str = DEFAULT_MODEL\n",
    "    api_key: str = DEFAULT_API_KEY\n",
    "    base_url: Optional[str] = DEFAULT_BASE_URL\n",
    "    system_prompt: Optional[str] = DEFAULT_SYSTEM_PROMPT\n",
    "    extra_headers: Optional[Dict[str, str]] = None\n",
    "    _client: OpenAI = field(init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self._client = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            default_headers=self.extra_headers if self.extra_headers else None,\n",
    "        )\n",
    "\n",
    "    def _consume_stream(self, stream_resp: Iterable[Any]) -> str:\n",
    "        \"\"\"消费流式响应，拼接内容。\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        for chunk in stream_resp:\n",
    "            choices = getattr(chunk, \"choices\", None)\n",
    "            if not choices:\n",
    "                continue\n",
    "            delta = getattr(choices[0], \"delta\", None)\n",
    "            if delta and getattr(delta, \"content\", None):\n",
    "                chunks.append(delta.content)\n",
    "        return \"\".join(chunks)\n",
    "\n",
    "    def ask(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        history: Optional[Sequence[ConversationTurn]] = None,\n",
    "        *,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"生成回复。\n",
    "\n",
    "        参数：\n",
    "            prompt: 当前用户输入。\n",
    "            history: 可选的历史 [(user, assistant), ...]，assistant 允许为 None。\n",
    "            temperature/max_tokens/stream/kwargs：直接透传给 OpenAI Chat Completion。\n",
    "        返回：\n",
    "            模型回复的纯文本（若响应为空则返回空字符串）。\n",
    "        \"\"\"\n",
    "        messages = _build_messages(\n",
    "            prompt=prompt,\n",
    "            history=history,\n",
    "            system_prompt=self.system_prompt,\n",
    "        )\n",
    "\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            stream=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        if hasattr(choice, \"message\") and getattr(choice.message, \"content\", None):\n",
    "            return choice.message.content  # type: ignore[return-value]\n",
    "        # 兼容历史版本/异常情况\n",
    "        return getattr(choice, \"text\", \"\")\n",
    "\n",
    "\n",
    "# 初始化一个通用实例，供 Notebook 其他单元直接调用\n",
    "llm = SimpleLLMClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacfeb41",
   "metadata": {},
   "source": [
    "## Call LLM to Generate Neutral Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d4715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from typing import Dict, Optional, Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEFAULT_TEMPERATURE = float(os.getenv(\"NEUTRAL_TEMPERATURE\", \"0.2\"))\n",
    "DEFAULT_SLEEP_SECONDS = float(os.getenv(\"NEUTRAL_SLEEP_SECONDS\", \"0.3\"))\n",
    "\n",
    "SHOTS_EXAMPLE = [(\"风格句: 我今晚吃了楠符电池呢！你要不要也来一块？（递上）\\n中性句: \", \"我今天吃了电池。\"), (\"风格句: 我才不是什么地雷女呢，只是长得像而已...\\n中性句: \", \"我不是地雷女。\")]\n",
    "\n",
    "\n",
    "def build_neutral_prompt(stylized_text: str) -> str:\n",
    "    \"\"\"根据带风格的原句构造改写提示。\"\"\"\n",
    "    return (\n",
    "        f\"风格句: {stylized_text}\\n\"\n",
    "        \"中性句: \"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_neutral_corpus_with_CoT(\n",
    "    stylized_texts: Sequence[str],\n",
    "    character: Optional[str] = None,\n",
    "    *,\n",
    "    limit: Optional[int] = None,\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    sleep_seconds: float = DEFAULT_SLEEP_SECONDS,\n",
    ") -> list[Dict[str, str]]:\n",
    "    \"\"\"调用 llm.ask 批量生成中性句。\"\"\"\n",
    "    records: list[Dict[str, str]] = []\n",
    "\n",
    "    for idx, text in enumerate(tqdm(stylized_texts, desc=f\"Generating {character} neutral sentences\", total=limit or len(stylized_texts))):\n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "\n",
    "        neutral_sentence = llm.ask(prompt=build_neutral_prompt(text), history=None, temperature=temperature).strip()\n",
    "        records.append({\"original\": text, \"neutral\": neutral_sentence})\n",
    "        \n",
    "        sleep(sleep_seconds)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c5527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating None neutral sentences:   0%|          | 0/3402 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m muice_neutral_records = \u001b[43mgenerate_neutral_corpus_with_CoT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmuice_responses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m ayaka_neutral_records = generate_neutral_corpus_with_CoT(ayaka_responses)\n\u001b[32m      3\u001b[39m zhongli_neutral_records = generate_neutral_corpus_with_CoT(zhongli_responses)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mgenerate_neutral_corpus_with_CoT\u001b[39m\u001b[34m(stylized_texts, character, limit, temperature, sleep_seconds)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx >= limit:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m neutral_sentence = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_neutral_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     36\u001b[39m records.append({\u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m: text, \u001b[33m\"\u001b[39m\u001b[33mneutral\u001b[39m\u001b[33m\"\u001b[39m: neutral_sentence})\n\u001b[32m     38\u001b[39m sleep(sleep_seconds)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mSimpleLLMClient.ask\u001b[39m\u001b[34m(self, prompt, history, temperature, max_tokens, stream, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"生成回复。\u001b[39;00m\n\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m \u001b[33;03m参数：\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m    模型回复的纯文本（若响应为空则返回空字符串）。\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     90\u001b[39m messages = _build_messages(\n\u001b[32m     91\u001b[39m     prompt=prompt,\n\u001b[32m     92\u001b[39m     history=history,\n\u001b[32m     93\u001b[39m     system_prompt=\u001b[38;5;28mself\u001b[39m.system_prompt,\n\u001b[32m     94\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_stream(response)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\http_proxy.py:343\u001b[39m, in \u001b[36mTunnelHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mself\u001b[39m._connection = HTTP11Connection(\n\u001b[32m    337\u001b[39m                 origin=\u001b[38;5;28mself\u001b[39m._remote_origin,\n\u001b[32m    338\u001b[39m                 stream=stream,\n\u001b[32m    339\u001b[39m                 keepalive_expiry=\u001b[38;5;28mself\u001b[39m._keepalive_expiry,\n\u001b[32m    340\u001b[39m             )\n\u001b[32m    342\u001b[39m         \u001b[38;5;28mself\u001b[39m._connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\ssl.py:1234\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1231\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1232\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1233\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Muika\\.conda\\envs\\Paper\\Lib\\ssl.py:1107\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "muice_neutral_records = generate_neutral_corpus_with_CoT(muice_responses)\n",
    "ayaka_neutral_records = generate_neutral_corpus_with_CoT(ayaka_responses)\n",
    "zhongli_neutral_records = generate_neutral_corpus_with_CoT(zhongli_responses)\n",
    "hutao_neutral_records = generate_neutral_corpus_with_CoT(hutao_responses)\n",
    "haruhi_neutral_records = generate_neutral_corpus_with_CoT(haruhi_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b9264",
   "metadata": {},
   "source": [
    "## Export Neutral Sentence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs = []\n",
    "\n",
    "def save_records(character: str, records: list[Dict[str, str]]):\n",
    "    for record in records:\n",
    "        final_outputs.append({\n",
    "            \"character\": character,\n",
    "            \"original\": record[\"original\"],\n",
    "            \"neutral\": record[\"neutral\"]\n",
    "        })\n",
    "\n",
    "save_records(\"Muice\", muice_neutral_records)\n",
    "save_records(\"Ayaka\", ayaka_neutral_records)\n",
    "save_records(\"Zhongli\", zhongli_neutral_records)\n",
    "save_records(\"Haruhi\", haruhi_neutral_records)\n",
    "save_records(\"Hutao\", hutao_neutral_records)\n",
    "\n",
    "# 输出到 JSONL 文件\n",
    "output_path = Path(\"./data/neutral_sentences.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with output_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    for item in final_outputs:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95340134",
   "metadata": {},
   "source": [
    "# CoT Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb76afe",
   "metadata": {},
   "source": [
    "## Load Style Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafd0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6596 neutral sentence items.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing_extensions import TypedDict\n",
    "import json\n",
    "\n",
    "class NeutralSentenceItem(TypedDict):\n",
    "    character: str\n",
    "    original: str\n",
    "    neutral: str\n",
    "\n",
    "\n",
    "neutral_path = Path(\"./data/neutral_sentences.jsonl\")\n",
    "neutral_items: list[NeutralSentenceItem] = []\n",
    "\n",
    "with neutral_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        neutral_items.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(neutral_items)} neutral sentence items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad53ee2",
   "metadata": {},
   "source": [
    "## Load Style Library Style Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ced5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# PMI\n",
    "def get_lexical_keywords_from_file(file: Path, top_n: int = 25) -> list[str]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data: dict[str, float] = json.loads(f.read())\n",
    "    return list(data.keys())[:top_n]\n",
    "\n",
    "Lexical_Muice = get_lexical_keywords_from_file(Path(\"./outputs/pmi/muice_pmi_filtered.json\"))\n",
    "Lexical_Ayaka = get_lexical_keywords_from_file(Path(\"./outputs/pmi/ayaka_pmi_filtered.json\"))\n",
    "Lexical_Zhongli = get_lexical_keywords_from_file(Path(\"./outputs/pmi/zhongli_pmi_filtered.json\"))\n",
    "Lexical_Hutao = get_lexical_keywords_from_file(Path(\"./outputs/pmi/hutao_pmi_filtered.json\"))\n",
    "Lexical_Haruhi = get_lexical_keywords_from_file(Path(\"./outputs/pmi/haruhi_pmi_filtered.json\"))\n",
    "\n",
    "Lexical_All = {\"Muice\": Lexical_Muice, \"Ayaka\": Lexical_Ayaka, \"Zhongli\": Lexical_Zhongli, \"Hutao\": Lexical_Hutao, \"Haruhi\": Lexical_Haruhi}\n",
    "\n",
    "# PCFG\n",
    "Syntactic_Muice = {'declarativity': 0.1103257643217572, 'parallelism': 0.02918150786583556, 'ellipsis_or_fragmentation': 0.08529979222321163, 'subordination': 0.19688705847432472, 'interjectionality': 0.008644998515880083, 'clausal_embedding': 0.034784060552092606, 'referentiality': 0.11624369249035323, 'syntactic_compression': 0.18596022558622738, 'nominal_complexity': 0.03342980112793114, 'coordination_density': 0.002615761353517364, 'quantificationality': 0.038197536360937964, 'modifier_density': 0.1393217571979816, 'prepositional_density': 0.01910804392994954}\n",
    "Syntactic_Ayaka = {'declarativity': 0.09320164543629895, 'parallelism': 0.029192583613203236, 'ellipsis_or_fragmentation': 0.061485264601259915, 'subordination': 0.18419745235587529, 'clausal_embedding': 0.046163629498618866, 'interjectionality': 0.002046859164166054, 'syntactic_compression': 0.1999165358399078, 'nominal_complexity': 0.05623894596689255, 'referentiality': 0.10019673694878878, 'coordination_density': 0.02416486158860118, 'quantificationality': 0.042964170028417556, 'modifier_density': 0.13753701238051708, 'prepositional_density': 0.022694302577452752}\n",
    "Syntactic_Zhongli = {'declarativity': 0.09879656160458453, 'parallelism': 0.029398280802292263, 'ellipsis_or_fragmentation': 0.06412607449856733, 'subordination': 0.1839541547277937, 'clausal_embedding': 0.037478510028653295, 'interjectionality': 0.003151862464183381, 'syntactic_compression': 0.21077363896848136, 'quantificationality': 0.04022922636103152, 'referentiality': 0.08240687679083095, 'nominal_complexity': 0.06372492836676218, 'coordination_density': 0.02332378223495702, 'modifier_density': 0.14068767908309457, 'prepositional_density': 0.02194842406876791}\n",
    "Syntactic_Hutao = {'parallelism': 0.03191357258164659, 'declarativity': 0.10471252949211474, 'ellipsis_or_fragmentation': 0.07646218800447038, 'clausal_embedding': 0.042685955544517575, 'subordination': 0.18254066807400968, 'interjectionality': 0.01809884515087545, 'syntactic_compression': 0.1987458090152738, 'referentiality': 0.09049422575437725, 'quantificationality': 0.05016763938904756, 'nominal_complexity': 0.034459207748665094, 'coordination_density': 0.013845771762076246, 'modifier_density': 0.1413448404321371, 'prepositional_density': 0.014528747050788526}\n",
    "Syntactic_Haruhi = {'declarativity': 0.0939982347749338, 'parallelism': 0.032288908502500734, 'subordination': 0.19174757281553398, 'ellipsis_or_fragmentation': 0.07542659605766402, 'clausal_embedding': 0.042144748455428066, 'interjectionality': 0.008017063842306561, 'syntactic_compression': 0.18409826419535158, 'quantificationality': 0.040453074433656956, 'referentiality': 0.12099146807884673, 'nominal_complexity': 0.043211238599588114, 'coordination_density': 0.01195204471903501, 'prepositional_density': 0.011253309796999117, 'modifier_density': 0.14441747572815533}\n",
    "\n",
    "Syntactic_All = {\"Muice\": Syntactic_Muice, \"Ayaka\": Syntactic_Ayaka, \"Zhongli\": Syntactic_Zhongli, \"Hutao\": Syntactic_Hutao, \"Haruhi\": Syntactic_Haruhi}\n",
    "\n",
    "# 语用风格\n",
    "Pragmatic_Muice = \"./outputs/pragmatic/muice.jsonl\"\n",
    "Pragmatic_ayaka = \"./outputs/pragmatic/ayaka.jsonl\"\n",
    "Pragmatic_zhongli = \"./outputs/pragmatic/zhongli.jsonl\"\n",
    "Pragmatic_hutao = \"./outputs/pragmatic/hutao.jsonl\"\n",
    "Pragmatic_haruhi = \"./outputs/pragmatic/haruhi.jsonl\"\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "class PCFGItem(TypedDict):\n",
    "    response: str\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "def read_pragmatic_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None) -> list[PCFGItem]:\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[PCFGItem] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_item: RawPCFGItem = json.loads(line)\n",
    "            raw_items.append(raw_item)\n",
    "\n",
    "    # list[dict[str, float]] -> dict[str, float] -> list[str]\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item[\"pragmatic_styles\"]\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        threshold = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > threshold:\n",
    "                final_styles.append(key)\n",
    "        \n",
    "        item = PCFGItem(response=raw_item[\"response\"], pragmatic_styles=final_styles)\n",
    "        items.append(item)\n",
    "\n",
    "    return items\n",
    "\n",
    "muice_pragmatic_items = read_pragmatic_jsonl_file(Path(Pragmatic_Muice), 0.4)\n",
    "ayaka_pragmatic_items = read_pragmatic_jsonl_file(Path(Pragmatic_ayaka), 0.4)\n",
    "zhongli_pragmatic_items = read_pragmatic_jsonl_file(Path(Pragmatic_zhongli), 0.4)\n",
    "hutao_pragmatic_items = read_pragmatic_jsonl_file(Path(Pragmatic_hutao), 0.4)\n",
    "haruhi_pragmatic_items = read_pragmatic_jsonl_file(Path(Pragmatic_haruhi), 0.4)\n",
    "\n",
    "pcfg_all_items = muice_pragmatic_items + ayaka_pragmatic_items + zhongli_pragmatic_items + hutao_pragmatic_items + haruhi_pragmatic_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2bf9d",
   "metadata": {},
   "source": [
    "## Call LLM to Generate CoT Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4afd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "DEFAULT_TEMPERATURE = float(os.getenv(\"NEUTRAL_TEMPERATURE\", \"0.4\"))\n",
    "DEFAULT_TOP_P = float(os.getenv(\"NEUTRAL_TOP_P\", \"0.9\"))\n",
    "DEFAULT_SLEEP_SECONDS = float(os.getenv(\"NEUTRAL_SLEEP_SECONDS\", \"0.3\"))\n",
    "MODEL = os.getenv(\"NEUTRAL_MODEL\", \"qwen3-30b-a3b-instruct-2507\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"你是一个角色风格分析器。 \n",
    "请你根据以下中性句和风格句，生成角色的思维链（CoT），解释角色如何根据对话风格逐步改变中性句的表达方式。 \n",
    "输出格式: `<think>...</think>`; 要求: 最多100字。清楚解释角色的情绪、动机，以及如何影响句式、语气和词汇使用。\n",
    "\"\"\"\n",
    "\n",
    "SHOTS_EXAMPLE = [\n",
    "    (\"角色: Katsumi\\n中性句: 谢谢你帮我。\\n风格句: 哼！才、才没想帮你呢，笨蛋！\\n人物风格: tsundere, defensive\\n关键字: 哼, 才没有, 笨蛋, 不需要, 自己\\n\", \"<think>她讨厌显得脆弱或欠别人情。当被感谢时，她会本能地否认帮助，以维护强硬和独立的形象。她使用“哼”“才没有”来掩饰感激，语气短促、带防御性。</think>\"),\n",
    "    (\"角色: Lady Elara\\n中性句: 我不知道这是否是个好主意。\\n风格句: 或许……我们还需要再仔细考虑，这是否真是最稳妥的选择。\\n人物风格: serious, rational, elegant\\n关键字: 考虑, 职责, 可能, 荣幸\\n\", \"<think>她理性冷静，不直接否定，而用审慎的表达来保持礼貌。她避免口语化词汇，使用书面词汇“或许”“值得考虑”，并保持句式平衡。</think>\")\n",
    "    ]\n",
    "\n",
    "EN_NAME_TO_ZH = {\"Muice\": \"沐雪\", \"Ayaka\": \"神里绫华\", \"Zhongli\": \"钟离\", \"Hutao\": \"胡桃\", \"Haruhi\": \"凉宫春日\"}\n",
    "\n",
    "llm = SimpleLLMClient(MODEL, DEFAULT_API_KEY, system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "def build_neutral_prompt(character: str, neutral_sentence: str, stylized_text: str, keywords:list[str], pcfg_items:dict[str, float], sentence_pragmatic_items:list[str]) -> str:\n",
    "    \"\"\"根据带风格的原句构造改写提示。\"\"\"\n",
    "    return (\n",
    "        f\"角色: {character}\\n\"\n",
    "        f\"中性句: {neutral_sentence}\\n\"\n",
    "        f\"风格句: {stylized_text}\\n\"\n",
    "        f\"关键字: {\", \".join(keywords)}\\n\"\n",
    "        f\"人物风格: {\", \".join(sentence_pragmatic_items)}\\n\"\n",
    "    )\n",
    "\n",
    "def find_sentence_pragmatic_items(stylized_text: str, pcfg_all_items: list[PCFGItem]) -> list[str]:\n",
    "    for item in pcfg_all_items:\n",
    "        if item[\"response\"] == stylized_text:\n",
    "            return item[\"pragmatic_styles\"]\n",
    "    return []  # 未找到则返回空列表\n",
    "\n",
    "\n",
    "_THINK_RE = re.compile(r\"<\\s*think\\s*>(.*?)<\\s*/\\s*think\\s*>\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def _extract_think_block(text: str) -> str | None:\n",
    "    if not text:\n",
    "        return None\n",
    "    text = text.strip()\n",
    "\n",
    "    if text.startswith(\"<think>\") and text.endswith(\"</think>\"):\n",
    "        return text\n",
    "    \n",
    "    # 1) 直接提取\n",
    "    m = _THINK_RE.search(text)\n",
    "    if m:\n",
    "        return f\"<think>{m.group(1).strip()}</think>\"\n",
    "    \n",
    "    # 2) 去掉代码块围栏再尝试\n",
    "    s2 = re.sub(r\"^```(?:[a-zA-Z0-9_-]+)?\\s*|\\s*```$\", \"\", text, flags=re.DOTALL).strip()\n",
    "    m = _THINK_RE.search(s2)\n",
    "    if m:\n",
    "        return f\"<think>{m.group(1).strip()}</think>\"\n",
    "    return None\n",
    "\n",
    "def generate_neutral_corpus_with_CoT(\n",
    "    neutral_sentences: list[NeutralSentenceItem],\n",
    "    *,\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    top_p: float = DEFAULT_TOP_P,\n",
    "    sleep_seconds: float = DEFAULT_SLEEP_SECONDS,\n",
    ") -> list[Dict[str, str]]:\n",
    "    \"\"\"调用 llm.ask 批量生成中性句，并确保输出为 <think>...</think> 格式。\"\"\"\n",
    "    records: list[Dict[str, str]] = []\n",
    "    MAX_RETRIES = 2\n",
    "\n",
    "    for item in tqdm(neutral_sentences):\n",
    "        character = item[\"character\"]\n",
    "        keywords = Lexical_All.get(character, [])\n",
    "        pcfg_items = Syntactic_All.get(character, {})\n",
    "        sentence_pragmatic_items = find_sentence_pragmatic_items(item[\"original\"], pcfg_all_items)\n",
    "        prompt = build_neutral_prompt(\n",
    "            EN_NAME_TO_ZH.get(item[\"character\"], item[\"character\"]),\n",
    "            item[\"neutral\"],\n",
    "            item[\"original\"],\n",
    "            keywords,\n",
    "            pcfg_items,\n",
    "            sentence_pragmatic_items,\n",
    "        )\n",
    "\n",
    "        attempt = 0\n",
    "        thinking_process = \"\"\n",
    "        while attempt <= MAX_RETRIES:\n",
    "            result = llm.ask(\n",
    "                prompt=prompt,\n",
    "                history=SHOTS_EXAMPLE,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            ).strip()\n",
    "\n",
    "            thinking_process = _extract_think_block(result)\n",
    "            if thinking_process:\n",
    "                break\n",
    "\n",
    "        if thinking_process is None or attempt > MAX_RETRIES:\n",
    "            print(f\"生成思维链失败: {item['original']}\")\n",
    "        else:\n",
    "            records.append({\n",
    "                \"character\": item[\"character\"],\n",
    "                \"original\": item[\"original\"],\n",
    "                \"neutral\": item[\"neutral\"],\n",
    "                \"CoT\": thinking_process\n",
    "            })\n",
    "\n",
    "        sleep(sleep_seconds)\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad46687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 6596, Done: 6511, Remaining: 0\n",
      "All items are already processed.\n"
     ]
    }
   ],
   "source": [
    "# 分批生成并可恢复的 CoT 生成与保存\n",
    "\n",
    "OUTPUT_PATH = Path(\"./data/neutral_sentences_with_CoT.jsonl\")\n",
    "BATCH_SIZE = 200             # 每批处理条数\n",
    "\n",
    "def _load_done_keys(path: Path) -> set[tuple[str, str]]:\n",
    "    \"\"\"从已存在的 JSONL 中读取已完成项的键 (character, original)。\"\"\"\n",
    "    done: set[tuple[str, str]] = set()\n",
    "    if path.is_file():\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    done.add((obj[\"character\"], obj[\"original\"]))\n",
    "                except Exception:\n",
    "                    # 跳过坏行\n",
    "                    continue\n",
    "    return done\n",
    "\n",
    "\n",
    "def _append_jsonl(path: Path, items: list[dict]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for item in items:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# 读取进度\n",
    "done_keys = _load_done_keys(OUTPUT_PATH)\n",
    "total = len(neutral_items)\n",
    "remaining_items = [it for it in neutral_items if (it[\"character\"], it[\"original\"]) not in done_keys]\n",
    "\n",
    "print(f\"Total: {total}, Done: {len(done_keys)}, Remaining: {len(remaining_items)}\")\n",
    "\n",
    "if not remaining_items:\n",
    "    print(\"All items are already processed.\")\n",
    "else:\n",
    "    start_done = len(done_keys)\n",
    "    batches_run = 0\n",
    "\n",
    "    # 逐批处理\n",
    "    while remaining_items:\n",
    "        batch = remaining_items[:BATCH_SIZE]\n",
    "        remaining_items = remaining_items[BATCH_SIZE:]\n",
    "\n",
    "        print(f\"Processing batch {batches_run + 1}: {len(batch)} items...\")\n",
    "        records = generate_neutral_corpus_with_CoT(batch)\n",
    "\n",
    "        # 追加保存，便于中断后恢复\n",
    "        _append_jsonl(OUTPUT_PATH, records)\n",
    "\n",
    "        # 更新已完成键，便于重复运行时即时跳过\n",
    "        for r in records:\n",
    "            done_keys.add((r[\"character\"], r[\"original\"]))\n",
    "\n",
    "        print(f\"Saved {len(records)} items. Progress: {len(done_keys)}/{total}\")\n",
    "        batches_run += 1\n",
    "\n",
    "    print(f\"Run finished. New processed: {len(done_keys) - start_done}. Remaining: {total - len(done_keys)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}