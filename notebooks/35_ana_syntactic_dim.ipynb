{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ffdf28",
   "metadata": {},
   "source": [
    "# Syntactic Pattern Dimension Ablation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734a294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/OtakuLab'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this on Remote Jupyter Book.\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "# os.chdir(\"/root/OtakuLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e449c",
   "metadata": {},
   "source": [
    "## Define PCFG Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de18cc",
   "metadata": {},
   "source": [
    "### d=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113dd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_to_10dim = {\n",
    "    # Referentiality: 专名、指称性\n",
    "    \"NP -> NR\": \"referentiality\",\n",
    "    \"NP -> PN\": \"referentiality\",\n",
    "    \"NP -> NR NN\": \"referentiality\",\n",
    "    \"DP -> DT\": \"referentiality\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "    # Interjectionality: 感叹与插入语气\n",
    "    \"INTJ -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> SP\": \"interjectionality\",\n",
    "    \"IP -> INTJ PU VP\": \"interjectionality\",\n",
    "    \"IP -> INTJ VP\": \"interjectionality\",\n",
    "\n",
    "    # Declarativity: 陈述语气/句型层级\n",
    "    \"CP -> IP SP\": \"declarativity\",\n",
    "    \"CP -> IP SP PU\": \"declarativity\",\n",
    "    \"CP -> IP DEC\": \"declarativity\",\n",
    "    \"TOP -> CP\": \"declarativity\",\n",
    "    \"CP -> CP\": \"declarativity\",\n",
    "    \"TOP -> IP\": \"declarativity\",\n",
    "    \"VP -> VC NP\": \"declarativity\",\n",
    "\n",
    "    # Clausal Embedding: 子句嵌套\n",
    "    \"VP -> VV IP\": \"clausal_embedding\",\n",
    "    \"LCP -> IP LC\": \"clausal_embedding\",\n",
    "    \"IP -> ADVP PU NP VP\": \"clausal_embedding\",\n",
    "    \"NP -> CP NP\": \"clausal_embedding\",\n",
    "\n",
    "    # Subordination: 修饰性从属结构\n",
    "    \"VP -> ADVP VP\": \"subordination\",\n",
    "    \"IP -> NP VP\": \"subordination\",\n",
    "    \"IP -> VP\": \"subordination\",\n",
    "    \"VP -> VV NP\": \"subordination\",\n",
    "    \"VP -> VA\": \"subordination\",\n",
    "    \"IP -> VP SP\": \"subordination\",\n",
    "    \"VP -> PP VP\": \"subordination\",\n",
    "\n",
    "    # Parallelism: 句式并列\n",
    "    \"VP -> VP PU VP\": \"parallelism\",\n",
    "    \"VP -> VP PU VP PU VP\": \"parallelism\",\n",
    "    \"UCP -> IP PU CP\": \"parallelism\",\n",
    "    \"IP -> VP PU\": \"parallelism\",\n",
    "    \"TOP -> UCP\": \"parallelism\",\n",
    "\n",
    "    # Modifier Density: 修饰成分密度\n",
    "    \"DNP -> ADJP DEG\": \"modifier_density\",\n",
    "    \"DNP -> NP DEG\": \"modifier_density\",\n",
    "    \"NP -> DNP NP\": \"modifier_density\",\n",
    "    \"ADVP -> AD\": \"modifier_density\",\n",
    "    \"ADJP -> JJ\": \"modifier_density\",\n",
    "\n",
    "    # Nominal Complexity: 名词短语复杂度\n",
    "    \"NP -> ADJP NP\": \"nominal_complexity\",\n",
    "    \"NP -> DNP NP\": \"nominal_complexity\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "\n",
    "    # Topic Fronting: 话题提前结构\n",
    "    \"TOP -> NP IP\": \"topic_fronting\",\n",
    "\n",
    "    # Ellipsis or Fragmentation: 口语省略、残缺句\n",
    "    \"IP -> VP\": \"ellipsis_or_fragmentation\",\n",
    "    \"IP -> ADVP VP\": \"ellipsis_or_fragmentation\",\n",
    "\n",
    "    # Deep Embedding: 深层嵌套\n",
    "    \"VP -> VV NP IP\": \"clausal_embedding\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4d167",
   "metadata": {},
   "source": [
    "### d=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1452db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Revised PCFG-to-Dimension Mapping ----\n",
    "rule_to_18dim = {\n",
    "    # Referentiality: 专名、指称性\n",
    "    \"NP -> NR\": \"referentiality\",\n",
    "    \"NP -> PN\": \"referentiality\",\n",
    "    \"NP -> NR NN\": \"referentiality\",\n",
    "    \"DP -> DT\": \"referentiality\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "    # Interjectionality: 感叹与插入语气\n",
    "    \"INTJ -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> SP\": \"interjectionality\",\n",
    "    \"IP -> INTJ PU VP\": \"interjectionality\",\n",
    "    \"IP -> INTJ VP\": \"interjectionality\",\n",
    "\n",
    "    # Declarativity: 陈述语气/句型层级\n",
    "    \"CP -> IP SP\": \"declarativity\",\n",
    "    \"CP -> IP SP PU\": \"declarativity\",\n",
    "    \"CP -> IP DEC\": \"declarativity\",\n",
    "    \"TOP -> CP\": \"declarativity\",\n",
    "    \"CP -> CP\": \"declarativity\",\n",
    "    \"TOP -> IP\": \"declarativity\",\n",
    "    \"VP -> VC NP\": \"declarativity\",\n",
    "\n",
    "    # Clausal Embedding: 子句嵌套\n",
    "    \"VP -> VV IP\": \"clausal_embedding\",\n",
    "    \"LCP -> IP LC\": \"clausal_embedding\",\n",
    "    \"IP -> ADVP PU NP VP\": \"clausal_embedding\",\n",
    "    \"NP -> CP NP\": \"clausal_embedding\",\n",
    "\n",
    "    # Subordination: 修饰性从属结构\n",
    "    \"VP -> ADVP VP\": \"subordination\",\n",
    "    \"IP -> NP VP\": \"subordination\",\n",
    "    \"IP -> VP\": \"subordination\",\n",
    "    \"VP -> VV NP\": \"subordination\",\n",
    "    \"VP -> VA\": \"subordination\",\n",
    "    \"IP -> VP SP\": \"subordination\",\n",
    "    \"VP -> PP VP\": \"subordination\",\n",
    "\n",
    "    # Parallelism: 句式并列\n",
    "    \"VP -> VP PU VP\": \"parallelism\",\n",
    "    \"VP -> VP PU VP PU VP\": \"parallelism\",\n",
    "    \"UCP -> IP PU CP\": \"parallelism\",\n",
    "    \"IP -> VP PU\": \"parallelism\",\n",
    "    \"TOP -> UCP\": \"parallelism\",\n",
    "\n",
    "    # Coordination Density: 并列结构复杂度\n",
    "    \"NP -> NP CC NP\": \"coordination_density\",\n",
    "    \"NP -> NN CC NN\": \"coordination_density\",\n",
    "    \"CP -> CP CC CP\": \"coordination_density\",\n",
    "\n",
    "    # Modifier Density: 修饰成分密度\n",
    "    \"DNP -> ADJP DEG\": \"modifier_density\",\n",
    "    \"DNP -> NP DEG\": \"modifier_density\",\n",
    "    \"NP -> DNP NP\": \"modifier_density\",\n",
    "    \"ADVP -> AD\": \"modifier_density\",\n",
    "    \"ADJP -> JJ\": \"modifier_density\",\n",
    "\n",
    "    # Nominal Complexity: 名词短语复杂度\n",
    "    \"NP -> ADJP NP\": \"nominal_complexity\",\n",
    "    \"NP -> DNP NP\": \"nominal_complexity\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "    # Prepositional Density: 介词结构使用密度\n",
    "    \"PP -> P LCP\": \"prepositional_density\",\n",
    "    \"PP -> P NP\": \"prepositional_density\",\n",
    "\n",
    "    # Topic Fronting: 话题提前结构\n",
    "    \"TOP -> NP IP\": \"topic_fronting\",\n",
    "\n",
    "    # Ellipsis or Fragmentation: 口语省略、残缺句\n",
    "    \"IP -> VP\": \"ellipsis_or_fragmentation\",\n",
    "    \"IP -> ADVP VP\": \"ellipsis_or_fragmentation\",\n",
    "\n",
    "    # Syntactic Compression: 句法压缩\n",
    "    \"NP -> NN\": \"syntactic_compression\",\n",
    "    \"VP -> VV\": \"syntactic_compression\",\n",
    "    \"VP -> VV VP\": \"syntactic_compression\",\n",
    "\n",
    "    # Quantificationality: 数量词使用\n",
    "    \"CLP -> M\": \"quantificationality\",\n",
    "    \"QP -> CD CLP\": \"quantificationality\",\n",
    "    \"NP -> DP NP\": \"quantificationality\",\n",
    "    \"NP -> QP CP NP\": \"quantificationality\",\n",
    "\n",
    "    # Deep Embedding: 深层嵌套\n",
    "    \"VP -> VV NP IP\": \"clausal_embedding\",\n",
    "\n",
    "    # Aspectuality: 体貌表达（了 / 过 / 着）\n",
    "    \"VP -> VV AS\": \"aspectuality\",\n",
    "    \"VP -> VV AS NP\": \"aspectuality\",\n",
    "\n",
    "    # Existential / Presentational: 存现与判断\n",
    "    \"VP -> VE NP\": \"existentiality\",\n",
    "    \"IP -> NP VE NP\": \"existentiality\",\n",
    "\n",
    "    # Serial Verb Construction: 连动结构\n",
    "    \"VP -> VP VP\": \"serial_verb_construction\",\n",
    "    \"VP -> VV NP VP\": \"serial_verb_construction\",\n",
    "\n",
    "    # Discourse Framing: 话语框架 / 状语堆叠\n",
    "    \"VP -> ADVP ADVP VP\": \"discourse_framing\",\n",
    "    \"IP -> ADVP NP VP\": \"discourse_framing\",\n",
    "    \"ADVP -> CS\": \"discourse_framing\",\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2d824",
   "metadata": {},
   "source": [
    "## Data Processing and Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7305d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any, Dict, Iterable, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "DATASET_DIR = BASE_DIR / \"dataset\"\n",
    "OUTPUTS_DIR = BASE_DIR / \"evaluate\" / \"outputs\" / \"DimensionDissolution\"\n",
    "OUTPUTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "NEUTRAL_SENTENCES_FILE = DATASET_DIR / \"neutral_sentences_with_CoT.jsonl\"\n",
    "\n",
    "class InstructionComponents(TypedDict):\n",
    "    lexical_keywords: list[str]\n",
    "    syntactic_vector: dict[str, float]\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "class DatasetItem(TypedDict):\n",
    "    character: str\n",
    "    neutral_sentence: str\n",
    "    instruction_components: InstructionComponents | dict[str, list[str]|dict]\n",
    "    thinking_process: str\n",
    "    output: str\n",
    "\n",
    "class DatasetStorage:\n",
    "    def __init__(self) -> None:\n",
    "        self.items: dict[str, DatasetItem] = {}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def new_item(self, character: str, neutral_sentence: str, thinking_process:str, output: str):\n",
    "        item = DatasetItem(character=character,\n",
    "                           neutral_sentence=neutral_sentence,\n",
    "                           instruction_components={},\n",
    "                           thinking_process=thinking_process,\n",
    "                           output=output)\n",
    "        self.items[output] = item\n",
    "\n",
    "    def save_characters_keywords(self, character: str, lexical_keywords: list[str]):\n",
    "        saved = False\n",
    "        for item in self.items.values():\n",
    "            if item['character'] == character:\n",
    "                output = item['output']\n",
    "                self.items[output]['instruction_components']['lexical_keywords'] = lexical_keywords\n",
    "                saved = True\n",
    "        if not saved:\n",
    "            raise ValueError(f\"角色 {character} 未找到\")        \n",
    "\n",
    "    def save_component(self,\n",
    "                       output: str,\n",
    "                       syntactic_vector: Optional[dict[str, float]] = None,\n",
    "                       pragmatic_styles: Optional[list[str]] = None):\n",
    "        if output not in self.items.keys():\n",
    "            raise ValueError(f\"风格句: {output} 似乎未加载\")\n",
    "                \n",
    "        if syntactic_vector:\n",
    "            self.items[output]['instruction_components']['syntactic_vector'] = syntactic_vector\n",
    "        if pragmatic_styles:\n",
    "            self.items[output]['instruction_components']['pragmatic_styles'] = pragmatic_styles\n",
    "\n",
    "    @staticmethod\n",
    "    def _verify_validity(item: DatasetItem):\n",
    "        if not all((item['character'],\n",
    "                    item['neutral_sentence'],\n",
    "                    item['output'],\n",
    "                    item['instruction_components'])):\n",
    "            return False\n",
    "        \n",
    "        instruction_components = item['instruction_components']\n",
    "        \n",
    "        return all((\n",
    "            isinstance(instruction_components.get('lexical_keywords'), list),\n",
    "            isinstance(instruction_components.get('pragmatic_styles'), list),\n",
    "            isinstance(instruction_components.get(\"syntactic_vector\"), dict)\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _oversampling(items_list: list[DatasetItem], item: DatasetItem):\n",
    "        difficult_labels_5x = {'tsundere', 'sharp_tongued', 'proud'}\n",
    "        difficult_labels_3x = {'tsukkomi', 'chuunibyou', 'yandere', 'airhead'}\n",
    "        pragmatic_styles = set(item['instruction_components'].get('pragmatic_styles', []))\n",
    "        \n",
    "        if pragmatic_styles & difficult_labels_5x:\n",
    "            for _ in range(5):\n",
    "                items_list.append(item)\n",
    "        elif pragmatic_styles & difficult_labels_3x:\n",
    "            for _ in range(3):\n",
    "                items_list.append(item)\n",
    "\n",
    "    def output(self, output_path: Path, oversampling: bool = False):\n",
    "        items = list(self.items.values())\n",
    "        vaild_items = []\n",
    "        vaild_items_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if not self._verify_validity(item):\n",
    "                continue\n",
    "            vaild_items.append(item)\n",
    "            vaild_items_count += 1\n",
    "            \n",
    "            if oversampling:\n",
    "                self._oversampling(vaild_items, item)\n",
    "\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vaild_items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"训练集已导出至 {output_path}, 总有效训练集数量: {len(vaild_items)}, 跳过数量: {len(items) - vaild_items_count}\")\n",
    "\n",
    "\n",
    "class PCFGExtractor:\n",
    "    def __init__(self, rule_mapping: dict[str, str]) -> None:\n",
    "        self.rules_counter: Dict[str, Counter[Tuple[str, ...]]] = defaultdict(Counter)\n",
    "        self.total_rules: int = 0\n",
    "        self.rule_mapping = rule_mapping\n",
    "\n",
    "    def load_trees(self, file_path: Path) -> list[dict[str, Any]]:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def extract_rules_from_tree(self, tree: Any) -> None:\n",
    "        if not isinstance(tree, list) or len(tree) != 2:\n",
    "            return\n",
    "        lhs_symbol, rhs = tree\n",
    "        if isinstance(rhs, list) and all(isinstance(child, list) and len(child) == 2 for child in rhs):\n",
    "            rhs_symbols = tuple(child[0] for child in rhs)\n",
    "            self.rules_counter[lhs_symbol][rhs_symbols] += 1\n",
    "            self.total_rules += 1\n",
    "            for child in rhs:\n",
    "                self.extract_rules_from_tree(child)\n",
    "\n",
    "    def extract_from_data(self, data: Iterable[dict[str, Any]]) -> None:\n",
    "        for item in data:\n",
    "            for tree in item.get(\"con\", []):\n",
    "                self.extract_rules_from_tree(tree)\n",
    "\n",
    "    def build_feature_vectors(self) -> dict[str, float]:\n",
    "        feature_vector: dict[str, float] = defaultdict(float)\n",
    "        unmapped_rules_counter = Counter()\n",
    "        unmapped_rules_count = 0\n",
    "\n",
    "        # 确保所有维度都有初始值 0.0，避免维度缺失\n",
    "        all_dims = set(self.rule_mapping.values())\n",
    "        for dim in all_dims:\n",
    "            feature_vector[dim] = 0.0\n",
    "\n",
    "        for lhs_symbol, rhs_counter in self.rules_counter.items():\n",
    "            for rhs_symbols, freq in rhs_counter.items():\n",
    "                rule = f\"{lhs_symbol} -> {' '.join(rhs_symbols)}\"\n",
    "                dim = self.rule_mapping.get(rule)\n",
    "                if not dim:\n",
    "                    unmapped_rules_count += freq\n",
    "                    unmapped_rules_counter[rule] += freq\n",
    "                    continue\n",
    "                feature_vector[dim] += freq\n",
    "\n",
    "        print(f\"Total rules processed: {self.total_rules}, \"\n",
    "              f\"Unmapped rules: {unmapped_rules_count}({unmapped_rules_count / self.total_rules * 100:.2f}%), \"\n",
    "              f\"Mapped rules: {self.total_rules - unmapped_rules_count}({(self.total_rules - unmapped_rules_count) / self.total_rules * 100:.2f}%)\")\n",
    "\n",
    "        total = sum(feature_vector.values()) or 1.0\n",
    "        return {dim: freq / total for dim, freq in feature_vector.items()}\n",
    "\n",
    "# === 统一句法向量计算：先抽取 rule 计数，再按维度映射聚合 ===\n",
    "# 这样 10d/18d 的 syntactic_vec 都来自同一份底层 rule 分布，避免维度定义不一致导致的偏差。\n",
    "_PCFG_RULE_COUNT_CACHE: dict[str, tuple[dict[str, int], int]] = {}\n",
    "\n",
    "def extract_pcfg_rule_counts(path: Path) -> tuple[dict[str, int], int]:\n",
    "    key = str(path.resolve())\n",
    "    if key in _PCFG_RULE_COUNT_CACHE:\n",
    "        return _PCFG_RULE_COUNT_CACHE[key]\n",
    "\n",
    "    extractor = PCFGExtractor(rule_mapping={})\n",
    "    trees_data = extractor.load_trees(path)\n",
    "    extractor.extract_from_data(trees_data)\n",
    "\n",
    "    rule_counts: dict[str, int] = {}\n",
    "    for lhs_symbol, rhs_counter in extractor.rules_counter.items():\n",
    "        for rhs_symbols, count in rhs_counter.items():\n",
    "            rule = f\"{lhs_symbol} -> {' '.join(rhs_symbols)}\"\n",
    "            rule_counts[rule] = int(count)\n",
    "\n",
    "    result = (rule_counts, int(extractor.total_rules))\n",
    "    _PCFG_RULE_COUNT_CACHE[key] = result\n",
    "    return result\n",
    "\n",
    "def aggregate_rule_counts_to_dims(rule_counts: dict[str, int], rule_mapping: dict[str, str]) -> tuple[dict[str, float], int, int]:\n",
    "    feature_counts: dict[str, int] = {dim: 0 for dim in set(rule_mapping.values())}\n",
    "    unmapped_rules_count = 0\n",
    "    for rule, count in rule_counts.items():\n",
    "        dim = rule_mapping.get(rule)\n",
    "        if not dim:\n",
    "            unmapped_rules_count += count\n",
    "            continue\n",
    "        feature_counts[dim] += count\n",
    "\n",
    "    mapped_rules_count = int(sum(feature_counts.values()))\n",
    "    total = mapped_rules_count or 1\n",
    "    feature_vector = {dim: c / total for dim, c in feature_counts.items()}\n",
    "    return feature_vector, mapped_rules_count, unmapped_rules_count\n",
    "\n",
    "def load_and_extract_pcfg(path: Path, rule_mapping: dict[str, str]) -> dict[str, float]:\n",
    "    rule_counts, total_rules = extract_pcfg_rule_counts(path)\n",
    "    feature_vector, mapped_rules, unmapped_rules = aggregate_rule_counts_to_dims(rule_counts, rule_mapping)\n",
    "    if total_rules > 0:\n",
    "        print(\n",
    "            f\"Total rules processed: {total_rules}, \"\n",
    "            f\"Unmapped rules: {unmapped_rules}({unmapped_rules / total_rules * 100:.2f}%), \"\n",
    "            f\"Mapped rules: {mapped_rules}({mapped_rules / total_rules * 100:.2f}%)\"\n",
    "        )\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 数据集生成函数 ===\n",
    "\n",
    "EN_NAME_TO_ZH = {\"Muice\": \"沐雪\", \"Ayaka\": \"神里绫华\", \"Zhongli\": \"钟离\", \"Hutao\": \"胡桃\", \"Haruhi\": \"凉宫春日\"}\n",
    "\n",
    "def generate_dataset_for_dim(dim_name: str, rule_mapping: dict[str, str]):\n",
    "    \"\"\"\n",
    "    根据给定的 rule_mapping 生成训练集\n",
    "    dim_name: \"10d\" 或 \"18d\" 等标识\n",
    "    \"\"\"\n",
    "    print(f\"Generating {dim_name} dataset...\")\n",
    "    storage = DatasetStorage()\n",
    "    \n",
    "    # 1. 加载中性句\n",
    "    print(\"Loading neutral sentences...\")\n",
    "    with open(NEUTRAL_SENTENCES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        jsonl_lines = f.readlines()\n",
    "    for line in jsonl_lines:\n",
    "        if line := line.rstrip():\n",
    "            item = json.loads(line)\n",
    "            character = EN_NAME_TO_ZH.get(item[\"character\"], None) or item[\"character\"]\n",
    "            storage.new_item(character, item[\"neutral\"], item['CoT'], item[\"original\"])\n",
    "    \n",
    "    # 2. 加载词汇层向量 (复用输出目录下的文件)\n",
    "    print(\"Loading lexical keywords...\")\n",
    "    def get_lexical(name):\n",
    "        path = BASE_DIR / \"outputs\" / \"pmi\" / f\"{name}_pmi_filtered.json\"\n",
    "        if not path.exists():\n",
    "             # Fallback logic or empty\n",
    "             return []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return list(json.loads(f.read()).keys())[:25]\n",
    "\n",
    "    storage.save_characters_keywords(\"沐雪\", get_lexical(\"muice\"))\n",
    "    storage.save_characters_keywords(\"神里绫华\", get_lexical(\"ayaka\"))\n",
    "    storage.save_characters_keywords(\"钟离\", get_lexical(\"zhongli\"))\n",
    "    storage.save_characters_keywords(\"胡桃\", get_lexical(\"hutao\"))\n",
    "    storage.save_characters_keywords(\"凉宫春日\", get_lexical(\"haruhi\"))\n",
    "    \n",
    "    # 3. 提取 PCFG 向量\n",
    "    print(\"Extracting PCFG vectors...\")\n",
    "    \n",
    "    def process_pcfg(char_zh, char_en):\n",
    "        path = BASE_DIR / \"outputs\" / \"cons\" / f\"{char_en}.json\"\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: PCFG file not found for {char_en}\")\n",
    "            return\n",
    "        vector = load_and_extract_pcfg(path, rule_mapping)\n",
    "        # 为该角色的所有句子保存 PCFG 向量\n",
    "        count = 0\n",
    "        for item in storage.items.values():\n",
    "            if item[\"character\"] == char_zh:\n",
    "                storage.save_component(item[\"output\"], syntactic_vector=vector)\n",
    "                count += 1\n",
    "\n",
    "    process_pcfg(\"沐雪\", \"muice\")\n",
    "    process_pcfg(\"神里绫华\", \"ayaka\")\n",
    "    process_pcfg(\"钟离\", \"zhongli\")\n",
    "    process_pcfg(\"胡桃\", \"hutao\")\n",
    "    process_pcfg(\"凉宫春日\", \"haruhi\")\n",
    "\n",
    "    # 4. 加载语用风格\n",
    "    print(\"Loading pragmatic styles...\")\n",
    "    \n",
    "    class RawPCFGItem(TypedDict):\n",
    "        response: str\n",
    "        pragmatic_styles: list[dict[str, float]] # Note: structure might vary, handled below\n",
    "\n",
    "    def load_pragmatic(char_en):\n",
    "        path = BASE_DIR/ \"outputs\" / \"pragmatic\" / f\"{char_en}.jsonl\"\n",
    "        if not path.exists():\n",
    "            return\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if not line.strip(): continue\n",
    "            raw_item = json.loads(line)\n",
    "            # Flatten styles\n",
    "            raw_styles = raw_item[\"pragmatic_styles\"]\n",
    "            styles_map = {}\n",
    "            for vec in raw_styles:\n",
    "                styles_map.update(vec)\n",
    "            \n",
    "            final_styles = [k for k, v in styles_map.items() if v > 0.4]\n",
    "            try:\n",
    "                storage.save_component(raw_item[\"response\"], pragmatic_styles=final_styles)\n",
    "                count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"  Loaded pragmatic styles for {char_en} ({count} items)\")\n",
    "        \n",
    "    load_pragmatic(\"muice\")\n",
    "    load_pragmatic(\"ayaka\")\n",
    "    load_pragmatic(\"zhongli\")\n",
    "    load_pragmatic(\"hutao\")\n",
    "    load_pragmatic(\"haruhi\")\n",
    "\n",
    "    # 5. 导出\n",
    "    oversample_file = OUTPUTS_DIR / f\"llm_train_{dim_name}_oversampling.json\"\n",
    "    storage.output(oversample_file, oversampling=True)\n",
    "    return oversample_file\n",
    "\n",
    "# 生成两个版本的数据集\n",
    "data_path_10d = generate_dataset_for_dim(\"10d\", rule_to_10dim)\n",
    "data_path_18d = generate_dataset_for_dim(\"18d\", rule_to_18dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c20735",
   "metadata": {},
   "source": [
    "## Model Definition and Training\n",
    "Define model structures that support style vector inputs of different dimensions, and provide training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13023a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, Qwen3ForCausalLM\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers.optimization import get_scheduler\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.autonotebook import tqdm\n",
    "import math\n",
    "\n",
    "BASE_MODEL = Path(\"../Models/Qwen3-1.7B\")\n",
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, path:Path, tokenizer, prag_style_vocab: list[str], syntactic_dims: list[str], max_length=256):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data: list[DatasetItem] = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prag_vocab = {style: i for i, style in enumerate(prag_style_vocab)}\n",
    "        self.syntactic_dims = syntactic_dims\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        item = self.data[idx]\n",
    "        instr = item[\"instruction_components\"]\n",
    "        # instr可能有些是空的，这里做个简单健壮性处理\n",
    "        if not isinstance(instr, dict):\n",
    "             # Should be filtered by validity check, but strictly typed\n",
    "             print(f\"Warning: instruction_components is not a dict for item idx {idx}\")\n",
    "             instr = {\"lexical_keywords\":[], \"syntactic_vector\":{}, \"pragmatic_styles\":[]} # type:ignore\n",
    "\n",
    "        keywords = \", \".join(instr.get(\"lexical_keywords\", [])) if instr.get(\"lexical_keywords\") else \"None\"\n",
    "        pragmatic_styles = \", \".join(instr.get(\"pragmatic_styles\", [])) if instr.get(\"pragmatic_styles\") else \"None\"\n",
    "        \n",
    "        system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "        user_prompt = (\n",
    "            f\"Target Character {item['character']}\\n\"\n",
    "            f\"Personality: {pragmatic_styles}\\n\"\n",
    "            f\"Keywords: {keywords}\\n\"\n",
    "            f\"Neutral Content: {item['neutral_sentence']}\\n\"\n",
    "        )\n",
    "        assistant_response = f\"{item['thinking_process']}\\n\\n{item['output']}\"\n",
    "\n",
    "        full_text = self.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "\n",
    "        self.tokenizer.truncation_side = \"left\"\n",
    "        full_tokenized = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "        input_ids = full_tokenized[\"input_ids\"] \n",
    "\n",
    "        prompt_only_text = self.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompt_len = len(self.tokenizer(prompt_only_text).input_ids)\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        # mask prompt\n",
    "        labels_len = len(labels)\n",
    "        mask_len = min(prompt_len, labels_len)\n",
    "        labels[:mask_len] = [-100] * mask_len\n",
    "\n",
    "        syn_dict_raw = instr.get(\"syntactic_vector\", {})\n",
    "        syn_dict = syn_dict_raw if isinstance(syn_dict_raw, dict) else {}\n",
    "        syntactic_vec = torch.tensor(\n",
    "            [syn_dict.get(dim, 0.0) for dim in self.syntactic_dims],\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        prag_vec = torch.zeros(len(self.prag_vocab), dtype=torch.float32)\n",
    "\n",
    "        for tag in instr.get(\"pragmatic_styles\", []):\n",
    "            if tag in self.prag_vocab:\n",
    "                prag_vec[self.prag_vocab[tag]] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"syntactic_vec\": syntactic_vec,\n",
    "            \"prag_vec\": prag_vec,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "class StyleDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, padding=True, **kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        syntactic_vecs = torch.stack([f[\"syntactic_vec\"] for f in features])\n",
    "        prag_vecs = torch.stack([f[\"prag_vec\"] for f in features])\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "\n",
    "        base_features = [\n",
    "            {k: v for k, v in f.items() if k not in (\"syntactic_vec\", \"prag_vec\", \"labels\")}\n",
    "            for f in features\n",
    "        ]\n",
    "        batch = super().__call__(base_features)\n",
    "\n",
    "        max_label_length = batch[\"input_ids\"].shape[1]\n",
    "        padded_labels = []\n",
    "        for label_seq in labels:\n",
    "            truncated = label_seq[:max_label_length]\n",
    "            padding = max_label_length - len(truncated)\n",
    "            padded_labels.append(truncated + [-100] * padding)\n",
    "        \n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "        batch[\"syntactic_vec\"] = syntactic_vecs\n",
    "        batch[\"prag_vec\"] = prag_vecs\n",
    "        return batch\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim // 2, out_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, syntactic_vec):\n",
    "        return self.proj(syntactic_vec)\n",
    "    \n",
    "class StyleConditionedLoRAModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str|Path, syntactic_dim: int, lora_r=16, lora_alpha=16):\n",
    "        super().__init__()\n",
    "        base_model = Qwen3ForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            dtype=torch.bfloat16\n",
    "        )\n",
    "        config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        self.peft_model: PeftModel = get_peft_model(base_model, config) # type: ignore\n",
    "        # self.peft_model.print_trainable_parameters()\n",
    "        \n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.style_encoder = StyleEncoder(syntactic_dim, self.hidden_size)\n",
    "        self.style_encoder.to(base_model.dtype)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.peft_model.get_input_embeddings() # type: ignore\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, syntactic_vec):\n",
    "        style_emb = self.style_encoder(syntactic_vec.to(self.peft_model.dtype)) # type: ignore\n",
    "        style_emb_prefix = style_emb.unsqueeze(1)\n",
    "        token_embeds = self.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([style_emb_prefix, token_embeds], dim=1)\n",
    "        \n",
    "        prefix_mask = torch.ones(attention_mask.shape[0], 1, dtype=torch.long, device=attention_mask.device)\n",
    "        new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        \n",
    "        prefix_labels = torch.full((labels.shape[0], 1), -100, dtype=torch.long, device=labels.device)\n",
    "        new_labels = torch.cat([prefix_labels, labels], dim=1)\n",
    "        \n",
    "        outputs = self.peft_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=new_attention_mask,\n",
    "            labels=new_labels,\n",
    "            output_hidden_states=True \n",
    "        )\n",
    "        return {\n",
    "            \"loss\": outputs.loss,\n",
    "            \"logits\": outputs.logits,\n",
    "            \"last_hidden_state\": outputs.hidden_states[-1] \n",
    "        }\n",
    "\n",
    "# === 训练函数 ===\n",
    "\n",
    "def run_training_experiment(dataset_path: Path, syntactic_dims: list[str], output_dir: Path, epochs=2):\n",
    "    print(f\"\\nStarted training experiment for dataset: {dataset_path}\")\n",
    "    print(f\"Syntactic Dimensions ({len(syntactic_dims)}): {syntactic_dims}\")\n",
    "    \n",
    "    prag_style_vocab = ['kind', 'modest', 'clingy', 'playful', 'cold', 'proud', 'sharp_tongued', 'subservient', 'submissive', 'controlling',\n",
    "                        'strong', 'defensive', 'tsukkomi', 'rational', 'curious', 'imaginative', 'cautious', 'idealistic', 'conservative',\n",
    "                        'radical', 'obsessive', 'hesitant', 'energetic', 'optimistic', 'confident', 'passionate', 'melancholy', 'serious',\n",
    "                        'emotional', 'sensitive', 'shy', 'irritable', 'anxious', 'lazy', 'tsundere', 'yandere', 'chuunibyou', 'cute', 'naive',\n",
    "                        'airhead', 'elegant', 'humorous', 'loyal', 'responsible', 'willful', 'antisocial', 'talkative', 'masochistic', 'sadistic', 'evil']\n",
    "    \n",
    "    BATCH_SIZE = 6\n",
    "    LEARNING_RATE = 1e-5\n",
    "    lambda_recon = 0.05\n",
    "    lambda_style = 0.5\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load Setup\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    except Exception:\n",
    "        print(\"Model path not found, using a dummy path for compilation check or valid path required.\")\n",
    "        return\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "    dataset = StyleDataset(dataset_path, tokenizer, prag_style_vocab, syntactic_dims)\n",
    "        \n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    collator = StyleDataCollator(tokenizer, pad_to_multiple_of=8)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "    \n",
    "    styled_model = StyleConditionedLoRAModel(BASE_MODEL, len(syntactic_dims))\n",
    "    styled_model.to(device)\n",
    "    \n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    style_predictor_head = nn.Linear(styled_model.hidden_size, len(syntactic_dims) + len(prag_style_vocab)).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(list(styled_model.parameters()) + list(style_predictor_head.parameters()), lr=LEARNING_RATE)\n",
    "    num_training_steps = len(train_loader) * epochs\n",
    "    lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # Training Loop\n",
    "    styled_model.train()\n",
    "    style_predictor_head.train()\n",
    "    global_step = 0\n",
    "    epsilon = 1e-8 \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            syntactic_vec = batch[\"syntactic_vec\"].to(device)\n",
    "            pragmatic_tags = batch[\"prag_vec\"].to(device)\n",
    "\n",
    "            outputs = styled_model(input_ids, attention_mask, labels, syntactic_vec)\n",
    "            \n",
    "            L_lm = outputs[\"loss\"]\n",
    "            style_prefix_hidden_state = outputs[\"last_hidden_state\"][:, 0] \n",
    "            \n",
    "            aux_preds = style_predictor_head(style_prefix_hidden_state.float())\n",
    "            pred_syntactic = aux_preds[:, :len(syntactic_dims)]\n",
    "            pred_pragmatic = aux_preds[:, len(syntactic_dims):]\n",
    "            \n",
    "            L_recon = mse_loss_fn(pred_syntactic, syntactic_vec)\n",
    "            L_style_cls = bce_loss_fn(pred_pragmatic, pragmatic_tags.float())\n",
    "            \n",
    "            L_total = L_lm + lambda_recon * (L_recon / (L_recon.detach() + epsilon)) \\\n",
    "                   + lambda_style * (L_style_cls / (L_style_cls.detach() + epsilon))\n",
    "            \n",
    "            L_total.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\"L_total\": L_total.item(), \"L_lm\": L_lm.item()})\n",
    "            \n",
    "    # Save\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    styled_model.peft_model.save_pretrained(output_dir / \"lora\") # type: ignore\n",
    "    torch.save(styled_model.style_encoder.state_dict(), output_dir / \"style_encoder.pt\")\n",
    "    torch.save(style_predictor_head.state_dict(), output_dir / \"style_predictor_head.pt\")\n",
    "    tokenizer.save_pretrained(output_dir / \"tokenizer\")\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# 执行训练\n",
    "syntactic_dims_10 = sorted(list(set(rule_to_10dim.values())))\n",
    "syntactic_dims_18 = sorted(list(set(rule_to_18dim.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9e847-6350-4257-89a0-4d75caeea961",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_experiment(data_path_10d, syntactic_dims_10, OUTPUTS_DIR / \"styled-qwen-10d\")\n",
    "\n",
    "run_training_experiment(data_path_18d, syntactic_dims_18, OUTPUTS_DIR / \"styled-qwen-18d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c8062",
   "metadata": {},
   "source": [
    "## Construct Character Style Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499c7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "\n",
    "Pragmatic_Muice = \"./outputs/pragmatic/muice.jsonl\"\n",
    "Pragmatic_ayaka = \"./outputs/pragmatic/ayaka.jsonl\"\n",
    "Pragmatic_zhongli = \"./outputs/pragmatic/zhongli.jsonl\"\n",
    "Pragmatic_hutao = \"./outputs/pragmatic/hutao.jsonl\"\n",
    "Pragmatic_haruhi = \"./outputs/pragmatic/haruhi.jsonl\"\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "class PCFGItem(TypedDict):\n",
    "    response: str\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "def read_pcfg_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None, top_n: int = 5) -> list[str]:\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_item: RawPCFGItem = json.loads(line)\n",
    "            raw_items.append(raw_item)\n",
    "\n",
    "    # list[dict[str, float]] -> dict[str, float] -> list[str]\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item[\"pragmatic_styles\"]\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        threshold = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > threshold:\n",
    "                final_styles.append(key)\n",
    "        \n",
    "        items.extend(final_styles)\n",
    "\n",
    "    # 返回 Top N 风格\n",
    "    style_counter = Counter(items)\n",
    "    most_common_styles = [style for style, _ in style_counter.most_common(top_n)]\n",
    "\n",
    "    return most_common_styles\n",
    "\n",
    "pcfg_muice_items = read_pcfg_jsonl_file(Path(Pragmatic_Muice), 0.4)\n",
    "pcfg_ayaka_items = read_pcfg_jsonl_file(Path(Pragmatic_ayaka), 0.4)\n",
    "pcfg_zhongli_items = read_pcfg_jsonl_file(Path(Pragmatic_zhongli), 0.4)\n",
    "pcfg_hutao_items = read_pcfg_jsonl_file(Path(Pragmatic_hutao), 0.4)\n",
    "pcfg_haruhi_items = read_pcfg_jsonl_file(Path(Pragmatic_haruhi), 0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1217bb",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b92fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Optional, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, Qwen3ForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# === Collect all output 对齐区（schema/命名对齐 Collect_all_output.ipynb）===\n",
    "TEST_FILE = Path(\"./data/neutral_sentences_eval.jsonl\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data: list[str] = [json.loads(line)[\"neutral\"] for line in f.readlines()]\n",
    "\n",
    "class InferenceResult(TypedDict):\n",
    "    model: str\n",
    "    neutral: str\n",
    "    output: str\n",
    "    character: str\n",
    "\n",
    "results: list[InferenceResult] = []\n",
    "\n",
    "# 硬编码关键词\n",
    "KEYWORDS_MAP = {\n",
    "    \"Muice\": [\"喵\", \"沐沐\", \"AI\", \"恼\", \"沐雪\", \"女孩子\", \"~\", \"⭐\", \"不行\", \"聊天\", \"呀\", \"可爱\", \"才\", \"叫\", \"唔\", \"谁\", \"不会\", \"吃\", \"睡觉\", \"笨蛋\", \"答\", \"谢谢\", \"把\", \"即\", \"吧\"],\n",
    "    \"Ayaka\": [\"稻妻国\", \"神里家\", \"稻妻\", \"大小姐\", \"家族\", \"传统\", \"奉行\", \"文化\", \"人民\", \"眼狩令\", \"神\", \"当地\", \"社\", \"舞蹈\", \"美丽\", \"茶道\", \"神社\", \"祭典\", \"眼\", \"美食\", \"继承\", \"剑术\", \"国家\", \"将军\", \"责任\"],\n",
    "    \"Zhongli\": ['岩石', '岩', '璃月', '力', '璃', '契约', '炼金术', '月', '盐', '帝君', '魔神', '操控', '王', '并非', '岩王', '大地', '封印', '作战', '掌握', '大陆', '学问', '研究', '七星', '客卿', '岩元素'],\n",
    "    \"Hutao\": ['往生堂', '嘿嘿', '嘻嘻', '可是', '堂主', '哎呀呀', '哦哦哦', '宝藏', '惊喜', '诗歌', '可不是', '灵魂', '胡桃', '神秘', '生死', '谜题', '哈哈哈', '不过', '有趣', '亡灵', '秘密', '意想不到', '巫师', '哇', '奇妙'],\n",
    "    \"Haruhi\": ['团', 'SOS', '阿虚', '社团', '哼', '事件', '学校', '超自然', '朝比奈', '文化祭', '创意', '吸引', '古泉', '电影', '创新', '组织', '实玖瑠', '当然', '与众不同', '主题', '加入', '束缚', '凉宫', '团长', '外星人'],\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class CharacterProfile:\n",
    "    name: str\n",
    "    syntactic_vec: dict[str, float]\n",
    "    pragmatic_styles: list[str]\n",
    "    lexical_keywords: list[str]\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "def read_pcfg_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None, top_n: int = 5) -> list[str]:\n",
    "    \"\"\"从 jsonl 文件中提取 Top N 语用风格 (Collect_all_output 实现)\"\"\"\n",
    "    if not jsonl_file.exists():\n",
    "        print(f\"Warning: Pragmatic file not found: {jsonl_file}\")\n",
    "        return []\n",
    "\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_items.append(json.loads(line))\n",
    "\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item.get(\"pragmatic_styles\", [])\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        thresh = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > thresh:\n",
    "                final_styles.append(key)\n",
    "        items.extend(final_styles)\n",
    "\n",
    "    style_counter = Counter(items)\n",
    "    most_common_styles = [style for style, _ in style_counter.most_common(top_n)]\n",
    "    return most_common_styles\n",
    "\n",
    "def load_profiles(rule_mapping: dict[str, str]) -> list[CharacterProfile]:\n",
    "    \"\"\"严格对齐 Collect_all_output 的 profile 结构：关键词硬编码 + 语用 topN + 句法向量由 rule_mapping 动态提取\"\"\"\n",
    "    profiles: list[CharacterProfile] = []\n",
    "    char_map = [\n",
    "        (\"Muice\", \"沐雪\", \"muice\"),\n",
    "        (\"Ayaka\", \"神里绫华\", \"ayaka\"),\n",
    "        (\"Zhongli\", \"钟离\", \"zhongli\"),\n",
    "        (\"Hutao\", \"胡桃\", \"hutao\"),\n",
    "        (\"Haruhi\", \"凉宫春日\", \"haruhi\"),\n",
    "    ]\n",
    "    for en, zh, key in char_map:\n",
    "        cons_path = Path(\"./outputs/cons\") / f\"{key}.json\"\n",
    "        syn_vec: dict[str, float] = load_and_extract_pcfg(cons_path, rule_mapping) if cons_path.exists() else {}\n",
    "        prag_path = Path(\"./outputs/pragmatic\") / f\"{key}.jsonl\"\n",
    "        prag_styles = read_pcfg_jsonl_file(prag_path, threshold=0.4, top_n=5)\n",
    "        keywords = KEYWORDS_MAP.get(en, [])\n",
    "        profiles.append(CharacterProfile(name=zh, syntactic_vec=syn_vec, pragmatic_styles=prag_styles, lexical_keywords=keywords))\n",
    "    return profiles\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim // 2, out_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, syntactic_vec: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(syntactic_vec)\n",
    "\n",
    "# === 这些全局变量将由 _load_styled_model() 注入（避免 Pylance 未定义告警） ===\n",
    "tokenizer: Any = None\n",
    "model: Any = None\n",
    "style_encoder: Any = None\n",
    "syntactic_dims: list[str] = []\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_styled_response(\n",
    "    neutral_sentence: str,\n",
    "    syntactic_vec: dict[str, float],\n",
    "    character_name: str = \"Ayaka\",\n",
    "    lexical_keywords: list[str] = list(),\n",
    "    pragmatic_styles: list[str] = list(),\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    repetition_penalty: float = 1.3,\n",
    "    max_new_tokens: int = 100,\n",
    " ) -> str:\n",
    "    \"\"\"输入中性句和风格向量，生成风格化响应（实现/提示格式对齐 Collect_all_output.ipynb）\"\"\"\n",
    "    assert tokenizer is not None and model is not None and style_encoder is not None, \"Model/tokenizer/style_encoder 未加载\"\n",
    "    assert syntactic_dims, \"syntactic_dims 未初始化（请先调用 _load_styled_model）\"\n",
    "\n",
    "    lexical_keywords = lexical_keywords or []\n",
    "    pragmatic_styles = pragmatic_styles or []\n",
    "\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatics}\\n\"\n",
    "        f\"Keywords: {keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    syntactic_tensor = torch.tensor(\n",
    "        [syntactic_vec.get(dim, 0.0) for dim in syntactic_dims],\n",
    "        dtype=torch.float32, device=DEVICE,\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    enc_dtype = next(style_encoder.parameters()).dtype\n",
    "    style_emb = style_encoder(syntactic_tensor.to(enc_dtype)).to(model.dtype)\n",
    "    style_prefix = style_emb.unsqueeze(1)\n",
    "\n",
    "    token_embeds = model.get_input_embeddings()(input_ids)  # type: ignore\n",
    "    inputs_embeds = torch.cat([style_prefix, token_embeds], dim=1)\n",
    "\n",
    "    prefix_mask = torch.ones((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=new_attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 逐模型批量推理 ===\n",
    "import gc\n",
    "\n",
    "MODEL_PATH = Path(\"../Models/Qwen3-1.7B/\")\n",
    "\n",
    "SAVE_DIR_10D = OUTPUTS_DIR / \"styled-qwen-10d\"\n",
    "SAVE_DIR_18D = OUTPUTS_DIR / \"styled-qwen-18d\"\n",
    "\n",
    "def _check_model_dir(model_save_dir: Path) -> None:\n",
    "    required = [model_save_dir / \"tokenizer\", model_save_dir / \"lora\", model_save_dir / \"style_encoder.pt\"]\n",
    "    if any(not p.exists() for p in required):\n",
    "        raise FileNotFoundError(f\"模型目录不完整: {model_save_dir}\")\n",
    "\n",
    "def _load_styled_model(save_dir: Path, syntactic_dims_: list[str]):\n",
    "    global tokenizer, model, style_encoder, syntactic_dims\n",
    "    _check_model_dir(save_dir)\n",
    "\n",
    "    syntactic_dims = syntactic_dims_\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_dir / \"tokenizer\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    base_model = Qwen3ForCausalLM.from_pretrained(MODEL_PATH, dtype=torch.bfloat16)\n",
    "    model = PeftModel.from_pretrained(base_model, save_dir / \"lora\")\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    hidden_size = base_model.config.hidden_size\n",
    "    style_encoder = StyleEncoder(len(syntactic_dims), hidden_size)\n",
    "    style_encoder.load_state_dict(torch.load(save_dir / \"style_encoder.pt\", map_location=DEVICE))\n",
    "    style_encoder.to(DEVICE)\n",
    "    style_encoder.eval()\n",
    "\n",
    "    print(f\"✅ Styled model loaded: {save_dir}\")\n",
    "    return base_model\n",
    "\n",
    "def _unload_model(base_model):\n",
    "    global tokenizer, model, style_encoder\n",
    "    try:\n",
    "        del tokenizer\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del model\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del style_encoder\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        del base_model\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "# === 运行 10d ===\n",
    "profiles_10d = load_profiles(rule_to_10dim)\n",
    "base_model_10d = _load_styled_model(SAVE_DIR_10D, syntactic_dims_10)\n",
    "\n",
    "def batch_inference(profile: CharacterProfile, model_name: str):\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_styled_response(\n",
    "            item,\n",
    "            profile.syntactic_vec,\n",
    "            character_name=profile.name,\n",
    "            lexical_keywords=profile.lexical_keywords,\n",
    "            pragmatic_styles=profile.pragmatic_styles,\n",
    "        )\n",
    "        results.append(InferenceResult(model=model_name, neutral=item, output=output, character=profile.name))\n",
    "\n",
    "for prof in profiles_10d:\n",
    "    batch_inference(prof, \"10d-model\")\n",
    "\n",
    "_unload_model(base_model_10d)\n",
    "\n",
    "# === 运行 18d ===\n",
    "profiles_18d = load_profiles(rule_to_18dim)\n",
    "base_model_18d = _load_styled_model(SAVE_DIR_18D, syntactic_dims_18)\n",
    "\n",
    "for prof in profiles_18d:\n",
    "    batch_inference(prof, \"18d-model\")\n",
    "\n",
    "_unload_model(base_model_18d)\n",
    "\n",
    "print(f\"✅ Done. Total inference items: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a7f85",
   "metadata": {},
   "source": [
    "## Export Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd39bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: evaluate\\outputs\\DimensionDissolution\\batch_run_result.jsonl (n=1500)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "OUTPUT_PATH = Path(\"./outputs/DimensionDissolution/batch_run_result.jsonl\")\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 清空旧文件，避免重复累计\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "for item in results:\n",
    "    with open(OUTPUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Saved: {OUTPUT_PATH} (n={len(results)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c11f612",
   "metadata": {},
   "source": [
    "## Automated Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b858889",
   "metadata": {},
   "source": [
    "### Load Batch Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860e25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing_extensions import TypedDict\n",
    "import json\n",
    "\n",
    "class InferenceItem(TypedDict):\n",
    "    model: str\n",
    "    neutral: str\n",
    "    output: str\n",
    "    character: str\n",
    "\n",
    "# 读取本笔记本导出的消融实验输出（schema 与 Collect_all_output 一致）\n",
    "BATCH_INFERENCE_FILE = Path(\"./outputs/DimensionDissolution/batch_run_result.jsonl\")\n",
    "\n",
    "batch_inference_items: list[InferenceItem] = []\n",
    "\n",
    "with open(BATCH_INFERENCE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item: InferenceItem = json.loads(line.strip())\n",
    "        item[\"output\"] = item[\"output\"].split(\"</think>\")[1].strip() if \"</think>\" in item[\"output\"] else item[\"output\"]\n",
    "        batch_inference_items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd07cd",
   "metadata": {},
   "source": [
    "### Style Consistency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1932a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "BACKBONE_PATH = str(Path('../Models/chinese-roberta-wwm-ext').resolve())\n",
    "CHECKPOINT_PATH = Path('outputs/style-classifier')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CharacterStyleClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name: str, embed_dim: int = 768, proj_dim: int = 256, num_roles: int = 6, dropout: float = 0.4):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(backbone_name, output_hidden_states=False)\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        assert self.hidden_size == embed_dim, f\"Backbone hidden_size {self.hidden_size} != embed_dim {embed_dim}\"\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.classifier = nn.Linear(proj_dim, num_roles)\n",
    "\n",
    "    def mean_pooling(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_hidden = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        mean_pooled = sum_hidden / sum_mask\n",
    "        return mean_pooled\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state\n",
    "        sent_emb = self.mean_pooling(last_hidden, attention_mask)\n",
    "        proj = self.proj(sent_emb)\n",
    "        logits = self.classifier(proj)\n",
    "        return logits, proj\n",
    "\n",
    "def load_checkpoint(path: str, device='cpu') -> Tuple[nn.Module, 'Tokenizer', LabelEncoder, Dict[str, np.ndarray]]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    with open(os.path.join(path, 'label_encoder.json'), 'r', encoding='utf-8') as f:\n",
    "        le_json = json.load(f)\n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array(le_json['classes'])\n",
    "\n",
    "    backbone_hidden = AutoModel.from_pretrained(path).config.hidden_size\n",
    "    model = CharacterStyleClassifier(path, embed_dim=backbone_hidden, proj_dim=256, num_roles=len(le.classes_))\n",
    "    chk = torch.load(os.path.join(path, 'head.pt'), map_location=device)\n",
    "    model.proj.load_state_dict(chk['proj_state'])\n",
    "    model.classifier.load_state_dict(chk['classifier_state'])\n",
    "    centers_npz = np.load(os.path.join(path, 'role_centers.npz'))\n",
    "    role_centers = {k: centers_npz[k] for k in centers_npz.files}\n",
    "    return model.to(device), tokenizer, le, role_centers\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_role_centers(model: nn.Module, dataloader: DataLoader, label_encoder: LabelEncoder, device='cpu') -> Dict[str, np.ndarray]:\n",
    "    model.eval()\n",
    "    accum: Dict[int, List[np.ndarray]] = {}\n",
    "    for batch in tqdm(dataloader, desc=\"Computing centers\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].cpu().numpy()\n",
    "        _, embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emb_np = embeddings.detach().cpu().numpy()\n",
    "        for lbl, e in zip(labels, emb_np):\n",
    "            accum.setdefault(int(lbl), []).append(e)\n",
    "    role_centers = {}\n",
    "    for lbl, vecs in accum.items():\n",
    "        avg = np.mean(np.stack(vecs, axis=0), axis=0)\n",
    "        role_name = label_encoder.inverse_transform([lbl])[0]\n",
    "        role_centers[role_name] = avg\n",
    "    return role_centers\n",
    "\n",
    "def get_style_score(model: nn.Module, tokenizer, text: str, role_center: np.ndarray, device='cpu', max_length=128) -> float:\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n",
    "    input_ids = enc['input_ids'].to(device)\n",
    "    attention_mask = enc['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        _, emb = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emb_np = emb.detach().cpu().numpy()[0]\n",
    "    num = float(np.dot(emb_np, role_center))\n",
    "    den = float(np.linalg.norm(emb_np) * np.linalg.norm(role_center) + 1e-9)\n",
    "    return num / den\n",
    "\n",
    "model, tokenizer, label_encoder, role_centers = load_checkpoint(str(CHECKPOINT_PATH), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af456de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Style: 100%|██████████| 1500/1500 [00:14<00:00, 101.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Style Consistency Scores:\n",
      "Model: 10d-model, Average Style Score: 0.6581 (n=750)\n",
      "Model: 18d-model, Average Style Score: 0.6644 (n=750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in tqdm(batch_inference_items, desc=\"Evaluating Style\"):\n",
    "    character = item['character']\n",
    "    output_text = item['output']\n",
    "\n",
    "    if not character in role_centers:\n",
    "        print(f\"Warning: Character '{character}' not found in role centers. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    center = role_centers[character]\n",
    "    score = get_style_score(model, tokenizer, output_text, center, device=device)\n",
    "\n",
    "    results.append({\n",
    "        **item,\n",
    "        \"style_score\": score,\n",
    "    })\n",
    "\n",
    "model_scores = {}\n",
    "for res in results:\n",
    "    m_name = res['model']\n",
    "    if m_name not in model_scores:\n",
    "        model_scores[m_name] = []\n",
    "    model_scores[m_name].append(res['style_score'])\n",
    "\n",
    "print(\"\\nStyle Consistency Scores:\")\n",
    "for m_name, scores in model_scores.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    print(f\"Model: {m_name}, Average Style Score: {avg_score:.4f} (n={len(scores)})\")\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24152b5",
   "metadata": {},
   "source": [
    "### Semantic Preservation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba24fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "MODEL_PATH = \"../Models/bge-large-zh-v1.5\"\n",
    "semantic_model = SentenceTransformer(MODEL_PATH)\n",
    "\n",
    "def compute_semantic_similarity(neutral_text, stylized_text):\n",
    "    embeddings = semantic_model.encode([neutral_text, stylized_text], normalize_embeddings=True)\n",
    "    similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6026d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Semantic: 100%|██████████| 1500/1500 [11:08<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Preservation + Style Scores(SEM_THRESHOLD=0.75):\n",
      "================================================================================\n",
      "Model: 10d-model\n",
      "  - Character: 凉宫春日       | Sem: 0.8511 (n=150)\n",
      "  - Character: 沐雪         | Sem: 0.8881 (n=150)\n",
      "  - Character: 神里绫华       | Sem: 0.8605 (n=150)\n",
      "  - Character: 胡桃         | Sem: 0.8575 (n=150)\n",
      "  - Character: 钟离         | Sem: 0.8915 (n=150)\n",
      "  >> Avg Semantic: 0.8698\n",
      "  >> Avg Style  : 0.6581\n",
      "  >> Avg H-Score: 0.7088\n",
      "  >> Avg Valid Style: 0.5877\n",
      "--------------------------------------------------------------------------------\n",
      "Model: 18d-model\n",
      "  - Character: 凉宫春日       | Sem: 0.8537 (n=150)\n",
      "  - Character: 沐雪         | Sem: 0.8915 (n=150)\n",
      "  - Character: 神里绫华       | Sem: 0.8799 (n=150)\n",
      "  - Character: 胡桃         | Sem: 0.8445 (n=150)\n",
      "  - Character: 钟离         | Sem: 0.8953 (n=150)\n",
      "  >> Avg Semantic: 0.8730\n",
      "  >> Avg Style  : 0.6644\n",
      "  >> Avg H-Score: 0.7139\n",
      "  >> Avg Valid Style: 0.5896\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(results, desc=\"Evaluating Semantic\"):\n",
    "    neutral_text = item['neutral']\n",
    "    stylized_text = item['output']\n",
    "    score = compute_semantic_similarity(neutral_text, stylized_text)\n",
    "    item['semantic_score'] = score\n",
    "\n",
    "# === 计算 H-Score 与有效风格得分 ===\n",
    "SEM_THRESHOLD = 0.75  # 语义门槛\n",
    "\n",
    "for item in results:\n",
    "    style_score = item.get('style_score', 0.0)\n",
    "    sem_score = item.get('semantic_score', 0.0)\n",
    "    h_score = (2 * style_score * sem_score) / (style_score + sem_score + 1e-9)\n",
    "    penalty_factor = 1.0 if sem_score > SEM_THRESHOLD else 0.1\n",
    "    penalized_style = style_score * penalty_factor\n",
    "    item['h_score'] = h_score\n",
    "    item['valid_style_score'] = penalized_style\n",
    "\n",
    "model_metrics = {}\n",
    "for item in results:\n",
    "    m_name = item['model']\n",
    "    char_name = item['character']\n",
    "    sem = item['semantic_score']\n",
    "    sty = item['style_score']\n",
    "    h = item['h_score']\n",
    "    vsty = item['valid_style_score']\n",
    "    if m_name not in model_metrics:\n",
    "        model_metrics[m_name] = {\n",
    "            'semantic': [],\n",
    "            'style': [],\n",
    "            'h_score': [],\n",
    "            'valid_style': [],\n",
    "            'per_char': {},\n",
    "        }\n",
    "    model_metrics[m_name]['semantic'].append(sem)\n",
    "    model_metrics[m_name]['style'].append(sty)\n",
    "    model_metrics[m_name]['h_score'].append(h)\n",
    "    model_metrics[m_name]['valid_style'].append(vsty)\n",
    "    if char_name not in model_metrics[m_name]['per_char']:\n",
    "        model_metrics[m_name]['per_char'][char_name] = []\n",
    "    model_metrics[m_name]['per_char'][char_name].append(sem)\n",
    "\n",
    "print(f\"\\nSemantic Preservation + Style Scores(SEM_THRESHOLD={SEM_THRESHOLD}):\")\n",
    "print(\"=\" * 80)\n",
    "for m_name, metrics in model_metrics.items():\n",
    "    avg_sem = np.mean(metrics['semantic'])\n",
    "    avg_style = np.mean(metrics['style'])\n",
    "    avg_h = np.mean(metrics['h_score'])\n",
    "    avg_vsty = np.mean(metrics['valid_style'])\n",
    "    print(f\"Model: {m_name}\")\n",
    "    sorted_chars = sorted(metrics['per_char'].keys())\n",
    "    for char_name in sorted_chars:\n",
    "        scores = metrics['per_char'][char_name]\n",
    "        avg_char_sem = np.mean(scores)\n",
    "        print(f\"  - Character: {char_name:<10} | Sem: {avg_char_sem:.4f} (n={len(scores)})\")\n",
    "    print(f\"  >> Avg Semantic: {avg_sem:.4f}\")\n",
    "    print(f\"  >> Avg Style  : {avg_style:.4f}\")\n",
    "    print(f\"  >> Avg H-Score: {avg_h:.4f}\")\n",
    "    print(f\"  >> Avg Valid Style: {avg_vsty:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2ec70",
   "metadata": {},
   "source": [
    "### Generate 2D Scatter Plot/Comprehensive Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_results, x=\"semantic_score\", y=\"style_score\", hue=\"model\", alpha=0.6)\n",
    "plt.xlabel(\"Semantic Score\")\n",
    "plt.ylabel(\"Style Score (Raw)\")\n",
    "plt.title(\"Semantic vs Style Trade-off\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "leaderboard_rows = []\n",
    "for m_name, metrics in model_metrics.items():\n",
    "    semantic_score = round(np.mean(metrics[\"semantic\"]), 4)\n",
    "    style_score = round(np.mean(metrics[\"style\"]), 4)\n",
    "    h_score = round(np.mean(metrics[\"h_score\"]), 4)\n",
    "    valid_style_score = round(np.mean(metrics[\"valid_style\"]), 4)\n",
    "    leaderboard_rows.append({\n",
    "        \"model\": m_name,\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"style_score_raw\": style_score,\n",
    "        \"h_score\": h_score,\n",
    "        \"valid_style_score\": valid_style_score,\n",
    "    })\n",
    "\n",
    "df_leaderboard = pd.DataFrame(leaderboard_rows).sort_values(by=[\"valid_style_score\"], ascending=False)\n",
    "df_leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
