{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdfa03e",
   "metadata": {},
   "source": [
    "# Construct PCFG Models for Three Different Style Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b762e",
   "metadata": {},
   "source": [
    "## Load Style Space (Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8a07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muice Responses: 2721\n",
      "Ayaka Responses: 991\n",
      "Zhongli Responses: 359\n",
      "Hutao Responses: 591\n",
      "Haruhi Responses: 770\n",
      "PsyDTCorpus Responses: 89652\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "MUICE_PATH = Path(\"../Dataset/Muice/train.jsonl\")\n",
    "HARUHI_PATH = Path(r\"..\\Dataset\\Haruhi\\Haruhi_clean.jsonl\")\n",
    "PSYDC_PATH1 = Path(r\"..\\Dataset\\PsyDTCorpus\\PsyDTCorpus_train_mulit_turn_packing.json\")\n",
    "PSYDC_PATH2 = Path(r\"..\\Dataset\\PsyDTCorpus\\PsyDTCorpus_test_single_turn_split.json\")\n",
    "\n",
    "# Check if Dataset exist\n",
    "assert MUICE_PATH.is_file(), \"请确保 Muice Dataset 的路径正确！\"\n",
    "assert HARUHI_PATH.is_file(), \"请确保 Haruhi Dataset 的路径正确！\"\n",
    "assert PSYDC_PATH1.is_file(), \"请确保 PsyDTCorpus Dataset 的路径正确！\"\n",
    "\n",
    "# Load Muice Dataset\n",
    "dataset_lines = MUICE_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "muice_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "    muice_responses.append(item[\"Response\"])\n",
    "\n",
    "# 出于计算效率，只取 80% 进行计算\n",
    "muice_responses = muice_responses[:int(len(muice_responses)*0.8)]\n",
    "\n",
    "# Load Haruhi Dataset\n",
    "dataset_lines = HARUHI_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "ayaka_responses: List[str] = []\n",
    "zhongli_responses: List[str] = []\n",
    "hutao_responses: List[str] = []\n",
    "haruhi_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "\n",
    "    # 只提取单一角色的训练集\n",
    "    if item[\"agent_role\"] == \"神里绫华\":\n",
    "        ayaka_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"钟离\":\n",
    "        zhongli_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"胡桃\":\n",
    "        hutao_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role_name_en\"] == \"haruhi\":\n",
    "        haruhi_responses.append(item[\"agent_response\"])\n",
    "\n",
    "# 出于计算效率，只取 70% 进行计算\n",
    "ayaka_responses = ayaka_responses[:int(len(ayaka_responses)*0.7)]\n",
    "zhongli_responses = zhongli_responses[:int(len(zhongli_responses)*0.7)]\n",
    "hutao_responses = hutao_responses[:int(len(hutao_responses)*0.7)]\n",
    "haruhi_responses = haruhi_responses[:int(len(haruhi_responses)*0.7)]\n",
    "\n",
    "# Load PsyDTCorpus Dataset\n",
    "psydc_part1 = json.loads(PSYDC_PATH1.read_text(encoding=\"utf-8\"))\n",
    "psydc_part2 = json.loads(PSYDC_PATH2.read_text(encoding=\"utf-8\"))\n",
    "psydc: list[dict] = psydc_part1 + psydc_part2\n",
    "psydc_responses: List[str] = []\n",
    "\n",
    "for item in psydc:\n",
    "    messages: list[dict[str, str]] = item[\"messages\"]\n",
    "    for index in range(2, len(messages), 2):\n",
    "        message = messages[index]\n",
    "        assert message[\"role\"] == \"assistant\", message\n",
    "        psydc_responses.append(message[\"content\"])\n",
    "\n",
    "# 取 70 %作为训练集\n",
    "psydc_responses = psydc_responses[:int(len(psydc_responses)*0.7)]\n",
    "\n",
    "# 输出所有训练集的长度\n",
    "\n",
    "print(f\"Muice Responses: {len(muice_responses)}\")\n",
    "print(f\"Ayaka Responses: {len(ayaka_responses)}\")\n",
    "print(f\"Zhongli Responses: {len(zhongli_responses)}\")\n",
    "print(f\"Hutao Responses: {len(hutao_responses)}\")\n",
    "print(f\"Haruhi Responses: {len(haruhi_responses)}\")\n",
    "print(f\"PsyDTCorpus Responses: {len(psydc_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24145a82",
   "metadata": {},
   "source": [
    "## Call HanLP for Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc945568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanlp_restful import HanLPClient\n",
    "from hanlp_common.document import Document\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=getenv(\"HANLP_API_KEY\", None), language='zh')  # type: ignore\n",
    "\n",
    "def limit_sentense_length(text: str, max_length: int = 150) -> list[str]:\n",
    "    if len(text) < 150:\n",
    "        return [text]\n",
    "    \n",
    "    # print(\"Warning: 句子超出了长度上限: \", text)\n",
    "\n",
    "    # 按照标点符号进行分割\n",
    "    sentences = re.split(r'([。！？；，,.!?;])', text)\n",
    "    chunks:list[str] = []\n",
    "    temp = ''\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        chunk = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "        if len(temp) + len(chunk) < max_length:\n",
    "            temp += chunk\n",
    "        else:\n",
    "            chunks.append(temp)\n",
    "            temp = chunk\n",
    "    if temp:\n",
    "        chunks.append(temp)\n",
    "    return chunks\n",
    "\n",
    "def split_texts(texts: list[str]) -> list[str]:\n",
    "    \"\"\"将文本列表中的每个文本分割成更小的句子\"\"\"\n",
    "    all_sentences = []\n",
    "    for text in texts:\n",
    "        sentences = limit_sentense_length(text)\n",
    "        all_sentences.extend(sentences)\n",
    "    return all_sentences\n",
    "\n",
    "def constituency_parsing_safe(texts: list[str], max_batch_num: int = 250, max_chars_per_batch: int = 15000, interval: int = 35) -> List[Document]:\n",
    "    \"\"\"对文本进行分词，同时限制每一批总字符数\"\"\"\n",
    "    all_docs = []\n",
    "    current_batch = []\n",
    "    current_length = 0\n",
    "    batch_id = 1\n",
    "\n",
    "    for text in texts:\n",
    "        text_len = len(text)\n",
    "\n",
    "        # 如果加上这个句子会超出限制，则先处理已有批次\n",
    "        if current_length + text_len > max_chars_per_batch or len(current_batch) + 1 > max_batch_num:\n",
    "            print(f\"Processing batch {batch_id} (Total chars: {current_length})...\", end='')\n",
    "            doc = HanLP.parse(current_batch, tasks=['pos', 'con'])\n",
    "            all_docs.append(doc)\n",
    "            print(\"done.\")\n",
    "            sleep(interval)\n",
    "\n",
    "            batch_id += 1\n",
    "            # 重置 batch\n",
    "            current_batch = [text]\n",
    "            current_length = text_len\n",
    "        else:\n",
    "            current_batch.append(text)\n",
    "            current_length += text_len\n",
    "\n",
    "    # 最后一批也别忘记\n",
    "    if current_batch:\n",
    "        print(f\"Processing batch {batch_id} (Total chars: {current_length})...\", end='')\n",
    "        doc = HanLP.parse(current_batch, tasks=['pos', 'con'])\n",
    "        all_docs.append(doc)\n",
    "        print(\"done.\")\n",
    "\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e50b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (Total chars: 9118)...done.\n",
      "Processing batch 2 (Total chars: 8006)...done.\n",
      "Processing batch 3 (Total chars: 13423)...done.\n",
      "Processing batch 4 (Total chars: 12001)...done.\n"
     ]
    }
   ],
   "source": [
    "muice_responses_split = split_texts(muice_responses)\n",
    "ayaka_responses_split = split_texts(ayaka_responses)\n",
    "zhongli_responses_split = split_texts(zhongli_responses)\n",
    "hutao_responses_split = split_texts(hutao_responses)\n",
    "haruhi_responses_split = split_texts(haruhi_responses)\n",
    "psydc_responses_split = split_texts(psydc_responses)\n",
    "\n",
    "muice_cons = constituency_parsing_safe(muice_responses_split, 200, interval=1)\n",
    "ayaka_cons = constituency_parsing_safe(ayaka_responses_split, 200, interval=1)\n",
    "zhongli_cons = constituency_parsing_safe(zhongli_responses_split, 200, interval=1)\n",
    "hutao_cons = constituency_parsing_safe(hutao_responses_split, 200, interval=1)\n",
    "haruhi_cons = constituency_parsing_safe(haruhi_responses_split, 200, interval=1)\n",
    "psydc_cons = constituency_parsing_safe(psydc_responses_split, 200, interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa259eed",
   "metadata": {},
   "source": [
    "## Archive Raw Results to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_cons_to_json(cons_docs: List[Document], character: str):\n",
    "    cons_json = [doc.to_dict() for doc in cons_docs]\n",
    "    file_path = f'./outputs/cons/{character}.json'\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cons_json, f, ensure_ascii=False)\n",
    "\n",
    "save_cons_to_json(muice_cons, \"muice\")\n",
    "save_cons_to_json(ayaka_cons, \"ayaka\")\n",
    "save_cons_to_json(zhongli_cons, \"zhongli\")\n",
    "save_cons_to_json(hutao_cons, \"hutao\")\n",
    "save_cons_to_json(haruhi_cons, \"haruhi\")\n",
    "save_cons_to_json(psydc_cons, \"psydc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ea133",
   "metadata": {},
   "source": [
    "## Calculate Log-Likelihood Ratio of Two Styles Based on Psydc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8396e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===./outputs/cons/muice_cons.json PCFG 产生式规则（按对数似然比排序） ===\n",
      "NP → NR                                       | freq=513   | P=0.0305 | PR=498.97 | LLR=3364.83\n",
      "INTJ → IJ                                       | freq=259   | P=0.8548 | PR=27928.32 | LLR=1756.39\n",
      "TOP → CP                                       | freq=396   | P=0.1415 | PR=506.86 | LLR=1549.77\n",
      "VP → VV                                       | freq=2924  | P=0.1439 | PR=5.83 | LLR=670.21\n",
      "CP → IP SP                                    | freq=1632  | P=0.3664 | PR=30.80 | LLR=585.91\n",
      "UCP → IP PU CP                                 | freq=61    | P=0.0685 | PR=6695.62 | LLR=497.26\n",
      "FLR → SP                                       | freq=84    | P=0.4615 | PR=16155.72 | LLR=471.29\n",
      "DNP → ADJP DEG                                 | freq=108   | P=0.0816 | PR=11.13 | LLR=468.22\n",
      "FLR → IJ                                       | freq=77    | P=0.4231 | PR=14354.33 | LLR=417.82\n",
      "PP → P LCP                                    | freq=158   | P=0.1300 | PR=16.71 | LLR=377.78\n",
      "IP → VP PU                                    | freq=83    | P=0.0076 | PR=156.98 | LLR=374.89\n",
      "NP → NN CC NN                                 | freq=100   | P=0.0059 | PR=0.96 | LLR=372.70\n",
      "IP → VP SP                                    | freq=75    | P=0.0069 | PR=183.71 | LLR=366.00\n",
      "CP → IP SP PU                                 | freq=131   | P=0.0294 | PR=141.82 | LLR=339.85\n",
      "IP → INTJ PU VP                               | freq=46    | P=0.0042 | PR=339.72 | LLR=327.11\n",
      "\n",
      "===./outputs/cons/haruhi_cons.json PCFG 产生式规则（按对数似然比排序） ===\n",
      "NP → NR                                       | freq=437   | P=0.0224 | PR=365.86 | LLR=2897.23\n",
      "VP → VP PU VP                                 | freq=1191  | P=0.0717 | PR=11.99 | LLR=1118.61\n",
      "NP → NN CC NN                                 | freq=1021  | P=0.0523 | PR=8.39 | LLR=705.24\n",
      "NP → NR NN                                    | freq=89    | P=0.0046 | PR=346.72 | LLR=680.55\n",
      "NP → PN                                       | freq=3946  | P=0.2020 | PR=2.20 | LLR=588.12\n",
      "DNP → NP DEG                                   | freq=2055  | P=0.7817 | PR=40.84 | LLR=514.59\n",
      "IP → ADVP PU NP VP                            | freq=404   | P=0.0475 | PR=25.00 | LLR=417.53\n",
      "NP → DNP NP                                   | freq=2192  | P=0.1122 | PR=5.04 | LLR=399.38\n",
      "CP → IP SP                                    | freq=272   | P=0.0713 | PR=6.00 | LLR=382.44\n",
      "NP → NN                                       | freq=6884  | P=0.3524 | PR=4.05 | LLR=375.65\n",
      "VP → VV IP                                    | freq=682   | P=0.0411 | PR=1.94 | LLR=363.39\n",
      "LCP → IP LC                                    | freq=21    | P=0.0587 | PR=15.29 | LLR=327.39\n",
      "VP → VP PU VP PU VP                           | freq=97    | P=0.0058 | PR=38.28 | LLR=276.63\n",
      "CP → CP                                       | freq=1566  | P=0.4107 | PR=25.54 | LLR=273.84\n",
      "PP → P LCP                                    | freq=195   | P=0.1540 | PR=19.79 | LLR=217.93\n",
      "\n",
      "===./outputs/cons/psydc_cons.json PCFG 产生式规则（按频率排序） ===\n",
      "NP → PN                                       | freq=405362 | P=0.3313\n",
      "NP → NN                                       | freq=384304 | P=0.3141\n",
      "ADVP → AD                                       | freq=294804 | P=0.9250\n",
      "IP → VP                                       | freq=272866 | P=0.3916\n",
      "IP → NP VP                                    | freq=222805 | P=0.3197\n",
      "VP → VV NP                                    | freq=180224 | P=0.1492\n",
      "VP → ADVP VP                                  | freq=159355 | P=0.1319\n",
      "VP → VV VP                                    | freq=127358 | P=0.1054\n",
      "VP → VV                                       | freq=108984 | P=0.0902\n",
      "NP → DNP NP                                   | freq=98342 | P=0.0804\n",
      "VP → VV IP                                    | freq=93577 | P=0.0774\n",
      "DNP → NP DEG                                   | freq=84483 | P=0.6032\n",
      "VP → VA                                       | freq=80360 | P=0.0665\n",
      "CP → IP DEC                                   | freq=73174 | P=0.3175\n",
      "PP → P NP                                     | freq=71280 | P=0.6010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Any, Optional, Literal\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "class PCFGExtractor:\n",
    "    def __init__(self):\n",
    "        self.rules_counter: Dict[str, Counter[Tuple[str, ...]]] = defaultdict(Counter)\n",
    "        self.name: str = \"\"\n",
    "        self.total_rules: int = 0\n",
    "\n",
    "    def load_trees(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        self.name = file_path\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def extract_rules_from_tree(self, tree: Any):\n",
    "        if not isinstance(tree, list) or len(tree) != 2:\n",
    "            return\n",
    "        lhs_symbol, rhs = tree\n",
    "        if isinstance(rhs, list) and all(isinstance(child, list) and len(child) == 2 for child in rhs):\n",
    "            rhs_symbols = tuple(child[0] for child in rhs)\n",
    "            self.rules_counter[lhs_symbol][rhs_symbols] += 1\n",
    "            self.total_rules += 1\n",
    "            for child in rhs:\n",
    "                self.extract_rules_from_tree(child)\n",
    "\n",
    "    def extract_from_data(self, data: List[Dict[str, Any]]):\n",
    "        for item in data:\n",
    "            for tree in item.get(\"con\", []):\n",
    "                self.extract_rules_from_tree(tree)\n",
    "\n",
    "    def build_pcfg(self) -> Dict[str, Dict[Tuple[str, ...], float]]:\n",
    "        pcfg_distribution = {}\n",
    "        for lhs_symbol, rhs_counter in self.rules_counter.items():\n",
    "            total_count = sum(rhs_counter.values())\n",
    "            pcfg_distribution[lhs_symbol] = {\n",
    "                rhs: count / total_count for rhs, count in rhs_counter.items()\n",
    "            }\n",
    "        return pcfg_distribution\n",
    "\n",
    "    def print_pcfg(\n",
    "        self,\n",
    "        pcfg: Dict[str, Dict[Tuple[str, ...], float]],\n",
    "        sort_by: Literal[\"freq\", \"prob\", \"llr\"] = 'freq',\n",
    "        top_k: Optional[int] = None,\n",
    "        baseline: Optional[\"PCFGExtractor\"] = None,\n",
    "        eps: float = 1e-5\n",
    "    ):\n",
    "        print(f\"==={self.name} PCFG 产生式规则（按{'频率' if sort_by == 'freq' else ('对数似然比' if sort_by == 'llr' else '概率')}排序） ===\")\n",
    "\n",
    "        all_rules = []\n",
    "        for lhs_symbol in self.rules_counter:\n",
    "            for rhs_symbols in self.rules_counter[lhs_symbol]:\n",
    "                freq = self.rules_counter[lhs_symbol][rhs_symbols]\n",
    "                prob = pcfg[lhs_symbol][rhs_symbols]\n",
    "\n",
    "                # PR / LLR\n",
    "                pr = llr = None\n",
    "                if baseline:\n",
    "                    base_freq = baseline.rules_counter.get(lhs_symbol, {}).get(rhs_symbols, 0)\n",
    "                    base_total = baseline.total_rules + eps\n",
    "                    base_prob = base_freq / base_total\n",
    "\n",
    "                    pr = (prob + eps) / (base_prob + eps)\n",
    "\n",
    "                    k1, n1 = freq + eps, self.total_rules + eps\n",
    "                    k2, n2 = base_freq + eps, base_total\n",
    "                    mu = (k1 + k2) / (n1 + n2)\n",
    "                    llr = 2 * (k1 * math.log(k1 / (n1 * mu)) + k2 * math.log(k2 / (n2 * mu)))\n",
    "\n",
    "                all_rules.append((lhs_symbol, rhs_symbols, freq, prob, pr, llr))\n",
    "\n",
    "        # 排序\n",
    "        if sort_by == 'llr':\n",
    "            all_rules.sort(key=lambda x: x[5] or 0, reverse=True)\n",
    "        else:\n",
    "            all_rules.sort(key=lambda x: x[2] if sort_by == 'freq' else x[3], reverse=True)\n",
    "\n",
    "        # 打印\n",
    "        for i, (lhs, rhs, freq, prob, pr, llr) in enumerate(all_rules):\n",
    "            if top_k is not None and i >= top_k:\n",
    "                break\n",
    "            rhs_str = ' '.join(rhs)\n",
    "            line = f\"{lhs} → {rhs_str:<40} | freq={freq:<5} | P={prob:.4f}\"\n",
    "            if baseline:\n",
    "                line += f\" | PR={pr:.2f} | LLR={llr:.2f}\"\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def build_and_print_pcfg(file_path: str, baseline: Optional[PCFGExtractor] = None):\n",
    "    extractor = PCFGExtractor()\n",
    "    trees_data = extractor.load_trees(file_path)\n",
    "    extractor.extract_from_data(trees_data)\n",
    "    pcfg = extractor.build_pcfg()\n",
    "\n",
    "    if baseline:\n",
    "        extractor.print_pcfg(pcfg, sort_by='llr', top_k=15, baseline=baseline)\n",
    "    else:\n",
    "        extractor.print_pcfg(pcfg, sort_by='freq', top_k=15, baseline=baseline)\n",
    "    print()\n",
    "\n",
    "# 1. 先加载基准语料（psydc）\n",
    "baseline_extractor = PCFGExtractor()\n",
    "baseline_data = baseline_extractor.load_trees(\"./outputs/cons/psydc.json\")\n",
    "baseline_extractor.extract_from_data(baseline_data)\n",
    "baseline_extractor.build_pcfg()  # 可选，但为了接口统一性\n",
    "\n",
    "# 2. 比较 muice / haruhi 与 psydc 的差异\n",
    "build_and_print_pcfg(\"./outputs/cons/muice.json\", baseline=baseline_extractor)\n",
    "build_and_print_pcfg(\"./outputs/cons/haruhi.json\", baseline=baseline_extractor)\n",
    "build_and_print_pcfg(\"./outputs/cons/psydc.json\")  # 自身基准不做对比\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}