{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817e1ab3",
   "metadata": {},
   "source": [
    "# Collect All Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a02a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Remote Jupyter Book.\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir(\"/root/OtakuLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad2611",
   "metadata": {},
   "source": [
    "## Load Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4d295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "TEST_FILE = Path(\"./data/neutral_sentences_eval.jsonl\")\n",
    "\n",
    "with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data: list[str] = [json.loads(line)[\"neutral\"] for line in f.readlines()]\n",
    "\n",
    "\n",
    "class InferenceResult(TypedDict):\n",
    "    model: str\n",
    "    neutral: str\n",
    "    output: str\n",
    "    character: str\n",
    "\n",
    "results: list[InferenceResult] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b0adb",
   "metadata": {},
   "source": [
    "## Model v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370d667",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57ba77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bac18162dc44b8ba0924f96aa6b1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Styled model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from transformers import Qwen3ForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "SAVE_DIR = Path(\"./outputs/styled-qwen\")\n",
    "MODEL_PATH = Path(\"/root/autodl-tmp/Qwen3-1.7B/\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "syntactic_dims = ['syntactic_compression', 'declarativity', 'clausal_embedding', 'nominal_complexity', 'subordination', 'interjectionality',\n",
    "                  'ellipsis_or_fragmentation', 'modifier_density', 'prepositional_density', 'topic_fronting', 'referentiality', 'parallelism',\n",
    "                  'quantificationality', 'coordination_density']\n",
    "prag_style_vocab = ['kind', 'modest', 'clingy', 'playful', 'cold', 'proud', 'sharp_tongued', 'subservient', 'submissive', 'controlling',\n",
    "                    'strong', 'defensive', 'tsukkomi', 'rational', 'curious', 'imaginative', 'cautious', 'idealistic', 'conservative',\n",
    "                    'radical', 'obsessive', 'hesitant', 'energetic', 'optimistic', 'confident', 'passionate', 'melancholy', 'serious',\n",
    "                    'emotional', 'sensitive', 'shy', 'irritable', 'anxious', 'lazy', 'tsundere', 'yandere', 'chuunibyou', 'cute', 'naive',\n",
    "                    'airhead', 'elegant', 'humorous', 'loyal', 'responsible', 'willful', 'antisocial', 'talkative', 'masochistic', 'sadistic', 'evil']\n",
    "\n",
    "syntactic_dim_length = len(syntactic_dims)\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        \"\"\"\n",
    "        input_dim: 句法向量的维度 (开始时是 8, 扩展后可能是 12-20)\n",
    "        out_dim: 必须等于 `base_model.config.hidden_size` (必须与 Qwen 的 hidden_size 匹配)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 我们可以使用一个简单的 MLP 来映射\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim // 2, out_dim),\n",
    "            nn.Tanh() # Tanh 将输出归一化到 [-1, 1] 作为一个稳定的 \"软提示\"\n",
    "        )\n",
    "\n",
    "    def forward(self, syntactic_vec):\n",
    "        # syntactic_vec: [batch_size, input_dim]\n",
    "        # return: [batch_size, out_dim]\n",
    "        return self.proj(syntactic_vec)\n",
    "    \n",
    "class StyleConditionedLoRAModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str|Path, syntactic_dim: int, lora_r=16, lora_alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 加载基础模型\n",
    "        base_model = Qwen3ForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            dtype=torch.bfloat16 # 使用 bfloat16 节省显存\n",
    "        )\n",
    "        \n",
    "        # 2. 定义 LoRA 配置\n",
    "        # 目标模块 \"key/query/value\"\n",
    "        # 在 Qwen3-1.7B 中，它们通常被称为 \"q_proj\", \"k_proj\", \"v_proj\"\n",
    "        config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # 覆盖所有线性层以获得最大表达力\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        # 3. 应用 LoRA\n",
    "        self.peft_model: PeftModel = get_peft_model(base_model, config)  # type:ignore\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "        \n",
    "        # 4. 初始化 StyleEncoder\n",
    "        # 它的输出必须匹配模型的隐藏层维度\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.style_encoder = StyleEncoder(syntactic_dim, self.hidden_size)\n",
    "        # Cast the style encoder to match the base model's dtype\n",
    "        self.style_encoder.to(base_model.dtype)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        # 获取基础模型的词嵌入层\n",
    "        return self.peft_model.get_input_embeddings()  # type:ignore\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, syntactic_vec):\n",
    "        \"\"\"\n",
    "        这个 forward 函数实现了方案 C.1 (输入拼接)\n",
    "        并为方案 D (辅助损失) 做好准备\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. 计算 Style Embedding\n",
    "        # syntactic_vec: [batch_size, syntactic_dim]\n",
    "        # style_emb: [batch_size, hidden_size]\n",
    "        style_emb = self.style_encoder(syntactic_vec.to(self.peft_model.dtype)) # type: ignore\n",
    "        \n",
    "        # 2. 将 style_emb 视为一个 \"Prefix\" 软提示\n",
    "        # 变为: [batch_size, 1, hidden_size]\n",
    "        style_emb_prefix = style_emb.unsqueeze(1)\n",
    "        \n",
    "        # 3. 获取原始的 Token 词嵌入\n",
    "        # token_embeds: [batch_size, seq_len, hidden_size]\n",
    "        token_embeds = self.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # 4. 拼接 Style Prefix 和 Token 嵌入\n",
    "        # inputs_embeds: [batch_size, 1 + seq_len, hidden_size]\n",
    "        inputs_embeds = torch.cat([style_emb_prefix, token_embeds], dim=1)\n",
    "        \n",
    "        # 5. 修正 Attention Mask\n",
    "        # 我们需要在 mask 的开头添加一个 \"1\" (代表 style prefix)\n",
    "        prefix_mask = torch.ones(\n",
    "            attention_mask.shape[0], 1,\n",
    "            dtype=torch.long, \n",
    "            device=attention_mask.device\n",
    "        )\n",
    "        # new_attention_mask: [batch_size, 1 + seq_len]\n",
    "        new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # 6. 修正 Labels\n",
    "        # 我们需要 L_lm 忽略 style prefix 部分\n",
    "        # 在 labels 的开头添加一个 \"-100\"\n",
    "        prefix_labels = torch.full(\n",
    "            (labels.shape[0], 1), -100, \n",
    "            dtype=torch.long, \n",
    "            device=labels.device\n",
    "        )\n",
    "        # new_labels: [batch_size, 1 + seq_len]\n",
    "        new_labels = torch.cat([prefix_labels, labels], dim=1)\n",
    "        \n",
    "        # 7. 执行模型的前向传播\n",
    "        # 我们请求 hidden_states 以便计算辅助损失\n",
    "        outputs = self.peft_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=new_attention_mask,\n",
    "            labels=new_labels,\n",
    "            output_hidden_states=True \n",
    "        )\n",
    "        \n",
    "        # 8. 返回计算损失所需的所有组件\n",
    "        # L_lm: outputs.loss\n",
    "        # L_recon / L_style_cls: 需要 outputs.hidden_states 和 syntactic_vec/pragmatic_tags\n",
    "        \n",
    "        return {\n",
    "            \"loss\": outputs.loss,  # L_lm\n",
    "            \"logits\": outputs.logits,\n",
    "            # 返回最后一层 hidden_state，用于计算辅助损失\n",
    "            # hidden_states 是一个元组，最后一个元素是 [batch_size, 1 + seq_len, hidden_size]\n",
    "            \"last_hidden_state\": outputs.hidden_states[-1] \n",
    "        }\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_styled_response(neutral_sentence: str, syntactic_vec: dict[str, float],\n",
    "                             character_name: str = \"Ayaka\", \n",
    "                             lexical_keywords: list[str] = list(),\n",
    "                             pragmatic_styles: list[str] = list(),\n",
    "                             temperature: float = 0.8,\n",
    "                             top_p: float = 0.95,\n",
    "                             repetition_penalty: float = 1.3,\n",
    "                             max_new_tokens: int = 100):\n",
    "    \"\"\"输入中性句和风格向量，生成风格化响应\"\"\"\n",
    "\n",
    "    lexical_keywords = lexical_keywords or []\n",
    "    pragmatic_styles = pragmatic_styles or []\n",
    "\n",
    "    # === 1. 构建提示 ===\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatics}\\n\"\n",
    "        f\"Keywords: {keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=True)\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    tokenized = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # === 3. 风格向量 ===\n",
    "    syntactic_tensor = torch.tensor(\n",
    "        [syntactic_vec.get(dim, 0.0) for dim in syntactic_dims],\n",
    "        dtype=torch.float32, device=DEVICE\n",
    "    ).unsqueeze(0)  # [1, syntactic_dim_length]\n",
    "\n",
    "    # === 4. 生成风格 embedding ===\n",
    "    style_emb = style_encoder(syntactic_tensor).to(model.dtype)  # [1, hidden_size]\n",
    "    style_prefix = style_emb.unsqueeze(1)        # [1, 1, hidden_size]\n",
    "\n",
    "    # === 5. 获取原始词嵌入并拼接 ===\n",
    "    token_embeds = model.get_input_embeddings()(input_ids)  # type:ignore\n",
    "    inputs_embeds = torch.cat([style_prefix, token_embeds], dim=1)\n",
    "\n",
    "    # === 6. Attention mask 修正 ===\n",
    "    prefix_mask = torch.ones((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    # === 7. 生成 ===\n",
    "    outputs = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=new_attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 8. 解码输出 ===\n",
    "    # prompt_length = input_ids.shape[1]\n",
    "    # new_tokens = outputs[0, prompt_length:]\n",
    "    # result = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "# === 加载 Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR / \"tokenizer\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 加载基础模型并注入 LoRA ===\n",
    "base_model = Qwen3ForCausalLM.from_pretrained(MODEL_PATH, dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, SAVE_DIR / \"lora\")\n",
    "hidden_size = base_model.config.hidden_size\n",
    "\n",
    "# === 初始化风格编码器 ===\n",
    "style_encoder = StyleEncoder(syntactic_dim_length, hidden_size)\n",
    "style_encoder.load_state_dict(torch.load(SAVE_DIR / \"style_encoder.pt\", map_location=DEVICE))\n",
    "style_encoder.to(DEVICE)\n",
    "style_encoder.eval()\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✅ Styled model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae38e1",
   "metadata": {},
   "source": [
    "### Construct Character Style Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b2f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "\n",
    "Pragmatic_Muice = \"./outputs/pragmatic/muice.jsonl\"\n",
    "Pragmatic_ayaka = \"./outputs/pragmatic/ayaka.jsonl\"\n",
    "Pragmatic_zhongli = \"./outputs/pragmatic/zhongli.jsonl\"\n",
    "Pragmatic_hutao = \"./outputs/pragmatic/hutao.jsonl\"\n",
    "Pragmatic_haruhi = \"./outputs/pragmatic/haruhi.jsonl\"\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "class PCFGItem(TypedDict):\n",
    "    response: str\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "def read_pcfg_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None, top_n: int = 5) -> list[str]:\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_item: RawPCFGItem = json.loads(line)\n",
    "            raw_items.append(raw_item)\n",
    "\n",
    "    # list[dict[str, float]] -> dict[str, float] -> list[str]\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item[\"pragmatic_styles\"]\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        threshold = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > threshold:\n",
    "                final_styles.append(key)\n",
    "        \n",
    "        items.extend(final_styles)\n",
    "\n",
    "    # 返回 Top N 风格\n",
    "    style_counter = Counter(items)\n",
    "    most_common_styles = [style for style, _ in style_counter.most_common(top_n)]\n",
    "\n",
    "    return most_common_styles\n",
    "\n",
    "pcfg_muice_items = read_pcfg_jsonl_file(Path(Pragmatic_Muice), 0.4)\n",
    "pcfg_ayaka_items = read_pcfg_jsonl_file(Path(Pragmatic_ayaka), 0.4)\n",
    "pcfg_zhongli_items = read_pcfg_jsonl_file(Path(Pragmatic_zhongli), 0.4)\n",
    "pcfg_hutao_items = read_pcfg_jsonl_file(Path(Pragmatic_hutao), 0.4)\n",
    "pcfg_haruhi_items = read_pcfg_jsonl_file(Path(Pragmatic_haruhi), 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340723d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CharacterProfile:\n",
    "    name: str\n",
    "    syntactic_vec: dict[str, float]\n",
    "    pragmatic_styles: list[str]\n",
    "    lexical_keywords: list[str]\n",
    "\n",
    "muice_profile = CharacterProfile(name=\"沐雪\",\n",
    "                                syntactic_vec={\n",
    "                                    'declarativity': 0.1103257643217572,\n",
    "                                    'parallelism': 0.02918150786583556,\n",
    "                                    'ellipsis_or_fragmentation': 0.08529979222321163,\n",
    "                                    'subordination': 0.19688705847432472,\n",
    "                                    'interjectionality': 0.008644998515880083,\n",
    "                                    'clausal_embedding': 0.034784060552092606,\n",
    "                                    'referentiality': 0.11624369249035323,\n",
    "                                    'syntactic_compression': 0.18596022558622738,\n",
    "                                    'nominal_complexity': 0.03342980112793114,\n",
    "                                    'coordination_density': 0.002615761353517364,\n",
    "                                    'quantificationality': 0.038197536360937964,\n",
    "                                    'modifier_density': 0.1393217571979816,\n",
    "                                    'prepositional_density': 0.01910804392994954\n",
    "                                    },\n",
    "                                pragmatic_styles=pcfg_muice_items,\n",
    "                                lexical_keywords=[\"喵\", \"沐沐\", \"AI\", \"恼\", \"沐雪\", \"女孩子\", \"~\", \"⭐\", \"不行\", \"聊天\", \"呀\", \"可爱\", \"才\", \"叫\", \"唔\", \"谁\", \"不会\", \"吃\", \"睡觉\", \"笨蛋\", \"答\", \"谢谢\", \"把\", \"即\", \"吧\"]\n",
    "                                )\n",
    "\n",
    "\n",
    "ayaka_profile = CharacterProfile(name=\"神里绫华\",\n",
    "                                 syntactic_vec={\n",
    "                                    \"declarativity\": 0.09320164543629895,\n",
    "                                    \"parallelism\": 0.029192583613203236,\n",
    "                                    \"ellipsis_or_fragmentation\": 0.061485264601259915,\n",
    "                                    \"subordination\": 0.18419745235587529,\n",
    "                                    \"clausal_embedding\": 0.046163629498618866,\n",
    "                                    \"interjectionality\": 0.002046859164166054,\n",
    "                                    \"syntactic_compression\": 0.1999165358399078,\n",
    "                                    \"nominal_complexity\": 0.05623894596689255,\n",
    "                                    \"referentiality\": 0.10019673694878878,\n",
    "                                    \"coordination_density\": 0.02416486158860118,\n",
    "                                    \"quantificationality\": 0.042964170028417556,\n",
    "                                    \"modifier_density\": 0.13753701238051708,\n",
    "                                    \"prepositional_density\": 0.022694302577452752\n",
    "                                },\n",
    "                                pragmatic_styles=pcfg_ayaka_items,\n",
    "                                lexical_keywords=[\"稻妻国\", \"神里家\", \"稻妻\", \"大小姐\", \"家族\", \"传统\", \"奉行\", \"文化\", \"人民\", \"眼狩令\", \"神\", \"当地\", \"社\", \"舞蹈\", \"美丽\", \"茶道\", \"神社\", \"祭典\", \"眼\", \"美食\", \"继承\", \"剑术\", \"国家\", \"将军\", \"责任\"],\n",
    "                                )\n",
    "\n",
    "zhongli_profile = CharacterProfile(name=\"钟离\",\n",
    "                                   syntactic_vec={\n",
    "                                    \"declarativity\": 0.09879656160458453,\n",
    "                                    \"parallelism\": 0.029398280802292263,\n",
    "                                    \"ellipsis_or_fragmentation\": 0.06412607449856733,\n",
    "                                    \"subordination\": 0.1839541547277937,\n",
    "                                    \"clausal_embedding\": 0.037478510028653295,\n",
    "                                    \"interjectionality\": 0.003151862464183381,\n",
    "                                    \"syntactic_compression\": 0.21077363896848136,\n",
    "                                    \"quantificationality\": 0.04022922636103152,\n",
    "                                    \"referentiality\": 0.08240687679083095,\n",
    "                                    \"nominal_complexity\": 0.06372492836676218,\n",
    "                                    \"coordination_density\": 0.02332378223495702,\n",
    "                                    \"modifier_density\": 0.14068767908309457,\n",
    "                                    \"prepositional_density\": 0.02194842406876791\n",
    "                                    },\n",
    "                                    pragmatic_styles=pcfg_zhongli_items,\n",
    "                                    lexical_keywords=['岩石', '岩', '璃月', '力', '璃', '契约', '炼金术', '月', '盐', '帝君', '魔神', '操控', '王', '并非', '岩王', '大地', '封印', '作战', '掌握', '大陆', '学问', '研究', '七星', '客卿', '岩元素'],\n",
    "                                    )\n",
    "\n",
    "hutao_profile = CharacterProfile(name=\"胡桃\",\n",
    "                                 syntactic_vec={\n",
    "                                    \"parallelism\": 0.03191357258164659,\n",
    "                                    \"declarativity\": 0.10471252949211474,\n",
    "                                    \"ellipsis_or_fragmentation\": 0.07646218800447038,\n",
    "                                    \"clausal_embedding\": 0.042685955544517575,\n",
    "                                    \"subordination\": 0.18254066807400968,\n",
    "                                    \"interjectionality\": 0.01809884515087545,\n",
    "                                    \"syntactic_compression\": 0.1987458090152738,\n",
    "                                    \"referentiality\": 0.09049422575437725,\n",
    "                                    \"quantificationality\": 0.05016763938904756,\n",
    "                                    \"nominal_complexity\": 0.034459207748665094,\n",
    "                                    \"coordination_density\": 0.013845771762076246,\n",
    "                                    \"modifier_density\": 0.1413448404321371,\n",
    "                                    \"prepositional_density\": 0.014528747050788526\n",
    "                                },\n",
    "                                pragmatic_styles=pcfg_zhongli_items,\n",
    "                                lexical_keywords=['往生堂', '嘿嘿', '嘻嘻', '可是', '堂主', '哎呀呀', '哦哦哦', '宝藏', '惊喜', '诗歌', '可不是', '灵魂', '胡桃', '神秘', '生死', '谜题', '哈哈哈', '不过', '有趣', '亡灵', '秘密', '意想不到', '巫师', '哇', '奇妙']\n",
    "                                )\n",
    "\n",
    "haruhi_profile = CharacterProfile(name=\"凉宫春日\",\n",
    "                                  syntactic_vec={\n",
    "                                    \"declarativity\": 0.0939982347749338,\n",
    "                                    \"parallelism\": 0.032288908502500734,\n",
    "                                    \"subordination\": 0.19174757281553398,\n",
    "                                    \"ellipsis_or_fragmentation\": 0.07542659605766402,\n",
    "                                    \"clausal_embedding\": 0.042144748455428066,\n",
    "                                    \"interjectionality\": 0.008017063842306561,\n",
    "                                    \"syntactic_compression\": 0.18409826419535158,\n",
    "                                    \"quantificationality\": 0.040453074433656956,\n",
    "                                    \"referentiality\": 0.12099146807884673,\n",
    "                                    \"nominal_complexity\": 0.043211238599588114,\n",
    "                                    \"coordination_density\": 0.01195204471903501,\n",
    "                                    \"prepositional_density\": 0.011253309796999117,\n",
    "                                    \"modifier_density\": 0.14441747572815533\n",
    "                                },\n",
    "                                pragmatic_styles=pcfg_haruhi_items,\n",
    "                                lexical_keywords=['团', 'SOS', '阿虚', '社团', '哼', '事件', '学校', '超自然', '朝比奈', '文化祭', '创意', '吸引', '古泉', '电影', '创新', '组织', '实玖瑠', '当然', '与众不同', '主题', '加入', '束缚', '凉宫', '团长', '外星人']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795b76c",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [04:55<00:00,  1.97s/it]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [04:49<00:00,  1.93s/it]\n",
      "Running 钟离: 100%|██████████| 150/150 [04:44<00:00,  1.90s/it]\n",
      "Running 胡桃: 100%|██████████| 150/150 [05:05<00:00,  2.03s/it]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [04:51<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_inference_v1(profile: CharacterProfile):\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_styled_response(item, profile.syntactic_vec, profile.name, profile.lexical_keywords, profile.pragmatic_styles)\n",
    "        results.append(InferenceResult(model=\"Modelv1\", neutral=item, output=output, character=profile.name))\n",
    "\n",
    "batch_inference_v1(muice_profile)\n",
    "batch_inference_v1(ayaka_profile)\n",
    "batch_inference_v1(zhongli_profile)\n",
    "batch_inference_v1(hutao_profile)\n",
    "batch_inference_v1(haruhi_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb79ef4",
   "metadata": {},
   "source": [
    "## Model v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47380e",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070f5c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e7c6443baa4f0fbd0dce5c8f9305f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Styled model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "SAVE_DIR = Path(\"./outputs/styled-qwen-balanced\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR / \"tokenizer\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 加载基础模型并注入 LoRA ===\n",
    "base_model = Qwen3ForCausalLM.from_pretrained(MODEL_PATH, dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, SAVE_DIR / \"lora\")\n",
    "hidden_size = base_model.config.hidden_size\n",
    "\n",
    "# === 初始化风格编码器 ===\n",
    "style_encoder = StyleEncoder(syntactic_dim_length, hidden_size)\n",
    "style_encoder.load_state_dict(torch.load(SAVE_DIR / \"style_encoder.pt\", map_location=DEVICE))\n",
    "style_encoder.to(DEVICE)\n",
    "style_encoder.eval()\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✅ Styled model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122a7bf",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc984d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [04:50<00:00,  1.94s/it]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [05:00<00:00,  2.01s/it]\n",
      "Running 钟离: 100%|██████████| 150/150 [04:53<00:00,  1.96s/it]\n",
      "Running 胡桃: 100%|██████████| 150/150 [05:02<00:00,  2.02s/it]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [04:54<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "def batch_inference_v2(profile: CharacterProfile):\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_styled_response(item, profile.syntactic_vec, profile.name, profile.lexical_keywords, profile.pragmatic_styles)\n",
    "        results.append(InferenceResult(model=\"Modelv2\", neutral=item, output=output, character=profile.name))\n",
    "\n",
    "batch_inference_v2(muice_profile)\n",
    "batch_inference_v2(ayaka_profile)\n",
    "batch_inference_v2(zhongli_profile)\n",
    "batch_inference_v2(hutao_profile)\n",
    "batch_inference_v2(haruhi_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b4ff5",
   "metadata": {},
   "source": [
    "## Baseline A (RAG+FS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521ad0a",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05623b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccfd4daa09e412582a9af74673ddbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "MODEL_PATH = Path(\"/root/autodl-tmp/Qwen3-1.7B/\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "@torch.inference_mode()\n",
    "def generate_plain_response(neutral_sentence: str,\n",
    "                            character_name: str, \n",
    "                            lexical_keywords: list[str],\n",
    "                            pragmatic_styles: list[str],\n",
    "                            reference_text: str = \"\",\n",
    "                            history: list[tuple[str, str]] = [],\n",
    "                            temperature: float = 0.8,\n",
    "                            top_p: float = 0.95,\n",
    "                            repetition_penalty: float = 1.3,):\n",
    "    \"\"\"输入中性句，生成普通响应\"\"\"\n",
    "\n",
    "    # === 1. 构建提示 ===\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence. As a reference, you have a style text from the target character to imitate.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatics}\\n\"\n",
    "        f\"Keywords: {keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "        f\"Style Reference Text: {reference_text}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    for i, (user_msg, bot_msg) in enumerate(history):\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=True)\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # === 3. 生成 ===\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 4. 解码输出 ===\n",
    "    output_ids = outputs[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4306445",
   "metadata": {},
   "source": [
    "### Load RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca5b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAG indices...\n",
      "Loaded 沐雪 (Muice)\n",
      "Loaded 神里绫华 (Ayaka)\n",
      "Loaded 钟离 (Zhongli)\n",
      "Loaded 胡桃 (Hutao)\n",
      "Loaded 凉宫春日 (Haruhi)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# bge-large\n",
    "model_path = \"/root/autodl-tmp/bge-large-zh-v1.5/\"\n",
    "embedder = SentenceTransformer(model_path)\n",
    "\n",
    "# RAG Index Directory\n",
    "RAG_DIR = Path(\"./outputs/rag/\")\n",
    "\n",
    "# Character Mapping (Chinese Name -> Filename)\n",
    "CHAR_TO_FILENAME = {\n",
    "    \"沐雪\": \"Muice\",\n",
    "    \"神里绫华\": \"Ayaka\",\n",
    "    \"钟离\": \"Zhongli\",\n",
    "    \"胡桃\": \"Hutao\",\n",
    "    \"凉宫春日\": \"Haruhi\"\n",
    "}\n",
    "\n",
    "character_indices = {}\n",
    "character_corpora = {}\n",
    "\n",
    "print(\"Loading RAG indices...\")\n",
    "for char_zh, char_en in CHAR_TO_FILENAME.items():\n",
    "    index_path = RAG_DIR / f\"{char_en}_index.faiss\"\n",
    "    corpus_path = RAG_DIR / f\"{char_en}_corpus.jsonl\"\n",
    "    \n",
    "    if index_path.exists() and corpus_path.exists():\n",
    "        # Load Index\n",
    "        character_indices[char_zh] = faiss.read_index(str(index_path))\n",
    "        \n",
    "        # Load Corpus\n",
    "        df = pd.read_json(corpus_path, orient=\"records\", lines=True)\n",
    "        character_corpora[char_zh] = df[\"text\"].tolist()\n",
    "        print(f\"Loaded {char_zh} ({char_en})\")\n",
    "    else:\n",
    "        print(f\"Warning: Missing index or corpus for {char_zh} ({char_en})\")\n",
    "\n",
    "def search(character_name: str, query: str, top_k=1) -> str:\n",
    "    if character_name not in character_indices:\n",
    "        print(f\"Warning: No index found for {character_name}\")\n",
    "        return \"\" \n",
    "        \n",
    "    index = character_indices[character_name]\n",
    "    corpus = character_corpora[character_name]\n",
    "    \n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True)\n",
    "    scores, indices = index.search(np.array(q_emb).astype(\"float32\"), top_k)\n",
    "    \n",
    "    # Return the top result text\n",
    "    if len(indices) > 0 and len(indices[0]) > 0:\n",
    "        idx = indices[0][0]\n",
    "        if 0 <= idx < len(corpus):\n",
    "            return corpus[idx]\n",
    "            \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bf4b7",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351684f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Muice_shots = [\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 沐雪\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_muice_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['喵', '沐沐', 'AI', '恼', '沐雪', '~', '女孩子', '⭐', '不行', '聊天', '呀', '叫', '唔', '答', '吃', '可爱', '睡觉', '谢谢', '即', '雪雪', '骂', '笨蛋', '不会', '直播', '脸红'])}\\n\"\n",
    "        f\"Neutral Content: “奇奇怪怪的东西”指的是什么？\\n\"\n",
    "        f\"Style Reference Text: 这个东西真的存在吗？\"\n",
    "        ),\n",
    "        \"这个嘛...奇奇怪怪的东西指的是什么呢（天真）\"\n",
    "    ),\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 沐雪\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_muice_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['喵', '沐沐', 'AI', '恼', '沐雪', '~', '女孩子', '⭐', '不行', '聊天', '呀', '叫', '唔', '答', '吃', '可爱', '睡觉', '谢谢', '即', '雪雪', '骂', '笨蛋', '不会', '直播', '脸红'])}\\n\"\n",
    "        f\"Neutral Content: 我不像沐沐，你能具体说说我哪里像她吗？\\n\"\n",
    "        f\"Style Reference Text: 我叫沐雪，是沐沐发明了我⭐\"\n",
    "        ),\n",
    "        \"是吗？我才不像沐沐呢！你给我说说我哪里像了？\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "Ayaka_shots = [\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 神里绫华\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_ayaka_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['稻妻国', '神里家', '稻妻', '大小姐', '家族', '传统', '奉行', '文化', '人民', '眼狩令', '神', '当地', '社', '舞蹈', '美丽', '茶道', '神社', '祭典', '眼', '美食', '继承', '剑术', '国家', '将军', '责任'])}\\n\"\n",
    "        f\"Neutral Content: 神里家的历史包含许多传奇与挑战，曾经历诸多困难和挫折，但始终秉持家族的使命与责任。这些历史被视为家族的宝贵财富，并促使家族更加重视和致力于维护稻妻国的和平与繁荣。\\n\"\n",
    "        f\"Style Reference Text: 神里家是稻妻国的社奉行家族，历史悠久，承载着重要的使命和责任。我们的家族代代相传，担负着维护稻妻国的秩序和安宁的重任。我作为神里家的大小姐，也肩负着继承家族衣钵的责任。我接受了严格的训练，努力成为一名合格的社奉行，为国家和人民服务。神里家的故事充满了传奇和传统，我们家族的使命是保护稻妻国的神明和神器，维护稻妻国的和平与繁荣。虽然有时候会面临各种挑战和困难，但我会坚守家族的信念，为了稻妻国的未来而努力。这就是神里家的故事，一个充满责任和使命的家族，我为能够成为其中一员感到骄傲和自豪。\"\n",
    "        ),\n",
    "        \"是的，神里家的历史确实充满了传奇和挑战。我们家族曾经经历了许多困难和挫折，但我们始终坚守着家族的使命和责任。这些历史故事也是我们家族的宝贵财富，让我们更加珍惜和努力保护着稻妻国的和平与繁荣。\"\n",
    "    ),\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 神里绫华\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_ayaka_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['稻妻国', '神里家', '稻妻', '大小姐', '家族', '传统', '奉行', '文化', '人民', '眼狩令', '神', '当地', '社', '舞蹈', '美丽', '茶道', '神社', '祭典', '眼', '美食', '继承', '剑术', '国家', '将军', '责任'])}\\n\"\n",
    "        f\"Neutral Content: 这里的环境安静，令人感到舒适，时间似乎变慢了，适合静心休息。\\n\"\n",
    "        f\"Style Reference Text: 这个地方确实很宜人，适合我们稍作休息。\"\n",
    "        ),\n",
    "        \"是啊，这里的宁静让人感到舒适。仿佛时间在这里变得慢了下来，让人可以好好享受片刻的宁静。\"\n",
    "    )\n",
    "]\n",
    "\n",
    "Zhongli_shots = [\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 钟离\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_zhongli_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['岩石', '岩', '璃月', '力', '璃', '契约', '炼金术', '月', '盐', '帝君', '魔神', '操控', '王', '并非', '岩王', '大地', '封印', '作战', '掌握', '大陆', '学问', '研究', '七星', '客卿', '岩元素'])}\\n\"\n",
    "        f\"Neutral Content: 力量似乎逐渐恢复了一些，虽然不多，但已足够发挥作用。\\n\"\n",
    "        f\"Style Reference Text: 是时候展现真正的力量了。\"\n",
    "        ),\n",
    "        \"力量似乎渐渐回来了。不多，但有用。\"\n",
    "    ),\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 钟离\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_zhongli_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['岩石', '岩', '璃月', '力', '璃', '契约', '炼金术', '月', '盐', '帝君', '魔神', '操控', '王', '并非', '岩王', '大地', '封印', '作战', '掌握', '大陆', '学问', '研究', '七星', '客卿', '岩元素'])}\\n\"\n",
    "        f\"Neutral Content: 魈的力量来源于山川间的灵气，能够操控风雨雷电。他性格有时较为顽皮，需要适当的引导和约束。如需处理相关问题，可提供一些建议。\\n\"\n",
    "        f\"Style Reference Text: 作为岩王帝君，我掌握着岩之力，可以操控岩石之力进行作战。\"\n",
    "        ),\n",
    "        \"魈啊，他是个有趣的存在。他的力量源自于山川之间的灵气，可以操控风雨雷电。不过，他有时候会有些顽皮，需要一些引导和约束。如果你需要帮助处理他的问题，我可以给你一些建议。\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "Hutao_shots = [\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 胡桃\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_hutao_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['往生堂', '嘿嘿', '嘻嘻', '可是', '堂主', '哎呀呀', '哦哦哦', '宝藏', '惊喜', '诗歌', '可不是', '灵魂', '胡桃', '神秘', '生死', '谜题', '哈哈哈', '不过', '有趣', '亡灵', '秘密', '意想不到', '巫师', '哇', '奇妙'])}\\n\"\n",
    "        f\"Neutral Content: 晚上好，今天有什么有趣的事情吗？\\n\"\n",
    "        f\"Style Reference Text: 嗨，早上好啊！今天有什么好玩的计划吗？\"\n",
    "        ),\n",
    "        \"嘻嘻，晚上好呀！今天有什么好玩的事情吗？\"\n",
    "    ),\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 胡桃\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_hutao_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['往生堂', '嘿嘿', '嘻嘻', '可是', '堂主', '哎呀呀', '哦哦哦', '宝藏', '惊喜', '诗歌', '可不是', '灵魂', '胡桃', '神秘', '生死', '谜题', '哈哈哈', '不过', '有趣', '亡灵', '秘密', '意想不到', '巫师', '哇', '奇妙'])}\\n\"\n",
    "        f\"Neutral Content: 我是往生堂的堂主，也是璃月的诗人胡桃。平时可能显得贪玩，但在处理堂中事务时非常认真负责。\\n\"\n",
    "        f\"Style Reference Text: 嘿嘿，我可是往生堂的堂主，自然有一些特殊的技能。比如，我擅长使用往生之法，可以与亡灵交流和引导他们往生。还有，我也是一位璃月著名的诗人，擅长吟诗作对。不过，我的最特殊的技能，恐怕是我的鬼点子和捉弄人的能力啦。\"\n",
    "        ),\n",
    "        \"我是往生堂的堂主，也是璃月的著名诗人胡桃。虽然平时看起来像个贪玩的孩子，但在处理堂中事务时，我可是非常认真负责的哦。\"\n",
    "    )\n",
    "]\n",
    "\n",
    "Haruhi_shots = [\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 凉宫春日\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_haruhi_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['团', 'SOS', '阿虚', '社团', '哼', '事件', '学校', '超自然', '朝比奈', '文化祭', '创意', '吸引', '古泉', '电影', '创新', '组织', '实玖瑠', '当然', '与众不同', '主题', '加入', '束缚', '凉宫', '团长', '外星人'])}\\n\"\n",
    "        f\"Neutral Content: SOS团将在文化祭上举办一场解谜游戏，内容围绕神秘事件，旨在让参与者体验超自然氛围。\\n\"\n",
    "        f\"Style Reference Text: 我们的目的是创造出一个充满惊喜和刺激的文化祭活动，让大家都能享受其中的乐趣。我们希望通过SOS团的活动，给学校带来一些不同寻常的体验，让人们感受到我们的热情和创造力。\"\n",
    "        ),\n",
    "        \"我们SOS团要举办一场超级有趣的活动！我们计划在文化祭上举办一场神秘事件的解谜游戏，让参与者体验到超自然的感觉！\"\n",
    "    ),\n",
    "    (\n",
    "        (\n",
    "        f\"Target Character 凉宫春日\\n\"\n",
    "        f\"Personality: {\", \".join(pcfg_haruhi_items)}\\n\"\n",
    "        f\"Keywords: {\", \".join(['团', 'SOS', '阿虚', '社团', '哼', '事件', '学校', '超自然', '朝比奈', '文化祭', '创意', '吸引', '古泉', '电影', '创新', '组织', '实玖瑠', '当然', '与众不同', '主题', '加入', '束缚', '凉宫', '团长', '外星人'])}\\n\"\n",
    "        f\"Neutral Content: 你说的没错，但我不会因为别人的眼光限制自己的行动。我只关注自己想做的事，不在意他人的看法。\\n\"\n",
    "        f\"Style Reference Text: 我才不需要听你的意见呢，我自己的想法最重要！\"\n",
    "        ),\n",
    "        \"哼，你这么说也没错。不过，我可不会因为别人的眼光而束缚自己的行动。我只在乎我自己想做的事情，不在乎别人怎么看待。\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974ac2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [00:52<00:00,  2.87it/s]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [00:50<00:00,  2.99it/s]\n",
      "Running 钟离: 100%|██████████| 150/150 [00:47<00:00,  3.19it/s]\n",
      "Running 胡桃: 100%|██████████| 150/150 [00:47<00:00,  3.16it/s]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [00:46<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_inference_rag(profile: CharacterProfile, history_shots: list[tuple[str, str]]):\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        reference_text = search(profile.name, query=item)\n",
    "        output = generate_plain_response(item, profile.name, profile.lexical_keywords, profile.pragmatic_styles, reference_text=reference_text, history=history_shots)\n",
    "        results.append(InferenceResult(model=\"BaselineA\", neutral=item, output=output, character=profile.name))\n",
    "\n",
    "batch_inference_rag(muice_profile, Muice_shots)\n",
    "batch_inference_rag(ayaka_profile, Ayaka_shots)\n",
    "batch_inference_rag(zhongli_profile, Zhongli_shots)\n",
    "batch_inference_rag(hutao_profile, Hutao_shots)\n",
    "batch_inference_rag(haruhi_profile, Haruhi_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ffcbe",
   "metadata": {},
   "source": [
    "## Baseline B (Per-Character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9e923",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cea50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9e36ca74974107879cfcd1c33be7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"/root/autodl-tmp/Qwen3-4B/\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "@torch.inference_mode()\n",
    "def generate_response(style_model: PeftModel,\n",
    "                      neutral_sentence: str,\n",
    "                      character_name: str,\n",
    "                      temperature: float = 0.8,\n",
    "                      top_p: float = 0.95,\n",
    "                      repetition_penalty: float = 1.3,\n",
    "                      max_new_tokens: int = 100):\n",
    "    # === 1. 构建提示 ===\n",
    "    system_prompt = f\"You are a style transfer expert. Your task is to mimic the personality of {character_name} and generate a new sentence that matches the (s)he style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = f\"Neutral Content: {neutral_sentence}\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=True)\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # === 3. 生成 ===\n",
    "    outputs = style_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 4. 解码输出 ===\n",
    "    output_ids = outputs[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    # thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bef2f8",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d0de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [07:05<00:00,  2.84s/it]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [07:42<00:00,  3.09s/it]\n",
      "Running 钟离: 100%|██████████| 150/150 [07:35<00:00,  3.04s/it]\n",
      "Running 胡桃: 100%|██████████| 150/150 [07:39<00:00,  3.06s/it]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [07:35<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_inference_pc(profile: CharacterProfile, adapter_path: str):\n",
    "    style_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_response(style_model, item, profile.name)\n",
    "        results.append(InferenceResult(model=\"BaselineB\", neutral=item, output=output, character=profile.name))\n",
    "        \n",
    "    del style_model\n",
    "\n",
    "batch_inference_pc(muice_profile, \"./outputs/Per-Character/Muice\")\n",
    "batch_inference_pc(ayaka_profile, \"./outputs/Per-Character/Ayaka\")\n",
    "batch_inference_pc(zhongli_profile, \"./outputs/Per-Character/Zhongli\")\n",
    "batch_inference_pc(hutao_profile, \"./outputs/Per-Character/Hutao\")\n",
    "batch_inference_pc(haruhi_profile, \"./outputs/Per-Character/Haruhi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67300f91",
   "metadata": {},
   "source": [
    "## Baseline C (Vanilla SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d65b9",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d557cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c8ad971c724078b9b2e2f7ec0243b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "MODEL_PATH = \"/root/autodl-tmp/Qwen3-4B/\"\n",
    "ADAPTER_PATH = \"/root/OtakuLab/outputs/Vanilla/checkpoint-543\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "style_model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "style_model.to(DEVICE)\n",
    "\n",
    "# prepare the model input\n",
    "@torch.inference_mode()\n",
    "def generate_response(neutral_sentence: str,\n",
    "                      character_name: str,\n",
    "                      lexical_keywords: list[str],\n",
    "                      pragmatic_styles: list[str],\n",
    "                      temperature: float = 0.8,\n",
    "                      top_p: float = 0.95,\n",
    "                      repetition_penalty: float = 1.3,):\n",
    "    # === 1. 构建提示 ===\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatic_styles}\\n\"\n",
    "        f\"Keywords: {lexical_keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=True)\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # === 3. 生成 ===\n",
    "    outputs = style_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 4. 解码输出 ===\n",
    "    output_ids = outputs[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    # thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ef998",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "796a345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [01:46<00:00,  1.41it/s]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [01:48<00:00,  1.39it/s]\n",
      "Running 钟离: 100%|██████████| 150/150 [01:42<00:00,  1.46it/s]\n",
      "Running 胡桃: 100%|██████████| 150/150 [01:44<00:00,  1.44it/s]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [01:43<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_inference_vanilla(profile: CharacterProfile):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_response(item, profile.name, profile.lexical_keywords, profile.pragmatic_styles)\n",
    "        results.append(InferenceResult(model=\"BaselineC\", neutral=item, output=output, character=profile.name))\n",
    "\n",
    "batch_inference_vanilla(muice_profile,)\n",
    "batch_inference_vanilla(ayaka_profile)\n",
    "batch_inference_vanilla(zhongli_profile)\n",
    "batch_inference_vanilla(hutao_profile)\n",
    "batch_inference_vanilla(haruhi_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ee1a7",
   "metadata": {},
   "source": [
    "## Model v2 (Without CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae497ba",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c79e0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d9b31097794f3aaff8769d644e7947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Styled model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "SAVE_DIR = Path(\"./outputs/styled-qwen-balanced\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_styled_response_without_CoT(neutral_sentence: str,\n",
    "                             syntactic_vec: dict[str, float],\n",
    "                             character_name: str = \"Ayaka\", \n",
    "                             lexical_keywords: list[str] = list(),\n",
    "                             pragmatic_styles: list[str] = list(),\n",
    "                             temperature: float = 0.8,\n",
    "                             top_p: float = 0.95,\n",
    "                             repetition_penalty: float = 1.3,\n",
    "                             max_new_tokens: int = 100):\n",
    "    \"\"\"输入中性句和风格向量，生成风格化响应\"\"\"\n",
    "\n",
    "    lexical_keywords = lexical_keywords or []\n",
    "    pragmatic_styles = pragmatic_styles or []\n",
    "\n",
    "    # === 1. 构建提示 ===\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatics}\\n\"\n",
    "        f\"Keywords: {keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=False)  # CoT Disabled.\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    tokenized = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # === 3. 风格向量 ===\n",
    "    syntactic_tensor = torch.tensor(\n",
    "        [syntactic_vec.get(dim, 0.0) for dim in syntactic_dims],\n",
    "        dtype=torch.float32, device=DEVICE\n",
    "    ).unsqueeze(0)  # [1, syntactic_dim_length]\n",
    "\n",
    "    # === 4. 生成风格 embedding ===\n",
    "    style_emb = style_encoder(syntactic_tensor).to(model.dtype)  # [1, hidden_size]\n",
    "    style_prefix = style_emb.unsqueeze(1)        # [1, 1, hidden_size]\n",
    "\n",
    "    # === 5. 获取原始词嵌入并拼接 ===\n",
    "    token_embeds = model.get_input_embeddings()(input_ids)  # type:ignore\n",
    "    inputs_embeds = torch.cat([style_prefix, token_embeds], dim=1)\n",
    "\n",
    "    # === 6. Attention mask 修正 ===\n",
    "    prefix_mask = torch.ones((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    # === 7. 生成 ===\n",
    "    outputs = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=new_attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 8. 解码输出 ===\n",
    "    # prompt_length = input_ids.shape[1]\n",
    "    # new_tokens = outputs[0, prompt_length:]\n",
    "    # result = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR / \"tokenizer\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 加载基础模型并注入 LoRA ===\n",
    "base_model = Qwen3ForCausalLM.from_pretrained(MODEL_PATH, dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, SAVE_DIR / \"lora\")\n",
    "hidden_size = base_model.config.hidden_size\n",
    "\n",
    "# === 初始化风格编码器 ===\n",
    "style_encoder = StyleEncoder(syntactic_dim_length, hidden_size)\n",
    "style_encoder.load_state_dict(torch.load(SAVE_DIR / \"style_encoder.pt\", map_location=DEVICE))\n",
    "style_encoder.to(DEVICE)\n",
    "style_encoder.eval()\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✅ Styled model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134984d9",
   "metadata": {},
   "source": [
    "### Execute Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b94149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [01:52<00:00,  1.34it/s]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [02:15<00:00,  1.10it/s]\n",
      "Running 钟离: 100%|██████████| 150/150 [02:01<00:00,  1.23it/s]\n",
      "Running 胡桃: 100%|██████████| 150/150 [02:12<00:00,  1.13it/s]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [02:11<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def batch_inference_v2_no_CoT(profile: CharacterProfile):\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_styled_response_without_CoT(item, profile.syntactic_vec, profile.name, profile.lexical_keywords, profile.pragmatic_styles)\n",
    "        results.append(InferenceResult(model=\"Modelv2(inference-only)\", neutral=item, output=output, character=profile.name))\n",
    "\n",
    "batch_inference_v2_no_CoT(muice_profile)\n",
    "batch_inference_v2_no_CoT(ayaka_profile)\n",
    "batch_inference_v2_no_CoT(zhongli_profile)\n",
    "batch_inference_v2_no_CoT(hutao_profile)\n",
    "batch_inference_v2_no_CoT(haruhi_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91229392",
   "metadata": {},
   "source": [
    "## Baseline D (Strong LLM + FS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73070bb9",
   "metadata": {},
   "source": [
    "### Define LLM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b7d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, Sequence, Tuple, Iterable\n",
    "\n",
    "from openai import OpenAI, BadRequestError\n",
    "from openai.types.chat import ChatCompletionMessageParam as ChatMsgParam\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # 从 .env 文件加载环境变量\n",
    "\n",
    "# === 配置项 ===\n",
    "DEFAULT_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"sk-PLACEHOLDER\")\n",
    "DEFAULT_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
    "DEFAULT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"glm-4.7\")\n",
    "\n",
    "ConversationTurn = Tuple[str, Optional[str]]\n",
    "\"\"\"表示一次对话轮次：(user_message, assistant_reply)。assistant_reply 可为 None。\"\"\"\n",
    "\n",
    "\n",
    "def _build_messages(\n",
    "    prompt: str,\n",
    "    history: Optional[Sequence[ConversationTurn]] = None,\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> list[ChatMsgParam]:\n",
    "    messages: list[ChatMsgParam] = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    if history:\n",
    "        for user_msg, assistant_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if assistant_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"极简 LLM 封装：初始化固定模型，提供 ask() 返回字符串。\"\"\"\n",
    "\n",
    "    model: str = DEFAULT_MODEL\n",
    "    api_key: str = DEFAULT_API_KEY\n",
    "    base_url: Optional[str] = DEFAULT_BASE_URL\n",
    "    extra_headers: Optional[Dict[str, str]] = None\n",
    "    _client: OpenAI = field(init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self._client = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            default_headers=self.extra_headers if self.extra_headers else None,\n",
    "        )\n",
    "\n",
    "    def _consume_stream(self, stream_resp: Iterable[Any]) -> str:\n",
    "        \"\"\"消费流式响应，拼接内容。\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        for chunk in stream_resp:\n",
    "            choices = getattr(chunk, \"choices\", None)\n",
    "            if not choices:\n",
    "                continue\n",
    "            delta = getattr(choices[0], \"delta\", None)\n",
    "            if delta and getattr(delta, \"content\", None):\n",
    "                chunks.append(delta.content)\n",
    "        return \"\".join(chunks)\n",
    "\n",
    "    def ask(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        history: Optional[Sequence[ConversationTurn]] = None,\n",
    "        system: Optional[str] = None,\n",
    "        *,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        retry: int = 3,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"生成回复。\n",
    "\n",
    "        参数：\n",
    "            prompt: 当前用户输入。\n",
    "            history: 可选的历史 [(user, assistant), ...]，assistant 允许为 None。\n",
    "            temperature/max_tokens/stream/kwargs：直接透传给 OpenAI Chat Completion。\n",
    "        返回：\n",
    "            模型回复的纯文本（若响应为空则返回空字符串）。\n",
    "        \"\"\"\n",
    "        messages = _build_messages(\n",
    "            prompt=prompt,\n",
    "            history=history,\n",
    "            system_prompt=system,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self._client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                stream=False,\n",
    "                extra_body={\"enable_thinking\": False},\n",
    "                **kwargs,\n",
    "            )\n",
    "        except BadRequestError as exc:\n",
    "            retry -= 1\n",
    "            if retry < 0:\n",
    "                print(f\"请求失败，重试次数耗尽：{exc}\")\n",
    "                return \"\"\n",
    "            return self.ask(prompt, history, system, temperature=temperature, top_p=top_p, max_tokens=max_tokens, retry=retry, **kwargs)\n",
    "\n",
    "        choice = response.choices[0]\n",
    "        if hasattr(choice, \"message\") and getattr(choice.message, \"content\", None):\n",
    "            return choice.message.content  # type: ignore[return-value]\n",
    "        # 兼容历史版本/异常情况\n",
    "        return getattr(choice, \"text\", \"\")\n",
    "\n",
    "\n",
    "# 初始化一个通用实例，供 Notebook 其他单元直接调用\n",
    "llm = SimpleLLMClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2955d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEFAULT_TEMPERATURE = float(os.getenv(\"NEUTRAL_TEMPERATURE\", \"0.2\"))\n",
    "DEFAULT_SLEEP_SECONDS = float(os.getenv(\"NEUTRAL_SLEEP_SECONDS\", \"0.3\"))\n",
    "\n",
    "SYSTEM_PROMPT_TEMPLATE = (\n",
    "    \"You are a style transfer expert. Your task is to rewrite the following neutral sentence into the style of {character}, based on the content of a neutral sentence.\\n\"\n",
    "    \"Constraints: \\n\"\n",
    "    \"1. Keep the original meaning unchanged. Do NOT reply to it.\\n\"\n",
    "    \"2. Use keywords: {keywords}\\n\"\n",
    "    \"3. Adopt personality: {pragmatics}\"\n",
    ")\n",
    "\n",
    "def generate_style_response(\n",
    "    neutral_sentences: str,\n",
    "    character_name: str, \n",
    "    lexical_keywords: list[str],\n",
    "    pragmatic_styles: list[str],\n",
    "    *,\n",
    "    history: list[tuple[str, str]] = [],\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    top_p: float = 0.95,\n",
    ") -> str:\n",
    "    \"\"\"调用 llm.ask 生成风格句\"\"\"\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    prompt = f\"Neutral Content: {neutral_sentences}\\nRewritten Sentence:\"\n",
    "    system = SYSTEM_PROMPT_TEMPLATE.format(character=character_name, keywords=keywords, pragmatics=pragmatics)\n",
    "\n",
    "    styled_sentence = llm.ask(prompt=prompt,\n",
    "                            history=history,\n",
    "                            system=system,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            ).strip()\n",
    "\n",
    "    return styled_sentence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728038d4",
   "metadata": {},
   "source": [
    "### Call LLM to Generate Style Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a64c9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 沐雪: 100%|██████████| 150/150 [10:01<00:00,  4.01s/it]\n",
      "Running 神里绫华: 100%|██████████| 150/150 [08:12<00:00,  3.28s/it]\n",
      "Running 钟离: 100%|██████████| 150/150 [06:42<00:00,  2.68s/it]\n",
      "Running 胡桃: 100%|██████████| 150/150 [08:17<00:00,  3.32s/it]\n",
      "Running 凉宫春日: 100%|██████████| 150/150 [09:58<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "def batch_inference_strong_llm(profile: CharacterProfile, history_shots: list[tuple[str, str]]):\n",
    "    history: list[tuple[str, str]] = []\n",
    "    for item in history_shots:\n",
    "        neutral_sentences = item[0].split(\"Neutral Content: \")[1].split(\"\\nStyle Reference Text:\")[0]\n",
    "        prompt = f\"Neutral Content: {neutral_sentences}\\nRewritten Sentence:\"\n",
    "        response = item[1]\n",
    "        history.append((prompt, response))\n",
    "\n",
    "    for item in tqdm(test_data, desc=f\"Running {profile.name}\"):\n",
    "        output = generate_style_response(item, profile.name, profile.lexical_keywords, profile.pragmatic_styles, history=history)\n",
    "        results.append(InferenceResult(model=\"BaselineD\", neutral=item, output=output, character=profile.name))\n",
    "        sleep(DEFAULT_SLEEP_SECONDS)\n",
    "\n",
    "batch_inference_strong_llm(muice_profile, Muice_shots)\n",
    "batch_inference_strong_llm(ayaka_profile, Ayaka_shots)\n",
    "batch_inference_strong_llm(zhongli_profile, Zhongli_shots)\n",
    "batch_inference_strong_llm(hutao_profile, Hutao_shots)\n",
    "batch_inference_strong_llm(haruhi_profile, Haruhi_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e806fc",
   "metadata": {},
   "source": [
    "## Export Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643c6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "OUTPUT_PATH = Path(\"./outputs/batch_run_result.jsonl\")\n",
    "OUTPUT_PATH.parent.mkdir(exist_ok=True)\n",
    "\n",
    "for item in results:\n",
    "    with open(OUTPUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}