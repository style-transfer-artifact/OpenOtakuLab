{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfc23ee",
   "metadata": {},
   "source": [
    "# Fine-tuning based on Qwen3-1.7B with Style Vector Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Remote Jupyter Book.\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "# os.chdir(\"/root/OtakuLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669a61a",
   "metadata": {},
   "source": [
    "## Define Training Set Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "352801ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "class InstructionComponents(TypedDict):\n",
    "    lexical_keywords: list[str]\n",
    "    syntactic_vector: dict[str, float]\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "class DatasetItem(TypedDict):\n",
    "    character: str\n",
    "    neutral_sentence: str\n",
    "    instruction_components: InstructionComponents | dict[str, list[str]|dict]\n",
    "    thinking_process: str\n",
    "    output: str\n",
    "\n",
    "class DatasetStorage:\n",
    "    def __init__(self) -> None:\n",
    "        self.items: dict[str, DatasetItem] = {}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def new_item(self, character: str, neutral_sentence: str, thinking_process:str, output: str):\n",
    "        item = DatasetItem(character=character,\n",
    "                           neutral_sentence=neutral_sentence,\n",
    "                           instruction_components={},\n",
    "                           thinking_process=thinking_process,\n",
    "                           output=output)\n",
    "        self.items[output] = item\n",
    "\n",
    "    def save_characters_keywords(self, character: str, lexical_keywords: list[str]):\n",
    "        saved = False\n",
    "        for item in self.items.values():\n",
    "            if item['character'] == character:\n",
    "                output = item['output']\n",
    "                self.items[output]['instruction_components']['lexical_keywords'] = lexical_keywords\n",
    "                saved = True\n",
    "        if not saved:\n",
    "            raise ValueError(f\"角色 {character} 未找到\")        \n",
    "\n",
    "    def save_component(self,\n",
    "                       output: str,\n",
    "                       syntactic_vector: Optional[dict[str, float]] = None,\n",
    "                       pragmatic_styles: Optional[list[str]] = None):\n",
    "        if output not in self.items.keys():\n",
    "            raise ValueError(f\"风格句: {output} 似乎未加载\")\n",
    "        \n",
    "        if syntactic_vector:\n",
    "            self.items[output]['instruction_components']['syntactic_vector'] = syntactic_vector\n",
    "        if pragmatic_styles:\n",
    "            self.items[output]['instruction_components']['pragmatic_styles'] = pragmatic_styles\n",
    "\n",
    "    @staticmethod\n",
    "    def _verify_validity(item: DatasetItem):\n",
    "        if not all((item['character'],\n",
    "                    item['neutral_sentence'],\n",
    "                    item['output'],\n",
    "                    item['instruction_components'])):\n",
    "            return False\n",
    "        \n",
    "        instruction_components = item['instruction_components']\n",
    "        \n",
    "        return all((instruction_components.get('lexical_keywords', None),\n",
    "                    instruction_components.get('pragmatic_styles', None),\n",
    "                    instruction_components.get(\"syntactic_vector\", None)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _oversampling(items_list: list[DatasetItem], item: DatasetItem):\n",
    "        \"\"\"\n",
    "        过采样部分学习难度大的标签\n",
    "\n",
    "        1. `tsundere` `sharp_tongued` `proud` * 5\n",
    "        2. `tsukkomi` `chuunibyou` `yandere` `airhead` * 3\n",
    "        \"\"\"\n",
    "        difficult_labels_5x = {'tsundere', 'sharp_tongued', 'proud'}\n",
    "        difficult_labels_3x = {'tsukkomi', 'chuunibyou', 'yandere', 'airhead'}\n",
    "\n",
    "        pragmatic_styles = set(item['instruction_components'].get('pragmatic_styles', []))\n",
    "        \n",
    "        if pragmatic_styles & difficult_labels_5x:\n",
    "            for _ in range(5):\n",
    "                items_list.append(item)\n",
    "        elif pragmatic_styles & difficult_labels_3x:\n",
    "            for _ in range(3):\n",
    "                items_list.append(item)\n",
    "\n",
    "    def output(self, output_path: Path, oversampling: bool = False):\n",
    "        items = list(self.items.values())\n",
    "        vaild_items = []\n",
    "        vaild_items_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if not self._verify_validity(item):\n",
    "                continue\n",
    "            vaild_items.append(item)\n",
    "            vaild_items_count += 1\n",
    "            \n",
    "            if oversampling:\n",
    "                self._oversampling(vaild_items, item)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vaild_items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"训练集已导出至 {output_path}, 总有效训练集数量: {len(vaild_items)}, 跳过数量: {len(items) - vaild_items_count}\")\n",
    "\n",
    "dataset_storage = DatasetStorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485a56e",
   "metadata": {},
   "source": [
    "## Load Neutral Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8043862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载了 6510 条训练集条目\n"
     ]
    }
   ],
   "source": [
    "neutral_sentences_jsonl_file = Path(\"./data/neutral_sentences_with_CoT.jsonl\")\n",
    "\n",
    "EN_NAME_TO_ZH = {\"Muice\": \"沐雪\", \"Ayaka\": \"神里绫华\", \"Zhongli\": \"钟离\", \"Hutao\": \"胡桃\", \"Haruhi\": \"凉宫春日\"}\n",
    "\n",
    "class NSFileItem(TypedDict):\n",
    "    character: str\n",
    "    original: str\n",
    "    neutral: str\n",
    "    CoT: str\n",
    "\n",
    "def load_jsonl_file(jsonl_file: Path):\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        jsonl_file_lines = f.readlines()\n",
    "\n",
    "    for line in jsonl_file_lines:\n",
    "        if line := line.rstrip():\n",
    "            item: NSFileItem = json.loads(line)\n",
    "            character = EN_NAME_TO_ZH.get(item[\"character\"], item[\"character\"])\n",
    "            neutral = item[\"neutral\"]\n",
    "            dataset_storage.new_item(character, neutral, item['CoT'], item[\"original\"])\n",
    "\n",
    "load_jsonl_file(neutral_sentences_jsonl_file)\n",
    "print(f\"成功加载了 {len(dataset_storage)} 条训练集条目\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e68f7",
   "metadata": {},
   "source": [
    "## Process Lexical Layer Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "717fb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_keywords_from_file(file: Path, top_n: int = 25) -> list[str]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data: dict[str, float] = json.loads(f.read())\n",
    "    return list(data.keys())[:top_n]\n",
    "\n",
    "Lexical_Muice = get_lexical_keywords_from_file(Path(\"./outputs/pmi/muice_pmi_filtered.json\"))\n",
    "Lexical_Ayaka = get_lexical_keywords_from_file(Path(\"./outputs/pmi/ayaka_pmi_filtered.json\"))\n",
    "Lexical_Zhongli = get_lexical_keywords_from_file(Path(\"./outputs/pmi/zhongli_pmi_filtered.json\"))\n",
    "Lexical_Hutao = get_lexical_keywords_from_file(Path(\"./outputs/pmi/hutao_pmi_filtered.json\"))\n",
    "Lexical_Haruhi = get_lexical_keywords_from_file(Path(\"./outputs/pmi/haruhi_pmi_filtered.json\"))\n",
    "\n",
    "dataset_storage.save_characters_keywords(\"沐雪\", Lexical_Muice)\n",
    "dataset_storage.save_characters_keywords(\"神里绫华\", Lexical_Ayaka)\n",
    "dataset_storage.save_characters_keywords(\"钟离\", Lexical_Zhongli)\n",
    "dataset_storage.save_characters_keywords(\"胡桃\", Lexical_Hutao)\n",
    "dataset_storage.save_characters_keywords(\"凉宫春日\", Lexical_Haruhi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cff993",
   "metadata": {},
   "source": [
    "## Process PCFG Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882dfadd",
   "metadata": {},
   "source": [
    "### Define PCFG Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f4e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Revised PCFG-to-Dimension Mapping ----\n",
    "rule_to_dim = {\n",
    "    # Referentiality: 专名、指称性\n",
    "    \"NP -> NR\": \"referentiality\",\n",
    "    \"NP -> PN\": \"referentiality\",\n",
    "    \"NP -> NR NN\": \"referentiality\",\n",
    "    \"DP -> DT\": \"referentiality\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "    # Interjectionality: 感叹与插入语气\n",
    "    \"INTJ -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> IJ\": \"interjectionality\",\n",
    "    \"FLR -> SP\": \"interjectionality\",\n",
    "    \"IP -> INTJ PU VP\": \"interjectionality\",\n",
    "    \"IP -> INTJ VP\": \"interjectionality\",\n",
    "\n",
    "    # Declarativity: 陈述语气/句型层级\n",
    "    \"CP -> IP SP\": \"declarativity\",\n",
    "    \"CP -> IP SP PU\": \"declarativity\",\n",
    "    \"CP -> IP DEC\": \"declarativity\",\n",
    "    \"TOP -> CP\": \"declarativity\",\n",
    "    \"CP -> CP\": \"declarativity\",\n",
    "    \"TOP -> IP\": \"declarativity\",\n",
    "    \"VP -> VC NP\": \"declarativity\",\n",
    "\n",
    "    # Clausal Embedding: 子句嵌套\n",
    "    \"VP -> VV IP\": \"clausal_embedding\",\n",
    "    \"LCP -> IP LC\": \"clausal_embedding\",\n",
    "    \"IP -> ADVP PU NP VP\": \"clausal_embedding\",\n",
    "    \"NP -> CP NP\": \"clausal_embedding\",\n",
    "\n",
    "    # Subordination: 修饰性从属结构\n",
    "    \"VP -> ADVP VP\": \"subordination\",\n",
    "    \"IP -> NP VP\": \"subordination\",\n",
    "    \"IP -> VP\": \"subordination\",\n",
    "    \"VP -> VV NP\": \"subordination\",\n",
    "    \"VP -> VA\": \"subordination\",\n",
    "    \"IP -> VP SP\": \"subordination\",\n",
    "    \"VP -> PP VP\": \"subordination\",\n",
    "\n",
    "    # Parallelism: 句式并列\n",
    "    \"VP -> VP PU VP\": \"parallelism\",\n",
    "    \"VP -> VP PU VP PU VP\": \"parallelism\",\n",
    "    \"UCP -> IP PU CP\": \"parallelism\",\n",
    "    \"IP -> VP PU\": \"parallelism\",\n",
    "    \"TOP -> UCP\": \"parallelism\",\n",
    "\n",
    "    # Coordination Density: 并列结构复杂度\n",
    "    \"NP -> NP CC NP\": \"coordination_density\",\n",
    "    \"NP -> NN CC NN\": \"coordination_density\",\n",
    "    \"CP -> CP CC CP\": \"coordination_density\",\n",
    "\n",
    "    # Modifier Density: 修饰成分密度\n",
    "    \"DNP -> ADJP DEG\": \"modifier_density\",\n",
    "    \"DNP -> NP DEG\": \"modifier_density\",\n",
    "    \"NP -> DNP NP\": \"modifier_density\",\n",
    "    \"ADVP -> AD\": \"modifier_density\",\n",
    "    \"ADJP -> JJ\": \"modifier_density\",\n",
    "\n",
    "    # Nominal Complexity: 名词短语复杂度\n",
    "    \"NP -> ADJP NP\": \"nominal_complexity\",\n",
    "    \"NP -> DNP NP\": \"nominal_complexity\",\n",
    "    \"NP -> NN NN\": \"nominal_complexity\",\n",
    "\n",
    "    # Prepositional Density: 介词结构使用密度\n",
    "    \"PP -> P LCP\": \"prepositional_density\",\n",
    "    \"PP -> P NP\": \"prepositional_density\",\n",
    "\n",
    "    # Topic Fronting: 话题提前结构\n",
    "    \"TOP -> NP IP\": \"topic_fronting\",\n",
    "\n",
    "    # Ellipsis or Fragmentation: 口语省略、残缺句\n",
    "    \"IP -> VP\": \"ellipsis_or_fragmentation\",\n",
    "    \"IP -> ADVP VP\": \"ellipsis_or_fragmentation\",\n",
    "\n",
    "    # Syntactic Compression: 句法压缩\n",
    "    \"NP -> NN\": \"syntactic_compression\",\n",
    "    \"VP -> VV\": \"syntactic_compression\",\n",
    "    \"VP -> VV VP\": \"syntactic_compression\",\n",
    "\n",
    "    # Quantificationality: 数量词使用\n",
    "    \"CLP -> M\": \"quantificationality\",\n",
    "    \"QP -> CD CLP\": \"quantificationality\",\n",
    "    \"NP -> DP NP\": \"quantificationality\",\n",
    "    \"NP -> QP CP NP\": \"quantificationality\",\n",
    "\n",
    "    # Deep Embedding: 深层嵌套\n",
    "    \"VP -> VV NP IP\": \"clausal_embedding\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a57e0",
   "metadata": {},
   "source": [
    "### Extract PCFG Frequency from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "025b895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rules processed: 70045, Unmapped rules: 16141(23.04%), Mapped rules: 53904(76.96%)\n",
      "\n",
      "Top-5 unmapped PCFG rules:\n",
      "  VP -> ADVP ADVP VP                                 Frequency: 466\n",
      "  NP -> NT                                           Frequency: 412\n",
      "  NP -> QP NP                                        Frequency: 408\n",
      "  VP -> VP VP                                        Frequency: 346\n",
      "  IP -> ADVP NP VP                                   Frequency: 283\n",
      "\n",
      "Total rules processed: 61916, Unmapped rules: 11595(18.73%), Mapped rules: 50321(81.27%)\n",
      "\n",
      "Top-5 unmapped PCFG rules:\n",
      "  QP -> CD                                           Frequency: 394\n",
      "  VP -> VE NP                                        Frequency: 375\n",
      "  VP -> VV AS NP                                     Frequency: 327\n",
      "  LCP -> NP LC                                       Frequency: 314\n",
      "  ADVP -> CS                                         Frequency: 289\n",
      "\n",
      "Total rules processed: 21896, Unmapped rules: 4446(20.31%), Mapped rules: 17450(79.69%)\n",
      "\n",
      "Top-5 unmapped PCFG rules:\n",
      "  VP -> VE NP                                        Frequency: 146\n",
      "  QP -> CD                                           Frequency: 127\n",
      "  VP -> VV AS NP                                     Frequency: 123\n",
      "  LCP -> NP LC                                       Frequency: 116\n",
      "  ADVP -> CS                                         Frequency: 113\n",
      "\n",
      "Total rules processed: 40463, Unmapped rules: 8251(20.39%), Mapped rules: 32212(79.61%)\n",
      "\n",
      "Top-5 unmapped PCFG rules:\n",
      "  QP -> CD                                           Frequency: 307\n",
      "  VP -> VE NP                                        Frequency: 280\n",
      "  QP -> CLP                                          Frequency: 225\n",
      "  NP -> QP NP                                        Frequency: 217\n",
      "  VP -> ADVP NP                                      Frequency: 217\n",
      "\n",
      "Total rules processed: 34012, Unmapped rules: 6820(20.05%), Mapped rules: 27192(79.95%)\n",
      "\n",
      "Top-5 unmapped PCFG rules:\n",
      "  QP -> CD                                           Frequency: 209\n",
      "  NP -> NP NP                                        Frequency: 201\n",
      "  VP -> VE NP                                        Frequency: 198\n",
      "  ADVP -> CS                                         Frequency: 165\n",
      "  NP -> QP NP                                        Frequency: 157\n",
      "\n",
      "为角色 沐雪 保存 PCFG 特征向量: {'declarativity': 0.1103257643217572, 'parallelism': 0.02918150786583556, 'ellipsis_or_fragmentation': 0.08529979222321163, 'subordination': 0.19688705847432472, 'interjectionality': 0.008644998515880083, 'clausal_embedding': 0.034784060552092606, 'referentiality': 0.11624369249035323, 'syntactic_compression': 0.18596022558622738, 'nominal_complexity': 0.03342980112793114, 'coordination_density': 0.002615761353517364, 'quantificationality': 0.038197536360937964, 'modifier_density': 0.1393217571979816, 'prepositional_density': 0.01910804392994954}\n",
      "为角色 神里绫华 保存 PCFG 特征向量: {'declarativity': 0.09320164543629895, 'parallelism': 0.029192583613203236, 'ellipsis_or_fragmentation': 0.061485264601259915, 'subordination': 0.18419745235587529, 'clausal_embedding': 0.046163629498618866, 'interjectionality': 0.002046859164166054, 'syntactic_compression': 0.1999165358399078, 'nominal_complexity': 0.05623894596689255, 'referentiality': 0.10019673694878878, 'coordination_density': 0.02416486158860118, 'quantificationality': 0.042964170028417556, 'modifier_density': 0.13753701238051708, 'prepositional_density': 0.022694302577452752}\n",
      "为角色 钟离 保存 PCFG 特征向量: {'declarativity': 0.09879656160458453, 'parallelism': 0.029398280802292263, 'ellipsis_or_fragmentation': 0.06412607449856733, 'subordination': 0.1839541547277937, 'clausal_embedding': 0.037478510028653295, 'interjectionality': 0.003151862464183381, 'syntactic_compression': 0.21077363896848136, 'quantificationality': 0.04022922636103152, 'referentiality': 0.08240687679083095, 'nominal_complexity': 0.06372492836676218, 'coordination_density': 0.02332378223495702, 'modifier_density': 0.14068767908309457, 'prepositional_density': 0.02194842406876791}\n",
      "为角色 胡桃 保存 PCFG 特征向量: {'parallelism': 0.03191357258164659, 'declarativity': 0.10471252949211474, 'ellipsis_or_fragmentation': 0.07646218800447038, 'clausal_embedding': 0.042685955544517575, 'subordination': 0.18254066807400968, 'interjectionality': 0.01809884515087545, 'syntactic_compression': 0.1987458090152738, 'referentiality': 0.09049422575437725, 'quantificationality': 0.05016763938904756, 'nominal_complexity': 0.034459207748665094, 'coordination_density': 0.013845771762076246, 'modifier_density': 0.1413448404321371, 'prepositional_density': 0.014528747050788526}\n",
      "为角色 凉宫春日 保存 PCFG 特征向量: {'declarativity': 0.0939982347749338, 'parallelism': 0.032288908502500734, 'subordination': 0.19174757281553398, 'ellipsis_or_fragmentation': 0.07542659605766402, 'clausal_embedding': 0.042144748455428066, 'interjectionality': 0.008017063842306561, 'syntactic_compression': 0.18409826419535158, 'quantificationality': 0.040453074433656956, 'referentiality': 0.12099146807884673, 'nominal_complexity': 0.043211238599588114, 'coordination_density': 0.01195204471903501, 'prepositional_density': 0.011253309796999117, 'modifier_density': 0.14441747572815533}\n",
      "成功构建并保存 PCFG 特征向量\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import Any, Dict, Iterable, Tuple\n",
    "\n",
    "class PCFGExtractor:\n",
    "    def __init__(self) -> None:\n",
    "        self.rules_counter: Dict[str, Counter[Tuple[str, ...]]] = defaultdict(Counter)\n",
    "        self.total_rules: int = 0\n",
    "\n",
    "    def load_trees(self, file_path: Path) -> list[dict[str, Any]]:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def extract_rules_from_tree(self, tree: Any) -> None:\n",
    "        if not isinstance(tree, list) or len(tree) != 2:\n",
    "            return\n",
    "        lhs_symbol, rhs = tree\n",
    "        if isinstance(rhs, list) and all(isinstance(child, list) and len(child) == 2 for child in rhs):\n",
    "            rhs_symbols = tuple(child[0] for child in rhs)\n",
    "            self.rules_counter[lhs_symbol][rhs_symbols] += 1\n",
    "            self.total_rules += 1\n",
    "            for child in rhs:\n",
    "                self.extract_rules_from_tree(child)\n",
    "\n",
    "    def extract_from_data(self, data: Iterable[dict[str, Any]]) -> None:\n",
    "        for item in data:\n",
    "            for tree in item.get(\"con\", []):\n",
    "                self.extract_rules_from_tree(tree)\n",
    "\n",
    "    def build_feature_vectors(self) -> dict[str, float]:\n",
    "        feature_vector: dict[str, float] = defaultdict(float)\n",
    "        unmapped_rules_counter = Counter()\n",
    "        unmapped_rules_count = 0\n",
    "\n",
    "        for lhs_symbol, rhs_counter in self.rules_counter.items():\n",
    "            for rhs_symbols, freq in rhs_counter.items():\n",
    "                rule = f\"{lhs_symbol} -> {' '.join(rhs_symbols)}\"\n",
    "                dim = rule_to_dim.get(rule)\n",
    "                if not dim:\n",
    "                    unmapped_rules_count += freq\n",
    "                    unmapped_rules_counter[rule] += freq\n",
    "                    continue\n",
    "                feature_vector[dim] += freq\n",
    "\n",
    "        total = sum(feature_vector.values()) or 1.0\n",
    "\n",
    "        print(f\"Total rules processed: {self.total_rules}, \"\n",
    "              f\"Unmapped rules: {unmapped_rules_count}({unmapped_rules_count / self.total_rules * 100:.2f}%), \"\n",
    "              f\"Mapped rules: {self.total_rules - unmapped_rules_count}({(self.total_rules - unmapped_rules_count) / self.total_rules * 100:.2f}%)\\n\")\n",
    "\n",
    "        if unmapped_rules_counter:\n",
    "            print(\"Top-5 unmapped PCFG rules:\")\n",
    "            for rule, freq in unmapped_rules_counter.most_common(5):\n",
    "                print(f\"  {rule:<50} Frequency: {freq}\")\n",
    "        else:\n",
    "            print(\"All rules have been successfully mapped ✅\")\n",
    "        \n",
    "        print()\n",
    "        return {dim: freq / total for dim, freq in feature_vector.items()}\n",
    "\n",
    "def load_and_extract(path: Path) -> dict[str, float]:\n",
    "    extractor = PCFGExtractor()\n",
    "    trees_data = extractor.load_trees(path)\n",
    "    extractor.extract_from_data(trees_data)\n",
    "    return extractor.build_feature_vectors()\n",
    "\n",
    "def save_vector_for_character(character: str, vector: dict[str, float]) -> None:\n",
    "    print(f\"为角色 {character} 保存 PCFG 特征向量: {vector}\")\n",
    "\n",
    "    stored = False\n",
    "    for item in dataset_storage.items.values():\n",
    "        if item[\"character\"] == character:\n",
    "            dataset_storage.save_component(item[\"output\"], syntactic_vector=vector)\n",
    "            stored = True\n",
    "    if not stored:\n",
    "        raise ValueError(f\"角色 {character} 的 PCFG 特征未匹配到任何句子\")\n",
    "\n",
    "\n",
    "pcfg_muice_vector = load_and_extract(Path(\"./outputs/cons/muice.json\"))\n",
    "pcfg_ayaka_vector = load_and_extract(Path(\"./outputs/cons/ayaka.json\"))\n",
    "pcfg_zhongli_vector = load_and_extract(Path(\"./outputs/cons/zhongli.json\"))\n",
    "pcfg_hutao_vector = load_and_extract(Path(\"./outputs/cons/hutao.json\"))\n",
    "pcfg_haruhi_vector = load_and_extract(Path(\"./outputs/cons/haruhi.json\"))\n",
    "\n",
    "save_vector_for_character(\"沐雪\", pcfg_muice_vector)\n",
    "save_vector_for_character(\"神里绫华\", pcfg_ayaka_vector)\n",
    "save_vector_for_character(\"钟离\", pcfg_zhongli_vector)\n",
    "save_vector_for_character(\"胡桃\", pcfg_hutao_vector)\n",
    "save_vector_for_character(\"凉宫春日\", pcfg_haruhi_vector)\n",
    "print(\"成功构建并保存 PCFG 特征向量\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e430c0",
   "metadata": {},
   "source": [
    "## Process Pragmatic Style Layer Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84cb00",
   "metadata": {},
   "source": [
    "### Read Pragmatic Style Files for Each Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a28e212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新了 7377 条训练集条目的风格向量，已跳过: 714 条\n"
     ]
    }
   ],
   "source": [
    "Pragmatic_Muice = \"./outputs/pragmatic/muice.jsonl\"\n",
    "Pragmatic_ayaka = \"./outputs/pragmatic/ayaka.jsonl\"\n",
    "Pragmatic_zhongli = \"./outputs/pragmatic/zhongli.jsonl\"\n",
    "Pragmatic_hutao = \"./outputs/pragmatic/hutao.jsonl\"\n",
    "Pragmatic_haruhi = \"./outputs/pragmatic/haruhi.jsonl\"\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "class PCFGItem(TypedDict):\n",
    "    response: str\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "def read_pcfg_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None) -> list[PCFGItem]:\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[PCFGItem] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_item: RawPCFGItem = json.loads(line)\n",
    "            raw_items.append(raw_item)\n",
    "\n",
    "    # list[dict[str, float]] -> dict[str, float] -> list[str]\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item[\"pragmatic_styles\"]\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        threshold = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > threshold:\n",
    "                final_styles.append(key)\n",
    "        \n",
    "        item = PCFGItem(response=raw_item[\"response\"], pragmatic_styles=final_styles)\n",
    "        items.append(item)\n",
    "\n",
    "    return items\n",
    "\n",
    "pcfg_muice_items = read_pcfg_jsonl_file(Path(Pragmatic_Muice), 0.4)\n",
    "pcfg_ayaka_items = read_pcfg_jsonl_file(Path(Pragmatic_ayaka), 0.4)\n",
    "pcfg_zhongli_items = read_pcfg_jsonl_file(Path(Pragmatic_zhongli), 0.4)\n",
    "pcfg_hutao_items = read_pcfg_jsonl_file(Path(Pragmatic_hutao), 0.4)\n",
    "pcfg_haruhi_items = read_pcfg_jsonl_file(Path(Pragmatic_haruhi), 0.4)\n",
    "\n",
    "pcfg_import_items = pcfg_muice_items + pcfg_ayaka_items + pcfg_zhongli_items + pcfg_hutao_items + pcfg_haruhi_items\n",
    "\n",
    "skiped = 0\n",
    "for item in pcfg_import_items:\n",
    "    try:\n",
    "        dataset_storage.save_component(item[\"response\"], pragmatic_styles=item[\"pragmatic_styles\"])\n",
    "    except ValueError as e:\n",
    "        skiped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"更新了 {len(pcfg_import_items)} 条训练集条目的风格向量，已跳过: {skiped} 条\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47864380",
   "metadata": {},
   "source": [
    "## Export Training Set File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74c90633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集已导出至 dataset\\llm_train.json, 总有效训练集数量: 5786, 跳过数量: 724\n",
      "训练集已导出至 dataset\\llm_train_oversampling.json, 总有效训练集数量: 6997, 跳过数量: 724\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = Path(\"./data/llm_train.json\")\n",
    "OVERSAMPLING_OUTPUT_PATH = Path(\"./data/llm_train_oversampling.json\")\n",
    "\n",
    "dataset_storage.output(OUTPUT_PATH)\n",
    "dataset_storage.output(OVERSAMPLING_OUTPUT_PATH, oversampling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50789b",
   "metadata": {},
   "source": [
    "## Define Model Structure & Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe09323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from typing_extensions import TypedDict\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from pathlib import Path\n",
    "from transformers import Qwen3ForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "class InstructionComponents(TypedDict):\n",
    "    lexical_keywords: list[str]\n",
    "    syntactic_vector: dict[str, float]\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "class DatasetItem(TypedDict):\n",
    "    character: str\n",
    "    neutral_sentence: str\n",
    "    instruction_components: InstructionComponents\n",
    "    thinking_process: str\n",
    "    output: str\n",
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, path:Path, tokenizer, prag_style_vocab: list[str], syntactic_dims: list[str], max_length=256):\n",
    "        self.data: list[DatasetItem] = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prag_vocab = {style: i for i, style in enumerate(prag_style_vocab)}\n",
    "        self.syntactic_dims = syntactic_dims\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        item = self.data[idx]\n",
    "        instr = item[\"instruction_components\"]\n",
    "\n",
    "        # --- (1) 生成提示-输出对 ---\n",
    "        keywords = \", \".join(instr[\"lexical_keywords\"]) if instr[\"lexical_keywords\"] else \"None\"\n",
    "        pragmatic_styles = \", \".join(instr[\"pragmatic_styles\"]) if instr[\"pragmatic_styles\"] else \"None\"\n",
    "        system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "        user_prompt = (\n",
    "            f\"Target Character {item['character']}\\n\"\n",
    "            f\"Personality: {pragmatic_styles}\\n\"\n",
    "            f\"Keywords: {keywords}\\n\"\n",
    "            f\"Neutral Content: {item['neutral_sentence']}\\n\"\n",
    "        )\n",
    "        assistant_response = f\"{item['thinking_process']}\\n\\n{item['output']}\"\n",
    "\n",
    "        # --- (2) Tokenize 完整文本 ---\n",
    "        full_text = self.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        )\n",
    "\n",
    "        self.tokenizer.truncation_side = \"left\"  # 保留结尾部分\n",
    "        full_tokenized = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "        input_ids = full_tokenized[\"input_ids\"] \n",
    "\n",
    "        # --- (2) 构建只包含答案部分的 labels ---\n",
    "        prompt_only_text = self.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                # 注意这里，我们使用 add_generation_prompt 来获取到 \"assistant\" 角色提示\n",
    "                # 这是为了精确计算 prompt 的长度\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompt_len = len(self.tokenizer(prompt_only_text).input_ids)\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        # 将 prompt 部分的 labels 设为 -100\n",
    "        labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "        # --- (3) 句法和语用向量 (multi-hot) ---\n",
    "        syntactic_vec = torch.tensor(\n",
    "            [instr[\"syntactic_vector\"].get(dim, 0.0) for dim in self.syntactic_dims],\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        prag_vec = torch.zeros(len(self.prag_vocab), dtype=torch.float32)\n",
    "\n",
    "        for tag in instr[\"pragmatic_styles\"]:\n",
    "            if tag in self.prag_vocab:\n",
    "                prag_vec[self.prag_vocab[tag]] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"syntactic_vec\": syntactic_vec,\n",
    "            \"prag_vec\": prag_vec,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "class StyleDataCollator(DataCollatorWithPadding):\n",
    "    \"\"\"动态 padding + 风格特征拼接 + 手动处理 labels\"\"\"\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, padding=True, **kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # 1. 从 features 中分离出风格向量和 labels\n",
    "        syntactic_vecs = torch.stack([f[\"syntactic_vec\"] for f in features])\n",
    "        prag_vecs = torch.stack([f[\"prag_vec\"] for f in features])\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "\n",
    "        # 2. 调用父类的 collator，它现在只会处理 input_ids\n",
    "        # 这会正确地 padding input_ids 并生成 attention_mask\n",
    "        base_features = [\n",
    "            {k: v for k, v in f.items() if k not in (\"syntactic_vec\", \"prag_vec\", \"labels\")}\n",
    "            for f in features\n",
    "        ]\n",
    "        batch = super().__call__(base_features)\n",
    "\n",
    "        # 3. 手动 padding a `labels`\n",
    "        # 获取 batch 中 input_ids 被 padding 到的最大长度\n",
    "        max_label_length = batch[\"input_ids\"].shape[1]\n",
    "        \n",
    "        # 对每个 label 序列进行 padding，使其长度与 input_ids 一致\n",
    "        padded_labels = []\n",
    "        for label_seq in labels:\n",
    "            # 首先，将 label 序列截断到与 input_ids 相同的最大长度\n",
    "            truncated_label_seq = label_seq[:max_label_length]\n",
    "\n",
    "            # 然后，计算基于截断后长度的 padding\n",
    "            padding_length = max_label_length - len(truncated_label_seq)\n",
    "            \n",
    "            # 最后，添加 padding\n",
    "            padded_labels.append(truncated_label_seq + [-100] * padding_length)\n",
    "        \n",
    "        # 将 padding 好的 labels list 转换为 tensor 并放入 batch\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        # 4. 把风格特征重新放回 batch\n",
    "        batch[\"syntactic_vec\"] = syntactic_vecs\n",
    "        batch[\"prag_vec\"] = prag_vecs\n",
    "        return batch\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        \"\"\"\n",
    "        input_dim: 句法向量的维度 (开始时是 8, 扩展后可能是 12-20)\n",
    "        out_dim: 必须等于 `base_model.config.hidden_size` (必须与 Qwen 的 hidden_size 匹配)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 我们可以使用一个简单的 MLP 来映射\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim // 2, out_dim),\n",
    "            nn.Tanh() # Tanh 将输出归一化到 [-1, 1] 作为一个稳定的 \"软提示\"\n",
    "        )\n",
    "\n",
    "    def forward(self, syntactic_vec):\n",
    "        # syntactic_vec: [batch_size, input_dim]\n",
    "        # return: [batch_size, out_dim]\n",
    "        return self.proj(syntactic_vec)\n",
    "    \n",
    "class StyleConditionedLoRAModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str|Path, syntactic_dim: int, lora_r=16, lora_alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 加载基础模型\n",
    "        base_model = Qwen3ForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            dtype=torch.bfloat16 # 使用 bfloat16 节省显存\n",
    "        )\n",
    "        \n",
    "        # 2. 定义 LoRA 配置\n",
    "        # 目标模块 \"key/query/value\"\n",
    "        # 在 Qwen3-1.7B 中，它们通常被称为 \"q_proj\", \"k_proj\", \"v_proj\"\n",
    "        config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # 覆盖所有线性层以获得最大表达力\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        # 3. 应用 LoRA\n",
    "        self.peft_model: PeftModel = get_peft_model(base_model, config)  # type:ignore\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "        \n",
    "        # 4. 初始化 StyleEncoder\n",
    "        # 它的输出必须匹配模型的隐藏层维度\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        self.style_encoder = StyleEncoder(syntactic_dim, self.hidden_size)\n",
    "        # Cast the style encoder to match the base model's dtype\n",
    "        self.style_encoder.to(base_model.dtype)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        # 获取基础模型的词嵌入层\n",
    "        return self.peft_model.get_input_embeddings()  # type:ignore\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, syntactic_vec):\n",
    "        \"\"\"\n",
    "        这个 forward 函数实现了方案 C.1 (输入拼接)\n",
    "        并为方案 D (辅助损失) 做好准备\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. 计算 Style Embedding\n",
    "        # syntactic_vec: [batch_size, syntactic_dim]\n",
    "        # style_emb: [batch_size, hidden_size]\n",
    "        style_emb = self.style_encoder(syntactic_vec.to(self.peft_model.dtype)) # type: ignore\n",
    "        \n",
    "        # 2. 将 style_emb 视为一个 \"Prefix\" 软提示\n",
    "        # 变为: [batch_size, 1, hidden_size]\n",
    "        style_emb_prefix = style_emb.unsqueeze(1)\n",
    "        \n",
    "        # 3. 获取原始的 Token 词嵌入\n",
    "        # token_embeds: [batch_size, seq_len, hidden_size]\n",
    "        token_embeds = self.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # 4. 拼接 Style Prefix 和 Token 嵌入\n",
    "        # inputs_embeds: [batch_size, 1 + seq_len, hidden_size]\n",
    "        inputs_embeds = torch.cat([style_emb_prefix, token_embeds], dim=1)\n",
    "        \n",
    "        # 5. 修正 Attention Mask\n",
    "        # 我们需要在 mask 的开头添加一个 \"1\" (代表 style prefix)\n",
    "        prefix_mask = torch.ones(\n",
    "            attention_mask.shape[0], 1,\n",
    "            dtype=torch.long, \n",
    "            device=attention_mask.device\n",
    "        )\n",
    "        # new_attention_mask: [batch_size, 1 + seq_len]\n",
    "        new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # 6. 修正 Labels\n",
    "        # 我们需要 L_lm 忽略 style prefix 部分\n",
    "        # 在 labels 的开头添加一个 \"-100\"\n",
    "        prefix_labels = torch.full(\n",
    "            (labels.shape[0], 1), -100, \n",
    "            dtype=torch.long, \n",
    "            device=labels.device\n",
    "        )\n",
    "        # new_labels: [batch_size, 1 + seq_len]\n",
    "        new_labels = torch.cat([prefix_labels, labels], dim=1)\n",
    "        \n",
    "        # 7. 执行模型的前向传播\n",
    "        # 我们请求 hidden_states 以便计算辅助损失\n",
    "        outputs = self.peft_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=new_attention_mask,\n",
    "            labels=new_labels,\n",
    "            output_hidden_states=True \n",
    "        )\n",
    "        \n",
    "        # 8. 返回计算损失所需的所有组件\n",
    "        # L_lm: outputs.loss\n",
    "        # L_recon / L_style_cls: 需要 outputs.hidden_states 和 syntactic_vec/pragmatic_tags\n",
    "        \n",
    "        return {\n",
    "            \"loss\": outputs.loss,  # L_lm\n",
    "            \"logits\": outputs.logits,\n",
    "            # 返回最后一层 hidden_state，用于计算辅助损失\n",
    "            # hidden_states 是一个元组，最后一个元素是 [batch_size, 1 + seq_len, hidden_size]\n",
    "            \"last_hidden_state\": outputs.hidden_states[-1] \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee298b9",
   "metadata": {},
   "source": [
    "## Load Model & Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc76024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Successfully loaded /root/autodl-tmp/Qwen3-1.7B/\n",
      "Loading Dataset...\n",
      "Successfully loaded dataset, length: 6997\n",
      "Preparing for train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2912b58488421e8d273c634eacaa88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=64, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.optimization import get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MODEL_PATH = \"/root/autodl-tmp/Qwen3-1.7B/\"\n",
    "# DATASET_PATH = Path(\"/root/OtakuLab/dataset/llm_train.json\")\n",
    "DATASET_PATH = Path(\"/root/OtakuLab/dataset/llm_train_oversampling.json\")\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCH = 2\n",
    "\n",
    "# 损失函数的权重\n",
    "lambda_recon = 0.05  # L_recon (句法) 在前期收敛太快 \n",
    "lambda_style = 0.5\n",
    "\n",
    "# 定义风格向量信息\n",
    "\n",
    "syntactic_dims = ['syntactic_compression', 'declarativity', 'clausal_embedding', 'nominal_complexity', 'subordination', 'interjectionality',\n",
    "                  'ellipsis_or_fragmentation', 'modifier_density', 'prepositional_density', 'topic_fronting', 'referentiality', 'parallelism',\n",
    "                  'quantificationality', 'coordination_density']\n",
    "prag_style_vocab = ['kind', 'modest', 'clingy', 'playful', 'cold', 'proud', 'sharp_tongued', 'subservient', 'submissive', 'controlling',\n",
    "                    'strong', 'defensive', 'tsukkomi', 'rational', 'curious', 'imaginative', 'cautious', 'idealistic', 'conservative',\n",
    "                    'radical', 'obsessive', 'hesitant', 'energetic', 'optimistic', 'confident', 'passionate', 'melancholy', 'serious',\n",
    "                    'emotional', 'sensitive', 'shy', 'irritable', 'anxious', 'lazy', 'tsundere', 'yandere', 'chuunibyou', 'cute', 'naive',\n",
    "                    'airhead', 'elegant', 'humorous', 'loyal', 'responsible', 'willful', 'antisocial', 'talkative', 'masochistic', 'sadistic', 'evil']\n",
    "\n",
    "syntactic_dim_length = len(syntactic_dims)\n",
    "pragmatic_dim_length = len(prag_style_vocab)\n",
    "\n",
    "# 加载模型和训练数据\n",
    "\n",
    "print(\"Loading Model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    # Qwen 通常使用 <|endoftext|> 作为 pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(f\"Set tokenizer.pad_token to {tokenizer.eos_token}\")\n",
    "\n",
    "print(f\"Successfully loaded {MODEL_PATH}\")\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "dataset = StyleDataset(DATASET_PATH, tokenizer, prag_style_vocab, syntactic_dims)\n",
    "train_data, val_data = train_test_split(dataset, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "collator = StyleDataCollator(tokenizer, pad_to_multiple_of=8)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator, persistent_workers=False)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "print(f\"Successfully loaded dataset, length: {len(dataset)}\")\n",
    "\n",
    "print(\"Preparing for train...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "styled_model = StyleConditionedLoRAModel(MODEL_PATH, syntactic_dim_length, lora_r=16, lora_alpha=16)\n",
    "\n",
    "# 定义辅助损失函数\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss() # 用于 L_style_cls\n",
    "\n",
    "style_predictor_head = nn.Linear(styled_model.hidden_size, syntactic_dim_length + pragmatic_dim_length).to(device)\n",
    "all_parameters = list(styled_model.parameters()) + list(style_predictor_head.parameters())\n",
    "optimizer = torch.optim.AdamW(all_parameters, lr=LEARNING_RATE)\n",
    "num_training_steps = len(train_loader) * EPOCH\n",
    "lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "\n",
    "styled_model.to(device)\n",
    "style_predictor_head.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159184ee",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede43dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4351eb5103411cb583ed3eb9981342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25: Train L_total: 4.7180, Train L_lm 4.1680, Train L_recon 1.1712, Train L_style 0.7672\n",
      "Step 25: Val PPL: 81.4830, Val L_recon: 1.1497, Val L_style: 0.7640\n",
      "Step 50: Train L_total: 4.3247, Train L_lm 3.7747, Train L_recon 0.8670, Train L_style 0.7022\n",
      "Step 50: Val PPL: 71.1566, Val L_recon: 0.8443, Val L_style: 0.6911\n",
      "Step 75: Train L_total: 4.0000, Train L_lm 3.4500, Train L_recon 0.4160, Train L_style 0.5766\n",
      "Step 75: Val PPL: 41.0008, Val L_recon: 0.3985, Val L_style: 0.5847\n",
      "Step 100: Train L_total: 3.7942, Train L_lm 3.2442, Train L_recon 0.0738, Train L_style 0.4661\n",
      "Step 100: Val PPL: 19.6680, Val L_recon: 0.0647, Val L_style: 0.4530\n",
      "Step 125: Train L_total: 2.9476, Train L_lm 2.3976, Train L_recon 0.0016, Train L_style 0.3406\n",
      "Step 125: Val PPL: 13.0148, Val L_recon: 0.0008, Val L_style: 0.3312\n",
      "Step 150: Train L_total: 2.7074, Train L_lm 2.1574, Train L_recon 0.0003, Train L_style 0.2634\n",
      "Step 150: Val PPL: 11.0672, Val L_recon: 0.0003, Val L_style: 0.2502\n",
      "Step 175: Train L_total: 2.7975, Train L_lm 2.2475, Train L_recon 0.0006, Train L_style 0.1992\n",
      "Step 175: Val PPL: 10.0174, Val L_recon: 0.0006, Val L_style: 0.1941\n",
      "Step 200: Train L_total: 2.9712, Train L_lm 2.4212, Train L_recon 0.0014, Train L_style 0.1616\n",
      "Step 200: Val PPL: 9.1179, Val L_recon: 0.0004, Val L_style: 0.1739\n",
      "Step 225: Train L_total: 2.7444, Train L_lm 2.1944, Train L_recon 0.0002, Train L_style 0.2014\n",
      "Step 225: Val PPL: 8.4232, Val L_recon: 0.0003, Val L_style: 0.1627\n",
      "Step 250: Train L_total: 2.6082, Train L_lm 2.0582, Train L_recon 0.0008, Train L_style 0.1426\n",
      "Step 250: Val PPL: 7.8437, Val L_recon: 0.0004, Val L_style: 0.1602\n",
      "Step 275: Train L_total: 2.7391, Train L_lm 2.1891, Train L_recon 0.0004, Train L_style 0.1607\n",
      "Step 275: Val PPL: 7.3508, Val L_recon: 0.0002, Val L_style: 0.1592\n",
      "Step 300: Train L_total: 2.7469, Train L_lm 2.1969, Train L_recon 0.0002, Train L_style 0.2339\n",
      "Step 300: Val PPL: 6.9392, Val L_recon: 0.0003, Val L_style: 0.1589\n",
      "Step 325: Train L_total: 2.3628, Train L_lm 1.8128, Train L_recon 0.0007, Train L_style 0.1990\n",
      "Step 325: Val PPL: 6.6271, Val L_recon: 0.0004, Val L_style: 0.1593\n",
      "Step 350: Train L_total: 2.6356, Train L_lm 2.0856, Train L_recon 0.0009, Train L_style 0.1652\n",
      "Step 350: Val PPL: 6.4157, Val L_recon: 0.0004, Val L_style: 0.1581\n",
      "Step 375: Train L_total: 2.3244, Train L_lm 1.7744, Train L_recon 0.0006, Train L_style 0.1526\n",
      "Step 375: Val PPL: 6.2137, Val L_recon: 0.0003, Val L_style: 0.1581\n",
      "Step 400: Train L_total: 2.8107, Train L_lm 2.2607, Train L_recon 0.0009, Train L_style 0.1403\n",
      "Step 400: Val PPL: 6.0439, Val L_recon: 0.0010, Val L_style: 0.1580\n",
      "Step 425: Train L_total: 2.0429, Train L_lm 1.4929, Train L_recon 0.0007, Train L_style 0.1647\n",
      "Step 425: Val PPL: 5.9204, Val L_recon: 0.0005, Val L_style: 0.1576\n",
      "Step 450: Train L_total: 2.2477, Train L_lm 1.6977, Train L_recon 0.0005, Train L_style 0.2077\n",
      "Step 450: Val PPL: 5.7918, Val L_recon: 0.0004, Val L_style: 0.1576\n",
      "Step 475: Train L_total: 2.3530, Train L_lm 1.8030, Train L_recon 0.0005, Train L_style 0.1423\n",
      "Step 475: Val PPL: 5.7047, Val L_recon: 0.0004, Val L_style: 0.1573\n",
      "Step 500: Train L_total: 2.3977, Train L_lm 1.8477, Train L_recon 0.0004, Train L_style 0.1762\n",
      "Step 500: Val PPL: 5.6119, Val L_recon: 0.0004, Val L_style: 0.1570\n",
      "Step 525: Train L_total: 2.4284, Train L_lm 1.8784, Train L_recon 0.0002, Train L_style 0.1317\n",
      "Step 525: Val PPL: 5.5398, Val L_recon: 0.0002, Val L_style: 0.1573\n",
      "Step 550: Train L_total: 2.2107, Train L_lm 1.6607, Train L_recon 0.0003, Train L_style 0.2039\n",
      "Step 550: Val PPL: 5.4727, Val L_recon: 0.0002, Val L_style: 0.1568\n",
      "Step 575: Train L_total: 2.5008, Train L_lm 1.9508, Train L_recon 0.0004, Train L_style 0.1635\n",
      "Step 575: Val PPL: 5.4078, Val L_recon: 0.0002, Val L_style: 0.1567\n",
      "Step 600: Train L_total: 2.2278, Train L_lm 1.6778, Train L_recon 0.0002, Train L_style 0.1178\n",
      "Step 600: Val PPL: 5.3482, Val L_recon: 0.0002, Val L_style: 0.1566\n",
      "Step 625: Train L_total: 1.9980, Train L_lm 1.4480, Train L_recon 0.0003, Train L_style 0.1230\n",
      "Step 625: Val PPL: 5.2912, Val L_recon: 0.0002, Val L_style: 0.1564\n",
      "Step 650: Train L_total: 2.1422, Train L_lm 1.5922, Train L_recon 0.0002, Train L_style 0.1886\n",
      "Step 650: Val PPL: 5.2410, Val L_recon: 0.0003, Val L_style: 0.1565\n",
      "Step 675: Train L_total: 2.6265, Train L_lm 2.0765, Train L_recon 0.0003, Train L_style 0.1187\n",
      "Step 675: Val PPL: 5.1966, Val L_recon: 0.0002, Val L_style: 0.1563\n",
      "Step 700: Train L_total: 2.5409, Train L_lm 1.9909, Train L_recon 0.0002, Train L_style 0.1378\n",
      "Step 700: Val PPL: 5.1547, Val L_recon: 0.0004, Val L_style: 0.1563\n",
      "Step 725: Train L_total: 2.0831, Train L_lm 1.5332, Train L_recon 0.0002, Train L_style 0.1674\n",
      "Step 725: Val PPL: 5.1147, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 750: Train L_total: 2.3195, Train L_lm 1.7695, Train L_recon 0.0002, Train L_style 0.1612\n",
      "Step 750: Val PPL: 5.0836, Val L_recon: 0.0002, Val L_style: 0.1562\n",
      "Step 775: Train L_total: 2.2880, Train L_lm 1.7380, Train L_recon 0.0002, Train L_style 0.1643\n",
      "Step 775: Val PPL: 5.0440, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 800: Train L_total: 2.2787, Train L_lm 1.7287, Train L_recon 0.0001, Train L_style 0.1583\n",
      "Step 800: Val PPL: 5.0050, Val L_recon: 0.0003, Val L_style: 0.1562\n",
      "Step 825: Train L_total: 2.1893, Train L_lm 1.6393, Train L_recon 0.0001, Train L_style 0.1517\n",
      "Step 825: Val PPL: 4.9761, Val L_recon: 0.0003, Val L_style: 0.1563\n",
      "Step 850: Train L_total: 2.0960, Train L_lm 1.5460, Train L_recon 0.0005, Train L_style 0.1727\n",
      "Step 850: Val PPL: 4.9484, Val L_recon: 0.0004, Val L_style: 0.1563\n",
      "Step 875: Train L_total: 1.9064, Train L_lm 1.3564, Train L_recon 0.0002, Train L_style 0.1656\n",
      "Step 875: Val PPL: 4.9219, Val L_recon: 0.0002, Val L_style: 0.1563\n",
      "Step 900: Train L_total: 2.3937, Train L_lm 1.8437, Train L_recon 0.0001, Train L_style 0.1766\n",
      "Step 900: Val PPL: 4.9010, Val L_recon: 0.0002, Val L_style: 0.1562\n",
      "Step 925: Train L_total: 2.2655, Train L_lm 1.7155, Train L_recon 0.0002, Train L_style 0.1567\n",
      "Step 925: Val PPL: 4.8732, Val L_recon: 0.0001, Val L_style: 0.1560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56398ed61b8e406a90cc13b4ab588b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 950: Train L_total: 2.2939, Train L_lm 1.7439, Train L_recon 0.0002, Train L_style 0.1442\n",
      "Step 950: Val PPL: 4.8566, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 975: Train L_total: 2.2725, Train L_lm 1.7225, Train L_recon 0.0002, Train L_style 0.1403\n",
      "Step 975: Val PPL: 4.8272, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1000: Train L_total: 2.0315, Train L_lm 1.4815, Train L_recon 0.0002, Train L_style 0.2136\n",
      "Step 1000: Val PPL: 4.8079, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1025: Train L_total: 1.8165, Train L_lm 1.2665, Train L_recon 0.0001, Train L_style 0.1356\n",
      "Step 1025: Val PPL: 4.7955, Val L_recon: 0.0001, Val L_style: 0.1562\n",
      "Step 1050: Train L_total: 2.1094, Train L_lm 1.5594, Train L_recon 0.0001, Train L_style 0.1913\n",
      "Step 1050: Val PPL: 4.7701, Val L_recon: 0.0001, Val L_style: 0.1561\n",
      "Step 1075: Train L_total: 1.8366, Train L_lm 1.2866, Train L_recon 0.0001, Train L_style 0.1666\n",
      "Step 1075: Val PPL: 4.7566, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1100: Train L_total: 1.8964, Train L_lm 1.3464, Train L_recon 0.0001, Train L_style 0.1817\n",
      "Step 1100: Val PPL: 4.7458, Val L_recon: 0.0001, Val L_style: 0.1560\n",
      "Step 1125: Train L_total: 2.2565, Train L_lm 1.7065, Train L_recon 0.0002, Train L_style 0.1414\n",
      "Step 1125: Val PPL: 4.7252, Val L_recon: 0.0001, Val L_style: 0.1559\n",
      "Step 1150: Train L_total: 1.9893, Train L_lm 1.4393, Train L_recon 0.0002, Train L_style 0.2149\n",
      "Step 1150: Val PPL: 4.7147, Val L_recon: 0.0001, Val L_style: 0.1560\n",
      "Step 1175: Train L_total: 2.0501, Train L_lm 1.5001, Train L_recon 0.0001, Train L_style 0.1388\n",
      "Step 1175: Val PPL: 4.7025, Val L_recon: 0.0001, Val L_style: 0.1558\n",
      "Step 1200: Train L_total: 2.1381, Train L_lm 1.5881, Train L_recon 0.0002, Train L_style 0.1477\n",
      "Step 1200: Val PPL: 4.6869, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 1225: Train L_total: 2.2132, Train L_lm 1.6632, Train L_recon 0.0001, Train L_style 0.1294\n",
      "Step 1225: Val PPL: 4.6798, Val L_recon: 0.0002, Val L_style: 0.1558\n",
      "Step 1250: Train L_total: 2.1067, Train L_lm 1.5567, Train L_recon 0.0002, Train L_style 0.1399\n",
      "Step 1250: Val PPL: 4.6661, Val L_recon: 0.0001, Val L_style: 0.1558\n",
      "Step 1275: Train L_total: 1.8951, Train L_lm 1.3451, Train L_recon 0.0002, Train L_style 0.1293\n",
      "Step 1275: Val PPL: 4.6585, Val L_recon: 0.0001, Val L_style: 0.1558\n",
      "Step 1300: Train L_total: 2.0603, Train L_lm 1.5103, Train L_recon 0.0002, Train L_style 0.1197\n",
      "Step 1300: Val PPL: 4.6537, Val L_recon: 0.0002, Val L_style: 0.1557\n",
      "Step 1325: Train L_total: 2.0400, Train L_lm 1.4900, Train L_recon 0.0002, Train L_style 0.1716\n",
      "Step 1325: Val PPL: 4.6447, Val L_recon: 0.0001, Val L_style: 0.1558\n",
      "Step 1350: Train L_total: 1.9753, Train L_lm 1.4253, Train L_recon 0.0001, Train L_style 0.1367\n",
      "Step 1350: Val PPL: 4.6379, Val L_recon: 0.0003, Val L_style: 0.1561\n",
      "Step 1375: Train L_total: 1.9294, Train L_lm 1.3794, Train L_recon 0.0002, Train L_style 0.1861\n",
      "Step 1375: Val PPL: 4.6303, Val L_recon: 0.0002, Val L_style: 0.1557\n",
      "Step 1400: Train L_total: 2.2551, Train L_lm 1.7051, Train L_recon 0.0002, Train L_style 0.1401\n",
      "Step 1400: Val PPL: 4.6242, Val L_recon: 0.0001, Val L_style: 0.1556\n",
      "Step 1425: Train L_total: 2.0408, Train L_lm 1.4908, Train L_recon 0.0003, Train L_style 0.1655\n",
      "Step 1425: Val PPL: 4.6192, Val L_recon: 0.0002, Val L_style: 0.1557\n",
      "Step 1450: Train L_total: 2.2014, Train L_lm 1.6514, Train L_recon 0.0001, Train L_style 0.1934\n",
      "Step 1450: Val PPL: 4.6154, Val L_recon: 0.0003, Val L_style: 0.1559\n",
      "Step 1475: Train L_total: 1.7666, Train L_lm 1.2166, Train L_recon 0.0002, Train L_style 0.1563\n",
      "Step 1475: Val PPL: 4.6116, Val L_recon: 0.0002, Val L_style: 0.1558\n",
      "Step 1500: Train L_total: 2.0187, Train L_lm 1.4687, Train L_recon 0.0001, Train L_style 0.1550\n",
      "Step 1500: Val PPL: 4.6077, Val L_recon: 0.0002, Val L_style: 0.1557\n",
      "Step 1525: Train L_total: 2.0632, Train L_lm 1.5132, Train L_recon 0.0002, Train L_style 0.1280\n",
      "Step 1525: Val PPL: 4.6035, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1550: Train L_total: 2.3669, Train L_lm 1.8169, Train L_recon 0.0001, Train L_style 0.1333\n",
      "Step 1550: Val PPL: 4.6001, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 1575: Train L_total: 1.8520, Train L_lm 1.3020, Train L_recon 0.0003, Train L_style 0.1771\n",
      "Step 1575: Val PPL: 4.5967, Val L_recon: 0.0002, Val L_style: 0.1562\n",
      "Step 1600: Train L_total: 2.2175, Train L_lm 1.6675, Train L_recon 0.0003, Train L_style 0.1637\n",
      "Step 1600: Val PPL: 4.5947, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 1625: Train L_total: 2.0848, Train L_lm 1.5348, Train L_recon 0.0002, Train L_style 0.1475\n",
      "Step 1625: Val PPL: 4.5954, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1650: Train L_total: 2.2510, Train L_lm 1.7010, Train L_recon 0.0002, Train L_style 0.1301\n",
      "Step 1650: Val PPL: 4.5928, Val L_recon: 0.0002, Val L_style: 0.1560\n",
      "Step 1675: Train L_total: 1.9059, Train L_lm 1.3560, Train L_recon 0.0001, Train L_style 0.1267\n",
      "Step 1675: Val PPL: 4.5920, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1700: Train L_total: 2.1861, Train L_lm 1.6361, Train L_recon 0.0002, Train L_style 0.1647\n",
      "Step 1700: Val PPL: 4.5910, Val L_recon: 0.0001, Val L_style: 0.1557\n",
      "Step 1725: Train L_total: 1.9475, Train L_lm 1.3975, Train L_recon 0.0003, Train L_style 0.2063\n",
      "Step 1725: Val PPL: 4.5902, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1750: Train L_total: 1.8489, Train L_lm 1.2989, Train L_recon 0.0002, Train L_style 0.1457\n",
      "Step 1750: Val PPL: 4.5893, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1775: Train L_total: 2.0893, Train L_lm 1.5393, Train L_recon 0.0001, Train L_style 0.1377\n",
      "Step 1775: Val PPL: 4.5894, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1800: Train L_total: 2.0631, Train L_lm 1.5131, Train L_recon 0.0001, Train L_style 0.1507\n",
      "Step 1800: Val PPL: 4.5892, Val L_recon: 0.0001, Val L_style: 0.1560\n",
      "Step 1825: Train L_total: 1.7718, Train L_lm 1.2218, Train L_recon 0.0002, Train L_style 0.1528\n",
      "Step 1825: Val PPL: 4.5889, Val L_recon: 0.0002, Val L_style: 0.1561\n",
      "Step 1850: Train L_total: 2.2335, Train L_lm 1.6835, Train L_recon 0.0002, Train L_style 0.1549\n",
      "Step 1850: Val PPL: 4.5900, Val L_recon: 0.0003, Val L_style: 0.1561\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import math\n",
    "\n",
    "VAL_INTERVAL = 25\n",
    "\n",
    "styled_model.train()\n",
    "style_predictor_head.train()\n",
    "\n",
    "global_step = 0\n",
    "epsilon = 1e-8 \n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "    for batch in progress_bar:\n",
    "        # 将数据移动到 GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        syntactic_vec = batch[\"syntactic_vec\"].to(device)\n",
    "        pragmatic_tags = batch[\"prag_vec\"].to(device)\n",
    "\n",
    "        # 1. 模型前向传播\n",
    "        outputs = styled_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            syntactic_vec=syntactic_vec\n",
    "        )\n",
    "        \n",
    "        # 2. 获取 L_lm (已由模型计算)\n",
    "        L_lm = outputs[\"loss\"]\n",
    "        \n",
    "        # --- 3. 计算辅助损失 ---\n",
    "        \n",
    "        # L_recon: 重建 syntactic_vec\n",
    "        # 我们使用 last_hidden_state 的第一个 token (即 style prefix token) 来重建\n",
    "        style_prefix_hidden_state = outputs[\"last_hidden_state\"][:, 0] # [batch_size, hidden_size]\n",
    "        \n",
    "        # 通过辅助头进行预测\n",
    "        aux_preds = style_predictor_head(style_prefix_hidden_state.float())\n",
    "        pred_syntactic = aux_preds[:, :syntactic_dim_length]\n",
    "        pred_pragmatic = aux_preds[:, syntactic_dim_length:]\n",
    "        \n",
    "        L_recon = mse_loss_fn(pred_syntactic, syntactic_vec)\n",
    "        \n",
    "        # L_style_cls: 预测 pragmatic_tags\n",
    "        L_style_cls = bce_loss_fn(pred_pragmatic, pragmatic_tags.float())\n",
    "        \n",
    "        # 4. 计算总损失\n",
    "        L_total = L_lm + lambda_recon * (L_recon / (L_recon.detach() + epsilon)) \\\n",
    "               + lambda_style * (L_style_cls / (L_style_cls.detach() + epsilon))\n",
    "        \n",
    "        # 5. 反向传播\n",
    "        L_total.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "                    \"L_total\": L_total.item(),\n",
    "                    \"L_lm\": L_lm.item(),\n",
    "                    \"L_recon\": L_recon.item(),\n",
    "                    \"L_style\": L_style_cls.item(),\n",
    "                    \"LR\": lr_scheduler.get_last_lr()[0]\n",
    "                })\n",
    "        \n",
    "        if global_step % VAL_INTERVAL == 0:\n",
    "            styled_model.eval()\n",
    "            style_predictor_head.eval()\n",
    "\n",
    "            val_loss_lm = 0\n",
    "            val_loss_recon = 0\n",
    "            val_loss_style = 0\n",
    "            val_steps = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    input_ids = val_batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "                    labels = val_batch[\"labels\"].to(device)\n",
    "                    syntactic_vec = val_batch[\"syntactic_vec\"].to(device)\n",
    "                    pragmatic_tags = val_batch[\"prag_vec\"].to(device) # 修正了key\n",
    "\n",
    "                    outputs = styled_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                        syntactic_vec=syntactic_vec\n",
    "                    )     \n",
    "\n",
    "                    L_lm_val = outputs[\"loss\"]\n",
    "                    style_prefix_hidden_state = outputs[\"last_hidden_state\"][:, 0]\n",
    "                    aux_preds = style_predictor_head(style_prefix_hidden_state.float())\n",
    "                    pred_syntactic = aux_preds[:, :syntactic_dim_length]\n",
    "                    pred_pragmatic = aux_preds[:, syntactic_dim_length:]\n",
    "\n",
    "                    L_recon_val = mse_loss_fn(pred_syntactic, syntactic_vec)\n",
    "                    L_style_cls_val = bce_loss_fn(pred_pragmatic, pragmatic_tags.float())\n",
    "\n",
    "                    val_loss_lm += L_lm_val.item()\n",
    "                    val_loss_recon += L_recon_val.item()\n",
    "                    val_loss_style += L_style_cls_val.item()\n",
    "                    val_steps += 1\n",
    "\n",
    "            # 计算平均验证损失和 PPL\n",
    "            avg_val_loss = val_loss_lm / val_steps\n",
    "            ppl = math.exp(min(avg_val_loss, 20))\n",
    "\n",
    "            print(f\"Step {global_step}: Train L_total: {L_total.item():.4f}, Train L_lm {L_lm.item():.4f}, Train L_recon {L_recon.item():.4f}, Train L_style {L_style_cls.item():.4f}\")\n",
    "            print(f\"Step {global_step}: Val PPL: {ppl:.4f}, Val L_recon: {val_loss_recon / val_steps:.4f}, Val L_style: {val_loss_style / val_steps:.4f}\")\n",
    "\n",
    "            # 切换回训练模式\n",
    "            styled_model.train()\n",
    "            style_predictor_head.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b3ba5",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c498d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /root/OtakuLab/outputs/styled-qwen-balanced...\n",
      "✅ Model checkpoint saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# SAVE_DIR = Path(\"/root/OtakuLab/outputs/styled-qwen\")\n",
    "SAVE_DIR = Path(\"/root/OtakuLab/outputs/styled-qwen-balanced\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {SAVE_DIR}...\")\n",
    "\n",
    "# 1️⃣ 保存 LoRA 权重（仅包含可训练部分）\n",
    "styled_model.peft_model.save_pretrained(SAVE_DIR / \"lora\")  # type:ignore\n",
    "\n",
    "# 2️⃣ 保存 StyleEncoder 权重\n",
    "torch.save(styled_model.style_encoder.state_dict(), SAVE_DIR / \"style_encoder.pt\")\n",
    "\n",
    "# 3️⃣ 保存 StylePredictorHead 权重\n",
    "torch.save(style_predictor_head.state_dict(), SAVE_DIR / \"style_predictor_head.pt\")\n",
    "\n",
    "# 4️⃣ 保存 Tokenizer 配置\n",
    "tokenizer.save_pretrained(SAVE_DIR / \"tokenizer\")\n",
    "\n",
    "print(\"✅ Model checkpoint saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fcafd",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Before running this cell, ensure that all code in the `Define Model Structure & Data Loader` cell has been executed and model training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96899c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9e92e53bf940978a3553b6a740a525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Styled model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# SAVE_DIR = Path(\"/root/OtakuLab/outputs/styled-qwen\")\n",
    "SAVE_DIR = Path(\"/root/OtakuLab/outputs/styled-qwen-balanced\")\n",
    "MODEL_PATH = Path(\"/root/autodl-tmp/Qwen3-1.7B/\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "syntactic_dims = ['syntactic_compression', 'declarativity', 'clausal_embedding', 'nominal_complexity', 'subordination', 'interjectionality',\n",
    "                  'ellipsis_or_fragmentation', 'modifier_density', 'prepositional_density', 'topic_fronting', 'referentiality', 'parallelism',\n",
    "                  'quantificationality', 'coordination_density']\n",
    "prag_style_vocab = ['kind', 'modest', 'clingy', 'playful', 'cold', 'proud', 'sharp_tongued', 'subservient', 'submissive', 'controlling',\n",
    "                    'strong', 'defensive', 'tsukkomi', 'rational', 'curious', 'imaginative', 'cautious', 'idealistic', 'conservative',\n",
    "                    'radical', 'obsessive', 'hesitant', 'energetic', 'optimistic', 'confident', 'passionate', 'melancholy', 'serious',\n",
    "                    'emotional', 'sensitive', 'shy', 'irritable', 'anxious', 'lazy', 'tsundere', 'yandere', 'chuunibyou', 'cute', 'naive',\n",
    "                    'airhead', 'elegant', 'humorous', 'loyal', 'responsible', 'willful', 'antisocial', 'talkative', 'masochistic', 'sadistic', 'evil']\n",
    "\n",
    "syntactic_dim_length = len(syntactic_dims)\n",
    "\n",
    "# === 加载 Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR / \"tokenizer\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === 加载基础模型并注入 LoRA ===\n",
    "base_model = Qwen3ForCausalLM.from_pretrained(MODEL_PATH, dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, SAVE_DIR / \"lora\")\n",
    "hidden_size = base_model.config.hidden_size\n",
    "\n",
    "# === 初始化风格编码器 ===\n",
    "style_encoder = StyleEncoder(syntactic_dim_length, hidden_size)\n",
    "style_encoder.load_state_dict(torch.load(SAVE_DIR / \"style_encoder.pt\", map_location=DEVICE))\n",
    "style_encoder.to(DEVICE)\n",
    "style_encoder.eval()\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✅ Styled model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7daced",
   "metadata": {},
   "source": [
    "## Call Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf89ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_styled_response(neutral_sentence: str, syntactic_vec: dict[str, float],\n",
    "                             character_name: str = \"Ayaka\", \n",
    "                             lexical_keywords: list[str] = list(),\n",
    "                             pragmatic_styles: list[str] = list(),\n",
    "                             temperature: float = 0.5,\n",
    "                             top_p: float = 0.9,\n",
    "                             repetition_penalty: float = 1.3,\n",
    "                             max_new_tokens: int = 100):\n",
    "    \"\"\"输入中性句和风格向量，生成风格化响应\"\"\"\n",
    "\n",
    "    lexical_keywords = lexical_keywords or []\n",
    "    pragmatic_styles = pragmatic_styles or []\n",
    "\n",
    "    # === 1. 构建提示 ===\n",
    "    keywords = \", \".join(lexical_keywords) if lexical_keywords else \"None\"\n",
    "    pragmatics = \", \".join(pragmatic_styles) if pragmatic_styles else \"None\"\n",
    "\n",
    "    system_prompt = \"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "    user_prompt = (\n",
    "        f\"Target Character {character_name}\\n\"\n",
    "        f\"Personality: {pragmatic_styles}\\n\"\n",
    "        f\"Keywords: {keywords}\\n\"\n",
    "        f\"Neutral Content: {neutral_sentence}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True,\n",
    "                                               enable_thinking=True)\n",
    "\n",
    "    # === 2. Tokenize 输入 ===\n",
    "    tokenized = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # === 3. 风格向量 ===\n",
    "    syntactic_tensor = torch.tensor(\n",
    "        [syntactic_vec.get(dim, 0.0) for dim in syntactic_dims],\n",
    "        dtype=torch.float32, device=DEVICE\n",
    "    ).unsqueeze(0)  # [1, syntactic_dim_length]\n",
    "\n",
    "    # === 4. 生成风格 embedding ===\n",
    "    style_emb = style_encoder(syntactic_tensor).to(model.dtype)  # [1, hidden_size]\n",
    "    style_prefix = style_emb.unsqueeze(1)        # [1, 1, hidden_size]\n",
    "\n",
    "    # === 5. 获取原始词嵌入并拼接 ===\n",
    "    token_embeds = model.get_input_embeddings()(input_ids)  # type:ignore\n",
    "    inputs_embeds = torch.cat([style_prefix, token_embeds], dim=1)\n",
    "\n",
    "    # === 6. Attention mask 修正 ===\n",
    "    prefix_mask = torch.ones((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    new_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    # === 7. 生成 ===\n",
    "    outputs = model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=new_attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # === 8. 解码输出 ===\n",
    "    # prompt_length = input_ids.shape[1]\n",
    "    # new_tokens = outputs[0, prompt_length:]\n",
    "    # result = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4dff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "身高和体重是个人隐私，不能随便告诉你。 -> <think>\n",
      "她用“才”和语气词~软化拒绝，自称沐雪人设可爱又带AI感。把严肃隐私话题变成撒娇式提醒，符合萌系少女的表达习惯。\n",
      "</think>\n",
      "\n",
      "身高体重是个人私密信息！你可别随便告诉别人哦\n",
      "\n",
      "我不知道这是否是个好主意。 -> <think>\n",
      "她理性而谨慎，用“可能”保留开放性。她的语言克制且优雅，“考虑是否是好主意”的措辞体现职责与使命的分寸感。\n",
      "</think>\n",
      "\n",
      "我不知道这可能是个好的决定\n",
      "\n",
      "谢谢你帮我。 -> <think>\n",
      "她用“才没有”掩饰害羞，语气强硬却暗藏软意。句尾加感叹号强化情绪，“笨蛋”和“讨厌”的反语词体现防御性与傲娇。\n",
      "</think>\n",
      "\n",
      "谢了！我才没那么好意思啊\n"
     ]
    }
   ],
   "source": [
    "# --- Input for Training Example ---\n",
    "neutral_sentence = \"身高和体重是个人隐私，不能随便告诉你。\"\n",
    "styled_sentence = generate_styled_response(\n",
    "    character_name=\"Muice\",\n",
    "    neutral_sentence=neutral_sentence,\n",
    "    syntactic_vec={'declarativity': 0.1103257643217572, 'parallelism': 0.02918150786583556, 'ellipsis_or_fragmentation': 0.08529979222321163,\n",
    "                      'subordination': 0.19688705847432472, 'interjectionality': 0.008644998515880083, 'clausal_embedding': 0.034784060552092606,\n",
    "                      'referentiality': 0.11624369249035323, 'syntactic_compression': 0.18596022558622738, 'nominal_complexity': 0.03342980112793114,\n",
    "                      'coordination_density': 0.002615761353517364, 'quantificationality': 0.038197536360937964, 'modifier_density': 0.1393217571979816,\n",
    "                      'prepositional_density': 0.01910804392994954},\n",
    "    pragmatic_styles=[\"cute\"],\n",
    "    lexical_keywords=[\"喵\", \"沐沐\", \"AI\", \"恼\", \"沐雪\", \"女孩子\", \"~\", \"⭐\", \"不行\", \"聊天\", \"呀\", \"可爱\", \"才\", \"叫\", \"唔\", \"谁\", \"不会\", \"吃\", \"睡觉\", \"笨蛋\", \"答\", \"谢谢\", \"把\", \"即\", \"吧，\"]\n",
    ")\n",
    "print(f\"{neutral_sentence} -> {styled_sentence}\")  # Expect: 身高体重是女孩子间的秘密，怎么能轻易告诉你这种杂鱼喵！\n",
    "print()\n",
    "\n",
    "# --- Input for Example 1 ---\n",
    "neutral_sentence = \"我不知道这是否是个好主意。\"\n",
    "styled_response_2 = generate_styled_response(\n",
    "    character_name=\"Lady Elara\",\n",
    "    neutral_sentence=neutral_sentence,\n",
    "    syntactic_vec={\n",
    "        'declarativity': 0.3, 'prepositional_density': 0.3, 'interjectionality': 0.1,\n",
    "        'parallelism': 0.3, 'nominal_coordination': 0.3, 'referentiality': 0.3,\n",
    "        'modifier_density': 0.7, 'subordination': 0.8\n",
    "    },\n",
    "    pragmatic_styles=['serious', 'rational', 'cautious', 'elegant'],\n",
    "    lexical_keywords=[\"考虑\", \"荣幸\", \"职责\", \"使命\", \"可能\", \"坚定\", \"承认\", \"未来\"]\n",
    ")\n",
    "print(f\"{neutral_sentence} -> {styled_response_2}\")\n",
    "print()\n",
    "\n",
    "# --- Input for Example 2 ---\n",
    "neutral_sentence = \"谢谢你帮我。\"\n",
    "styled_response_3 = generate_styled_response(\n",
    "    character_name=\"Katsumi\",\n",
    "    neutral_sentence=neutral_sentence,\n",
    "    syntactic_vec={\n",
    "        'declarativity': 0.8, 'prepositional_density': 0.4, 'interjectionality': 0.6,\n",
    "        'parallelism': 0.4, 'nominal_coordination': 0.4, 'referentiality': 0.4,\n",
    "        'modifier_density': 0.4, 'subordination': 0.4\n",
    "    },\n",
    "    pragmatic_styles=['tsundere', 'defensive'],\n",
    "    lexical_keywords=[\"哼\", \"才没有\", \"随便\", \"讨厌\", \"帮忙\", \"笨蛋\", \"不需要\", \"自己\"]\n",
    ")\n",
    "print(f\"{neutral_sentence} -> {styled_response_3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}