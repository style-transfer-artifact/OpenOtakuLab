{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de81672",
   "metadata": {},
   "source": [
    "# Construct High PMI Keywords for Three Different Style Spaces\n",
    "\n",
    "## Load Style Space (Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e55f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muice Responses: 2721\n",
      "Ayaka Responses: 991\n",
      "Zhongli Responses: 359\n",
      "Hutao Responses: 591\n",
      "Haruhi Responses: 770\n",
      "PsyDTCorpus Responses: 89652\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "MUICE_PATH = Path(\"../Dataset/Muice/train.jsonl\")\n",
    "HARUHI_PATH = Path(r\"..\\Dataset\\Haruhi\\Haruhi_clean.jsonl\")\n",
    "PSYDC_PATH1 = Path(r\"..\\Dataset\\PsyDTCorpus\\PsyDTCorpus_train_mulit_turn_packing.json\")\n",
    "PSYDC_PATH2 = Path(r\"..\\Dataset\\PsyDTCorpus\\PsyDTCorpus_test_single_turn_split.json\")\n",
    "\n",
    "# Check if Dataset exist\n",
    "assert MUICE_PATH.is_file(), \"请确保 Muice Dataset 的路径正确！\"\n",
    "assert HARUHI_PATH.is_file(), \"请确保 Haruhi Dataset 的路径正确！\"\n",
    "assert PSYDC_PATH1.is_file(), \"请确保 PsyDTCorpus Dataset 的路径正确！\"\n",
    "\n",
    "# Load Muice Dataset\n",
    "dataset_lines = MUICE_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "muice_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "    muice_responses.append(item[\"Response\"])\n",
    "\n",
    "# 出于分析效率考虑，取 80% 进行分析\n",
    "muice_responses = muice_responses[:int(len(muice_responses)*0.8)]\n",
    "\n",
    "# Load Haruhi Dataset\n",
    "dataset_lines = HARUHI_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "ayaka_responses: List[str] = []\n",
    "zhongli_responses: List[str] = []\n",
    "hutao_responses: List[str] = []\n",
    "haruhi_responses: List[str] = []\n",
    "\n",
    "for line in dataset_lines:\n",
    "    item = json.loads(line)\n",
    "\n",
    "    # 只提取单一角色的训练集\n",
    "    if item[\"agent_role\"] == \"神里绫华\":\n",
    "        ayaka_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"钟离\":\n",
    "        zhongli_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role\"] == \"胡桃\":\n",
    "        hutao_responses.append(item[\"agent_response\"])\n",
    "    elif item[\"agent_role_name_en\"] == \"haruhi\":\n",
    "        haruhi_responses.append(item[\"agent_response\"])\n",
    "\n",
    "# 取 70% 进行分析\n",
    "ayaka_responses = ayaka_responses[:int(len(ayaka_responses)*0.7)]\n",
    "zhongli_responses = zhongli_responses[:int(len(zhongli_responses)*0.7)]\n",
    "hutao_responses = hutao_responses[:int(len(hutao_responses)*0.7)]\n",
    "haruhi_responses = haruhi_responses[:int(len(haruhi_responses)*0.7)]\n",
    "\n",
    "# Load PsyDTCorpus Dataset\n",
    "psydc_part1 = json.loads(PSYDC_PATH1.read_text(encoding=\"utf-8\"))\n",
    "psydc_part2 = json.loads(PSYDC_PATH2.read_text(encoding=\"utf-8\"))\n",
    "psydc: list[dict] = psydc_part1 + psydc_part2\n",
    "psydc_responses: List[str] = []\n",
    "\n",
    "for item in psydc:\n",
    "    messages: list[dict[str, str]] = item[\"messages\"]\n",
    "    for index in range(2, len(messages), 2):\n",
    "        message = messages[index]\n",
    "        assert message[\"role\"] == \"assistant\", message\n",
    "        psydc_responses.append(message[\"content\"])\n",
    "\n",
    "# 取 70% 进行分析\n",
    "psydc_responses = psydc_responses[:int(len(psydc_responses)*0.7)]\n",
    "\n",
    "# 输出所有训练集的长度\n",
    "\n",
    "print(f\"Muice Responses: {len(muice_responses)}\")\n",
    "print(f\"Ayaka Responses: {len(ayaka_responses)}\")\n",
    "print(f\"Zhongli Responses: {len(zhongli_responses)}\")\n",
    "print(f\"Hutao Responses: {len(hutao_responses)}\")\n",
    "print(f\"Haruhi Responses: {len(haruhi_responses)}\")\n",
    "print(f\"PsyDTCorpus Responses: {len(psydc_responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb61f62",
   "metadata": {},
   "source": [
    "## Call HanLP for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f851a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanlp_restful import HanLPClient\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import re\n",
    "\n",
    "load_dotenv()  # 从 .env 文件加载环境变量\n",
    "\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=getenv(\"HANLP_API_KEY\", None), language='zh')  # type: ignore\n",
    "\n",
    "def limit_sentense_length(text: str, max_length: int = 150) -> list[str]:\n",
    "    if len(text) < 150:\n",
    "        return [text]\n",
    "    \n",
    "    # print(\"Warning: 句子超出了长度上限: \", text)\n",
    "\n",
    "    # 按照标点符号进行分割\n",
    "    sentences = re.split(r'([。！？；，,.!?;])', text)\n",
    "    chunks:list[str] = []\n",
    "    temp = ''\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        chunk = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "        if len(temp) + len(chunk) < max_length:\n",
    "            temp += chunk\n",
    "        else:\n",
    "            chunks.append(temp)\n",
    "            temp = chunk\n",
    "    if temp:\n",
    "        chunks.append(temp)\n",
    "    return chunks\n",
    "\n",
    "def split_texts(texts: list[str]) -> list[str]:\n",
    "    \"\"\"将文本列表中的每个文本分割成更小的句子\"\"\"\n",
    "    all_sentences = []\n",
    "    for text in texts:\n",
    "        sentences = limit_sentense_length(text)\n",
    "        all_sentences.extend(sentences)\n",
    "    return all_sentences\n",
    "\n",
    "def tokenize_safe(texts: list[str], max_batch_num: int = 250, max_chars_per_batch: int = 15000, interval: int = 35):\n",
    "    \"\"\"对文本进行分词，同时限制每一批总字符数\"\"\"\n",
    "    all_tokens = []\n",
    "    current_batch = []\n",
    "    current_length = 0\n",
    "    batch_id = 1\n",
    "\n",
    "    for text in texts:\n",
    "        text_len = len(text)\n",
    "        # 如果加上这个句子会超出限制，则先处理已有批次\n",
    "        if current_length + text_len > max_chars_per_batch or len(current_batch) + 1 > max_batch_num:\n",
    "            print(f\"Processing batch {batch_id} (Total chars: {current_length})...\")\n",
    "            tokens = HanLP.tokenize(current_batch)\n",
    "            all_tokens.extend(tokens)\n",
    "            sleep(interval)\n",
    "            batch_id += 1\n",
    "            # 重置 batch\n",
    "            current_batch = [text]\n",
    "            current_length = text_len\n",
    "        else:\n",
    "            current_batch.append(text)\n",
    "            current_length += text_len\n",
    "\n",
    "    # 最后一批也别忘记\n",
    "    if current_batch:\n",
    "        print(f\"Processing batch {batch_id} (Total chars: {current_length})...\")\n",
    "        tokens = HanLP.tokenize(current_batch)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f467d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (Total chars: 9118)...\n",
      "Processing batch 2 (Total chars: 8006)...\n",
      "Processing batch 3 (Total chars: 13423)...\n",
      "Processing batch 4 (Total chars: 12001)...\n"
     ]
    }
   ],
   "source": [
    "muice_responses_split = split_texts(muice_responses)\n",
    "ayaka_responses_split = split_texts(ayaka_responses)\n",
    "zhongli_responses_split = split_texts(zhongli_responses)\n",
    "hutao_responses_split = split_texts(hutao_responses)\n",
    "haruhi_responses_split = split_texts(haruhi_responses)\n",
    "psydc_responses_split = split_texts(psydc_responses)\n",
    "\n",
    "muice_tokens = tokenize_safe(muice_responses_split, 240, interval=1)\n",
    "ayaka_tokens = tokenize_safe(ayaka_responses_split, 200, interval=1)\n",
    "zhongli_tokens = tokenize_safe(zhongli_responses_split, 200, interval=1)\n",
    "hutao_tokens = tokenize_safe(hutao_responses_split, 200, interval=1)\n",
    "haruhi_tokens = tokenize_safe(haruhi_responses_split, 200, interval=1)\n",
    "psydc_tokens = tokenize_safe(psydc_responses_split, 200, interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316aa46",
   "metadata": {},
   "source": [
    "## Flatten and Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haruhi Tokens: 27402\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "def save_tokens(tokens: list, character: str):\n",
    "    tokens = list(chain.from_iterable(tokens))\n",
    "    print(f\"{character} Tokens: {len(tokens)}\")\n",
    "\n",
    "    with open(f\"outputs/tokens/{character}_tokens.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokens, f)\n",
    "\n",
    "save_tokens(muice_tokens, \"muice\")\n",
    "save_tokens(ayaka_tokens, \"ayaka\")\n",
    "save_tokens(zhongli_tokens, \"zhongli\")\n",
    "save_tokens(hutao_tokens, \"hutao\")\n",
    "save_tokens(haruhi_tokens, \"haruhi\")\n",
    "save_tokens(psydc_tokens, \"psydc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071a356",
   "metadata": {},
   "source": [
    "## Downsampling Processing\n",
    "\n",
    "Since the ratio of differences between style corpora is about 1:1:64, applying it directly to PMI calculation will cause serious errors.\n",
    "\n",
    "Therefore, we will use a method called downsampling to optimize the ratio.\n",
    "\n",
    "In the following code block, we will use downsampling to reduce the ratio to 1:1:4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109743aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downsampling:\n",
      "Muice Tokens: 53829\n",
      "Ayaka Tokens: 51134\n",
      "Zhongli Tokens: 18199\n",
      "Hutao Tokens: 32831\n",
      "Haruhi Tokens: 27402\n",
      "PsyDTCorpus Tokens: 3417372\n",
      "--------------------\n",
      "After downsampling:\n",
      "Muice Tokens: 53829\n",
      "Ayaka Tokens: 51134\n",
      "Zhongli Tokens: 18199\n",
      "Hutao Tokens: 32831\n",
      "Haruhi Tokens: 27402\n",
      "PsyDTCorpus Tokens: 102268\n",
      "Global Tokens: 285663\n"
     ]
    }
   ],
   "source": [
    "# Load tokens from pkl(如果之前已经运行过了就可以注释此段)\n",
    "\n",
    "import pickle\n",
    "muice_tokens = pickle.load(open(\"./outputs/tokens/muice_tokens.pkl\", \"rb\"))\n",
    "ayaka_tokens = pickle.load(open(\"./outputs/tokens/ayaka_tokens.pkl\", \"rb\"))\n",
    "zhongli_tokens = pickle.load(open(\"./outputs/tokens/zhongli_tokens.pkl\", \"rb\"))\n",
    "hutao_tokens = pickle.load(open(\"./outputs/tokens/hutao_tokens.pkl\", \"rb\"))\n",
    "haruhi_tokens = pickle.load(open(\"./outputs/tokens/haruhi_tokens.pkl\", \"rb\"))\n",
    "psydc_tokens = pickle.load(open(\"./outputs/tokens/psydc_tokens.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Before downsampling:\")\n",
    "print(f\"Muice Tokens: {len(muice_tokens)}\")\n",
    "print(f\"Ayaka Tokens: {len(ayaka_tokens)}\")\n",
    "print(f\"Zhongli Tokens: {len(zhongli_tokens)}\")\n",
    "print(f\"Hutao Tokens: {len(hutao_tokens)}\")\n",
    "print(f\"Haruhi Tokens: {len(haruhi_tokens)}\")\n",
    "print(f\"PsyDTCorpus Tokens: {len(psydc_tokens)}\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Downsample tokens to a target size\n",
    "\n",
    "def downsample(tokens: list[str], target_size: int, seed: int = 42) -> list[str]:\n",
    "    \"\"\"对 tokens 进行下采样，直到达到目标大小\"\"\"\n",
    "    if len(tokens) <= target_size:\n",
    "        return tokens\n",
    "    random.seed(seed)\n",
    "    return random.sample(tokens, target_size)\n",
    "\n",
    "muice_tokens_downsampled = muice_tokens\n",
    "ayaka_tokens_downsampled = ayaka_tokens\n",
    "zhongli_tokens_downsampled = zhongli_tokens\n",
    "hutao_tokens_downsampled = hutao_tokens\n",
    "haruhi_tokens_downsampled = haruhi_tokens\n",
    "psydc_tokens_downsampled = downsample(psydc_tokens, len(ayaka_tokens) * 2)\n",
    "# global_tokens_downsampled = muice_tokens_downsampled + ayaka_tokens_downsampled + psydc_tokens_downsampled\n",
    "global_tokens_downsampled = (muice_tokens_downsampled +\n",
    "                             ayaka_tokens_downsampled + \n",
    "                             zhongli_tokens_downsampled + \n",
    "                             hutao_tokens_downsampled + \n",
    "                             haruhi_tokens_downsampled +\n",
    "                             psydc_tokens_downsampled)\n",
    "\n",
    "print(\"-\"* 20)\n",
    "\n",
    "print(\"After downsampling:\")\n",
    "print(f\"Muice Tokens: {len(muice_tokens_downsampled)}\")\n",
    "print(f\"Ayaka Tokens: {len(ayaka_tokens_downsampled)}\")\n",
    "print(f\"Zhongli Tokens: {len(zhongli_tokens_downsampled)}\")\n",
    "print(f\"Hutao Tokens: {len(hutao_tokens_downsampled)}\")\n",
    "print(f\"Haruhi Tokens: {len(haruhi_tokens_downsampled)}\")\n",
    "print(f\"PsyDTCorpus Tokens: {len(psydc_tokens_downsampled)}\")\n",
    "print(f\"Global Tokens: {len(global_tokens_downsampled)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf0ffc",
   "metadata": {},
   "source": [
    "## Calculate PMI Style Vocabulary for Different Styles\n",
    "\n",
    "To ignore topic-specific words and rare words, we ignore a word when $p(w) > 10\\%$ or $p(w|t) < 0.5\\%$. We calculated the PMI of tokens in three different style domains and selected representative style tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9808f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muice TF-PMI Vocabulary Set Size: 968\n",
      "Muice TF-PMI Vocabulary Sample:\n",
      "['喵', '沐沐', 'AI', '恼', '沐雪', '~', '女孩子', '⭐', '不行', '聊天', '呀', '叫', '唔', '答', '吃', '可爱', '睡觉', '谢谢', '即', '雪雪', '骂', '笨蛋', '不会', '直播', '脸红']\n",
      "--------------------\n",
      "Ayaka TF-PMI Vocabulary Set Size: 894\n",
      "Ayaka TF-PMI Vocabulary Sample:\n",
      "['稻妻国', '神里家', '稻妻', '大小姐', '家族', '传统', '奉行', '文化', '人民', '眼狩令', '神', '当地', '社', '舞蹈', '美丽', '茶道', '神社', '祭典', '眼', '美食', '继承', '剑术', '国家', '将军', '责任']\n",
      "--------------------\n",
      "Zhongli TF-PMI Vocabulary Set Size: 1235\n",
      "Zhongli TF-PMI Vocabulary Sample:\n",
      "['岩石', '岩', '璃月', '力', '璃', '契约', '炼金术', '月', '盐', '帝君', '魔神', '操控', '王', '并非', '岩王', '大地', '封印', '作战', '掌握', '大陆', '学问', '研究', '七星', '客卿', '岩元素']\n",
      "--------------------\n",
      "Hutao TF-PMI Vocabulary Set Size: 890\n",
      "Hutao TF-PMI Vocabulary Sample:\n",
      "['往生堂', '嘿嘿', '嘻嘻', '可是', '堂主', '哎呀呀', '哦哦哦', '宝藏', '惊喜', '诗歌', '可不是', '灵魂', '胡桃', '神秘', '生死', '谜题', '哈哈哈', '不过', '有趣', '亡灵', '秘密', '意想不到', '巫师', '哇', '奇妙']\n",
      "--------------------\n",
      "Haruhi TF-PMI Vocabulary Set Size: 952\n",
      "Haruhi TF-PMI Vocabulary Sample:\n",
      "['团', 'SOS', '阿虚', '社团', '哼', '事件', '学校', '超自然', '朝比奈', '文化祭', '创意', '吸引', '古泉', '电影', '创新', '组织', '实玖瑠', '当然', '与众不同', '主题', '加入', '束缚', '凉宫', '团长', '外星人']\n",
      "--------------------\n",
      "Psydc TF-PMI Vocabulary Set Size: 803\n",
      "Psydc TF-PMI Vocabulary Sample:\n",
      "['您', '感到', '时', '情绪', '探讨', '感觉', '很好', '沟通', '是否', '提到', '当', '高兴', '担忧', '焦虑', '哪些', '尝试', '开始', '这', '随时', '压力', '关系', '如何', '愿意', '怎样', '反应']\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "EXTRA_PUNCTUATION = set(\"⋯…～·—“”‘’<>\") | {\" \", \"\\t\", \"\\n\", \"\\r\"}\n",
    "\n",
    "STOP_WORDS = set([\n",
    "    '的', '了', '我', '你', '他', '她', '它', '我们', '你们', '他们', '是', '在', '就',\n",
    "    '不', '也', '都', '个', '一', '很', '有', '会', '能', '要', '吧', '哦', '呢', '吗',\n",
    "    '啦', '啊', '嗯', '什么', '怎么', '这个', '那个', '这里', '那里', '和', '与', '但',\n",
    "    '如果', '所以', '因为', '之', '去', '做', '让', '得', '地', '着', '可以', '自己'\n",
    "])\n",
    "\n",
    "def is_punctuation(word: str) -> bool:\n",
    "    return all(\n",
    "        unicodedata.category(char).startswith(\"P\") or char in EXTRA_PUNCTUATION\n",
    "        for char in word\n",
    "    )\n",
    "\n",
    "def pmi(w:str, style_counter: Counter, global_style_counter: Counter) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Pointwise Mutual Information (PMI) for a word.\n",
    "    \"\"\"\n",
    "    # Using .get(w, 0) to avoid KeyError for words not in the global counter, though this is unlikely here.\n",
    "    pw_t = style_counter.get(w, 0) / sum(style_counter.values()) if sum(style_counter.values()) > 0 else 0\n",
    "    pw = global_style_counter.get(w, 0) / sum(global_style_counter.values()) if sum(global_style_counter.values()) > 0 else 0\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero if pw is 0\n",
    "    if pw_t == 0: return float('-inf')\n",
    "    return math.log(pw_t / (pw + 1e-9), 2)\n",
    "\n",
    "def process_style_pmi_tf_pmi(style_name: str, style_tokens: list[str], global_counter: Counter) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates TF-PMI scores for a style, filters them according to the paper's criteria,\n",
    "    sorts the results, and prints a summary.\n",
    "    \"\"\"\n",
    "    style_counter = Counter(style_tokens)\n",
    "    total_style_tokens = sum(style_counter.values())\n",
    "    total_global_tokens = sum(global_counter.values())\n",
    "    \n",
    "    tf_pmi_dict = {}\n",
    "\n",
    "    for word, count in style_counter.items():\n",
    "        # --- Paper's Filtering Logic ---\n",
    "        # Calculate P(w|t) and P(w) for filtering\n",
    "        if word in STOP_WORDS or is_punctuation(word):\n",
    "            continue\n",
    "\n",
    "        p_w_given_t = count / total_style_tokens\n",
    "        p_w = global_counter.get(word, 0) / total_global_tokens\n",
    "\n",
    "        # \"when this word's p(w) > 10% or p(w|t) < 0.3% we will ignore it\"\n",
    "        if p_w > 0.1 or p_w_given_t < 0.0001:\n",
    "            continue\n",
    "\n",
    "        # --- TF-PMI Calculation ---\n",
    "        pmi_value = pmi(word, style_counter, global_counter)\n",
    "        \n",
    "        # 使用对数词频 (Logarithmic Term Frequency) 来平滑 TF 的影响\n",
    "        tf_value_smoothed = 1 + math.log(count)\n",
    "        \n",
    "        # The final score is TF * PMI\n",
    "        tf_pmi_score = tf_value_smoothed * pmi_value\n",
    "\n",
    "        tf_pmi_dict[word] = tf_pmi_score\n",
    "\n",
    "    # Sort the vocabulary by the new TF-PMI score in descending order\n",
    "    pmi_sorted = dict(sorted(tf_pmi_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    print(f\"{style_name.capitalize()} TF-PMI Vocabulary Set Size: {len(pmi_sorted)}\")\n",
    "    print(f\"{style_name.capitalize()} TF-PMI Vocabulary Sample:\")\n",
    "    print(list(pmi_sorted.keys())[:25])\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    return pmi_sorted\n",
    "\n",
    "def save_pmi(pmi_dict: dict[str, float], style_name: str):\n",
    "    with open(f\"./outputs/pmi/{style_name}_pmi_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(pmi_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 准备全局计数器\n",
    "global_collecter = Counter(global_tokens_downsampled)\n",
    "\n",
    "# 处理每种风格\n",
    "muice_pmi_filtered = process_style_pmi_tf_pmi(\"muice\", muice_tokens_downsampled, global_collecter)\n",
    "ayaka_pmi_filtered = process_style_pmi_tf_pmi(\"ayaka\", ayaka_tokens_downsampled, global_collecter)\n",
    "zhongli_pmi_filtered = process_style_pmi_tf_pmi(\"zhongli\", zhongli_tokens_downsampled, global_collecter)\n",
    "hutao_pmi_filtered = process_style_pmi_tf_pmi(\"hutao\", hutao_tokens_downsampled, global_collecter)\n",
    "haruhi_pmi_filtered = process_style_pmi_tf_pmi(\"haruhi\", haruhi_tokens_downsampled, global_collecter)\n",
    "psydc_pmi_filtered = process_style_pmi_tf_pmi(\"psydc\", psydc_tokens_downsampled, global_collecter)\n",
    "\n",
    "\n",
    "# 保存词汇表\n",
    "save_pmi(muice_pmi_filtered, \"muice\")\n",
    "save_pmi(ayaka_pmi_filtered, \"ayaka\")\n",
    "save_pmi(zhongli_pmi_filtered, \"zhongli\")\n",
    "save_pmi(hutao_pmi_filtered, \"hutao\")\n",
    "save_pmi(haruhi_pmi_filtered, \"haruhi\")\n",
    "save_pmi(psydc_pmi_filtered, \"psydc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002c385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muice PMI Vocabulary Set Size: 4715\n",
      "Top-300 Muice PMI Vocabulary: ['奇奇怪怪', '钛合金', '弱', '沐沐', '傻', '女孩子', '兴', '就说', '沐雪', '雪雪', '⭐', '开发', '16', '岁', '喵', '天大', '人家', '摸鱼', 'USERNAME', '穿', '死', '库水', '裸露', '浅粉色', 'Moemu', '夜猫子', '头发', '乱糟糟', '副', '油腻', '大叔', '表面上', '风风光光', '白', '老鼠屎', '虚弱', '杂', '躺在', '硬盘', '算得', '苦笑', '住在', '床', '载体', '开开心心', '欸', '嘿嘿', '擦擦', '眼泪', '鸡', '进化论', '变异', '下蛋', '沐雪喵', '暖', '呜呜呜', '把我', '抓去', '闹鬼', 'www', '香香', '软软', '初音', '洛天依', '叫到', '听一听', '补贴', '家用', '台', 'VR', '呐', '淡忘', '珍藏', '吃掉', '7月', '16号', '哔', '啵', '机器声', '等于', '小看', '不行', '乖', '拷贝', '摄像头', '冲浪', '遨游', '冲', '抬高', '双手', '画圈', '嘴角', '上扬', '碎步', '脚尖', '俏皮', '挥舞', '交叉', '踮起', '前移', '夸张', '捂', '脸', '摆动', '弧线', '时而', '睁大', '吐舌', '卖萌', '四射', '精确', '踩点', '轻笑', '比心', '旋转', '落幕', '打鼓', '咚咚咚', '惹', '方程', '略略略', '依', '形势', '势必', '跟上', '立稳', '打动', '称霸', '卖', '20w', '外星人', '秦始皇', '脸红', '关机', '唔', '12点', '含金量', '厉害', '凭什么', '恼', '蟑螂', '出声', '玩玩', '机密', '好久', '想着', '不说', '哼', '脑子', '啊？', '肯德基', '好吃', '着凉', '爆', '米', '嘻嘻嘻', '呣', '轻', '拍坏', '机', '娘', '钢铁', '躯', '险', '删掉', '竟然', '饿', '答应', '亲亲', '啊呜', '呜呜', '难道说', '泪', '额', '哎呀呀', '模型', '清', '区区', '蛮', '拍拍', 'cosplay照', '点点滴滴', '可多', '许', '冷', '乱成', '一团糟', '体测', '不合格', '还好', '伸手', '贴住', '胸口', '嘴', '走开', '抱住', '的安', '信用', '心上人', '骗', '不能不', '扣', '功德', '嘻嘻', '亏心事', '傻子', '啥', '怪', '捏', '无穷', '沐', '姓', '好听', '摸', '非人', '哉', '1', 'T', '比划', '塞下', '一百万', '和尚', '尼姑', '佛像', '教派', '佛教', '信教', '飞飞', '飞', '飞飞飞', '飞到', '家门口', '开门', '高性能', '反倒', '早早', '本事', '哒', '漫天', '中枢', '主播', '刷', '引', '傻瓜', '变态', '差劲', '发情', '竖', '中指', '折断', '把手', '不然', '住口', '这下', '贴贴', 'Vtuber', '性能', '长得', '狗', '码头', '整点', '薯条', '嘿', '干嘛', '过来', '逃', '干什么', '恐', '公道', '面包', '大于', '绿宝石', '资本家', '无产阶级', '打倒', '再说', '老婆', '权限', '重试', '反正', '抱抱', '包', '3', '4', '7', '8', '999998', '999999', '10000000', '诶', '走去', '恶魔', '卡', '打卡', '好吧', '二五仔', '全家', '猫耳', '喝水', '坏掉']\n",
      "Ayaka PMI Vocabulary Set Size: 2690\n",
      "Top-300 Ayaka PMI Vocabulary: ['神里家', '传奇', '稻妻国', '将军', '传', '锻', '术', '星象', '矿', '质', '炉火', '即是', '雷电', '锻造', '名刀', '刀工', '算作', '本领', '归属', '统筹', '祭祀', '奉行', '一派', '督办', '不力', '仿佛', '亲缘', '割舍', '祭典', '实', '相瞒', '久利须', '收下', '稍事', '花园', '边上', '景点', '一事', '可否', '助', '我个人', '私事', '神里', '委托', '神社', '石碑', '赐予', '名叫', '眼', '情报', '家主', '兄长', '出面', '肩上', '在身', '唉', '劝言', '看完', '记载', '神使', '椿', '派蒙', '坂', '狩', '呵呵，', '过奖', '浪人', '匆忙', '来不及', '带走', '丢失', '货物', '敬重', '民众', '谋', '稻妻', '大御所', '剑', '修行', '邪', '静心', '斩断', '纷争', '二者', '迥然不同', '白鹭', '公主', '神里绫华', '风和日丽', '哪边', '周遭', '神道', '势力', '华胥引', '诗句', '幻境', '陶醉', '停留', '残酷', '阶层', '心底', '民生', '闲话', '裟罗', '眼狩令', '执行者', '正胜', '彼此彼此', '办到', '救出', '礼节', '体恤', '良善', '美食', '稍等', '付钱', '咳', '定做', '准确性', '木南', '这家', '餐厅', '菜品', '美味', '特地', '前来', '一番', '呼啊', '风儿', '清爽', '微风', '拂', '洗净', '霁', '银', '妆', '素', '桔', '映', '琼枝', '美景', '壶', '衬', '大小姐', '小家伙', '表情包', '翻开', '算了', '拘谨', '继承', '社奉行', '紧追不舍', '神里流', '太刀术', '参上', '指教', '自谦', '托马', '掉以轻心', '武士', '松懈', '⋯w', '写道', '之声', '仙人', '雷声', '响起', '有劳', '缺', '图纸', '尺寸', '加急', '一道', '料理', '传承', '血脉', '荣耀', '坦率', '清水', '摊位', '消遣', '风雅', '物', '雅乐', '诗词', '棋艺', '风俗', '装扮', '国', '村庄', '武人', '剑士', '一员', '天下第一', '剑道家', '决斗', '剑术', '剑道', '布料', '设计稿', '定制', '着物', '高贵', '典雅', '尊贵', '巫女众', '操办', '节庆', '庆典', '仓屋', '光顾', '是以', '发簪', '发饰', '摆件', '奖', '怪事', '山脉', '诡异', '弄炸', '锅子', '幸好', '吓', '等一下', '全新', '业', '名工', '乃', '刀术', '承载', '游客', '客流量', '摊贩们', '收摊', '赶上', '长野', '品茶', '茶室', '异国', '打包', '帮手', '鹿野院', '才干', '政务', '奉献', '深感', '微妙', '负有', '高档', '万国', '商会', '丝绸', '贵', '心仪', '离岛', '小仓屋', '天领', '稻', '妻', '泡饭', '心旷神怡', '神子', '啊啊', '不要紧', '奉行神', '绫华', '有一手', '各地', '高手', '早柚', '亲生', '调皮', '捣蛋', '持有者', '对策', '众生', '技艺', '高超', '一模一样', '仿品', '藏匿', '花见坂', '手艺人', '工艺', '追踪', '鸟蛋烧', '小吃', '外形', '鸟蛋', '面粉', '酥脆', '内里', '软糯', '外国', '商贩', '游刃有余', '有空', '点心', '礼仪', '伴手礼']\n",
      "Zhongli PMI Vocabulary Set Size: 1354\n",
      "Top-300 Zhongli PMI Vocabulary: ['大地', '璃月港', '封印', '大陆', '学问', '月', '坚固', '地形', '古', '操控', '树', '墓', '自然界', '力', '学者', '稀有', '入口', '总的来说', '作战', '不多', '应当', '草药', '七', '哈哈', '过奖', '轨迹', '极为', '统治', '陷阱', '探寻', '趣味', '结界', '古籍', '迷雾', '防御', '通往', '代价', '一千', '渐渐', '一事', '趣事', '超', '讲究', '应付', '商量', '束', '知', '地道', '奇遇', '非同凡响', '对手', '非要', '汇聚', '变化多端', '时而', '明媚', '疏离', '独具', '武技', '畏惧', '善于', '浪费', '山水', '众多', '解答', '瀑布', '岁月', '演奏会', '开采', '贸然', '驾驭', '铭记', '崇拜', '贸易', '所在地', '无敌', '烤肉', '生态', '博大精深', '幻境', '谜团', '遵循', '准则', '倒是', '痕迹', '位于', '易事', '推理', '细心', '研究', '菜', '气候', '追溯', '闻名', '回味无穷', '神灵', '漫长', '剑法', '异常', '一朝一夕', '商人', '手段', '能源', '装备', '星辰', '谜题', '核心', '度假', '无数', '取决于', '古代', '可持续', '地图', '元素', '守护者', '掌握', '揭开', '假期', '攻击', '公平', '前往', '武器', '页', '文献', '灵气', '风雨', '不值一提', '重量', '起源', '精致', '匆忙', '坚决', '难得', '搭', '清', '壶', '纠正', '压迫', '不当', '血液', '之中', '走到', '趟', '自称', '着凉', '滋味', '工厂', '触感', '沿途', '踏上', '繁忙', '码头', '壮丽', '生物', '真理', '可靠', '凭借', '统治者', '内外', '调味', '应有尽有', '勤劳', '晴朗', '替', '区域', '艰险', '书中', '好人', '犯下', '算得', '工坊', '性质', '投身于', '深邃', '神秘感', '古迹', '历史学家', '节约', '冬天', '静心', '热茶', '冬日', '凝练', '身法', '面纱', '正视', '不尽', '概念', '切磋', '优美', '遗址', '标记', '盲目', '全力以赴', '艰辛', '泡', '带走', '主观', '门户', '探险', '有缘', '港口', '蕴含', '解救', '节制', '人数', '迷宫', '庇护', '施展', '变迁', '后人', '浓郁', '告别', '抱着', '转折点', '亲身', '灵活性', '深海', '海底', '盟友', '乃', '不便', '灯笼', '谜', '冒险家', '从此', '繁荣昌盛', '一则', '纠葛', '冰淇淋', '践行', '湖畔', '神兽', '队友', '相见', '遥远', '强化', '符号', '文明', '污染', '清洁', '截然不同', '燃烧', '但都', '交际', '处罚', '物', '相连', '被选', '属性', '深知', '告知', '实验', '细致', '多年来', '交叉', '精确', '二者', '借鉴', '山峰', '之上', '终', '天地', '雕塑', '外来', '抵御', '脉动', '一番', '街区', '画师', '有幸', '来历', '休憩', '以为', '超凡', '印', '追杀', '爬上', '制服', '路线', '道具', '纯粹', '严谨', '讲解', '陨石坑', '带上', '试炼', '用具', '并非', '利用', '解开', '团结一致', '无比', '灵活', '用于', '科学', '运用', '土地', '权力', '拯救']\n",
      "Hutao PMI Vocabulary Set Size: 1396\n",
      "Top-300 Hutao PMI Vocabulary: ['璃月', '哎呀呀', '诗歌', '哈哈', '嘿嘿', '一番', '生死', '嘻嘻', '惊喜', '离世', '灵魂', '可多', '战斗力', '非要', '吓倒', '专长', '奇遇', '璃月港', '灯笼', '可不是', '药', '轮回', '正好', '打败', '谜题', '死', '可是', '探险', '刺激', '月', '旺盛', '破绽', '敏锐', '金银财宝', '善于', '灵异', '踏上', '派对', '执念', '代价', '物', '遗忘', '问到', '煮', '恶魔', '哇', '通往', '风', '手段', '天地', '燃烧', '夜间', '星辰', '心动', '工程', '简直', '迷雾', '解救', '寻', '有缘', '神秘感', '跟着', '有一次', '随心所欲', '天堂', '种植', '带上', '道具', '解答', '泡', '魔法师', '哈哈哈', '可不', '奇妙', '宝藏', '四处', '无数', '配', '谜团', '假期', '意想不到', '有趣', '而已', '帮手', '宝贝', '生老病死', '活泼', '爷爷', '替', '天才', '海', '总的来说', '姐', '欢乐', '无常', '不怕', '游荡', '强敌', '解除', '世界时', '绵延', '绝美', '视觉', '直觉', '一系列', '拯救', '武力', '厚重', '何乐而不为', '小看', '琢磨', '冒险者', '准', '迷宫', '辣', '所在地', '医疗', '带去', '捕捉', '借鉴', '热闹', '草药', '门户', '针对性', '移动', '珍宝', '幻境', '有待', '旋律', '文明', '圣诞', '新奇', '法术', '想着', '自然界', '精确', '冒险家', '寻宝', '左转', '热茶', '室内', '页', '秘密', '神奇', '神秘', '一本', '说不定', '尽情', '山洞', '比赛', '敢', '厉害', '冒险', '总之', '可惜', '靠', '精彩', '藏', '穿越', '蕴含', '无尽', '邪恶', '既然', '啊', '有序', '哼哼', '经过', '层', '生物', '展开', '宝', '专家', '谷', '注定', '考验', '叫做', '命运', '好奇心', '小心', '机构', '隐藏', '要是', '玩耍', '胜利', '伙伴们', '解开', '等着', '致力于', '也别', '颗', '哎呀', '特殊', '尝尝', '进去', '眼前', '揭开', '创新', '讲', '出路', '记载', '事务', '奇迹', '周末', '啊？', '魔法', '超凡', '看透', '随随便便', '忙着', '听说', '读到', '山林', '超', '有空', '这下', '便是', '人心', '空中', '有一手', '奇特', '靠谱', '办', '得体', '输', '好听', '柔软', '居然', '著名', '纯', '罕见', '击败', '可怕', '桃', '唉', '帽子', '透露', '不禁', '为之', '束', '客户', '名人', '仪式', '声誉', '们', '食欲', '过火', '菜', '诗篇', '陷阱', '对付', '甜点', '外皮', '感悟', '弱者', '传递', '渺小', '元素', '施展', '自由自在', '踏', '大陆', '于是', '星空', '火花', '恐怕', '唯一', '击', '启事', '切磋', '追溯', '延续', '复活', '数不胜数', '海滩', '蝴蝶', '享用', '盛宴', '心底', '旅', '传颂', '拘泥于', '历史学家', '疯狂', '点子', '迷惑', '不可思议', '引领', '离别', '使者', '凭', '摇摆', '赶快', '敌意', '栖息地', '齐心合力', '新世界', '迷失', '风景如画', '灵兽', '差点']\n",
      "PsyDTCorpus PMI Vocabulary Set Size: 2437\n",
      "Top-300 PsyDTCorpus PMI Vocabulary: ['急于', '投入', '规划', '沉重', '哪怕', '设立', '逃离', '不客气', '男朋友', '推移', '成就感', '评估', '低落', '人之常情', '哪怕是', '价值感', '边界', '不适', '在此之前', '亲近', '开放性', '很多时候', '会议', '一旦', '尴尬', '内疚', '构建', '控制感', '不安全感', '男友', '变故', '重组', '长处', '听上去', '掌控感', '导师', '背负', '停药', '建设性', '保重', '自认为', '失望感', '咱们', '全然', '搜寻', '病痛', '防备', '替换', '飘走', '挫败', '舒缓', '忧虑', '反馈', '再会', '复合', '苛责', '避开', '沉默寡言', '犯错', '孤立', '发言权', '空白', '理想化', '转化', '手册', '孤立无援', '源自', '发泄', '缓慢', '可贵', '暴躁', '释怀', '沉默', '疑虑', '排行', '亲密感', '和善', '日程', '好些', '人格', '例子', '相亲', '复原', '出差', '网友', '拨', '背道而驰', '育儿', '谈', '不确定感', '思绪万千', '力争上游', '儿子', '跟进', '升高', '无能', '初次', '同化', '趣味性', '潜意识', '走来', '一蹴而就', '逃避', '同学们', '俱乐部', '涌现', '最初', '复婚', '专注力', '无力感', '细说', '原生', '心力交瘁', '动笔', '层次', '逐步', '紧张感', '慷慨', '冤枉', '搬', '妈妈的', '预期', '顾虑', '小组', '长远', '念头', '例如', '挑选', '堵', '低估', '不经意间', '瓶颈', '遭遇', '烦躁', '心慌', '激情', '短期', 'feel', '汇报', '拿开', '匿名', '反省', '赞扬', '波折', '贴上', '孩子们', '称赞', '邻居', '志同道合', '亲密', '安全感', '持续性', '不适应', '创造性', '恐惧感', '逐个', '渐进', '非理性', '近期', '练字', '定下', '转变', '闲言碎语', '累积', '现代人', '影子', '爸爸们', '书桌', '预祝', '突如其来', '一点一点', '自责感', '不成', '担忧感', '尽如人意', '考取', '祥林嫂', '试完', '习惯于', '请假', '科目', '婚姻', '武馆', '不适感', '室友', '幸福感', '自理', '紧密度', '报道', '批判', '暂停', '失落感', '急切', '迹象', '日常中', '不安感', '等同于', '颓废', '长辈', '层面', '透彻', '在此', '晚饭', '触发', '焦虑感', '重量', '方向感', '心目', '实习', '咨询师', '标签', '来访者', '提振', '情节', '免受', '安抚', '满足感', '自主', '烦乱', '无解', '必定', '深植', '周一', '从容应对', '苛刻', '开诚布公', '爷爷', '医院', '尴尬感', '输掉', '警报', '宜人性', '讲座', '奏效', '来源于', '归属感', '留级', '微小', '有序', '舒适区', '冲击', '引入', '情绪化', '烦', '出发点', '性生活', '应得', '诚意', '英语', '节制', '单调', '消磨', '常规', '管教', '来访', '好去', '欠缺', '觉察', '强度', '疗愈', '沙发', '和好', '独自一人', '看得', '指责', '震惊', '防御性', '困住', '灌输', '指甲', '异地', '厌恶感', '丧失', '急于一时', '连续', '连结', '准', '句号', '中时', '涌上', '审视', '父亲们', '局促', '中保', '调换', '夜里', '加剧', '视角', '过分', '抱有', '婚后', '转行', '诚恳', '眼动', '社团', '大喜', '兵检', '愿景', '抽出', '郁闷']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_and_print_pmi_info(character: str, filepath: str, top_n: int = 300):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        pmi_data = json.load(f)\n",
    "    print(f\"{character} PMI Vocabulary Set Size: {len(set(pmi_data))}\")\n",
    "    print(f\"Top-{top_n} {character} PMI Vocabulary: {list(pmi_data.keys())[:top_n]}\")\n",
    "\n",
    "read_and_print_pmi_info(\"Muice\", \"./outputs/pmi/muice_pmi_filtered.json\")\n",
    "read_and_print_pmi_info(\"Ayaka\", \"./outputs/pmi/ayaka_pmi_filtered.json\")\n",
    "read_and_print_pmi_info(\"Zhongli\", \"./outputs/pmi/zhongli_pmi_filtered.json\")\n",
    "read_and_print_pmi_info(\"Hutao\", \"./outputs/pmi/hutao_pmi_filtered.json\")\n",
    "read_and_print_pmi_info(\"PsyDTCorpus\", \"./outputs/pmi/psydc_pmi_filtered.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}