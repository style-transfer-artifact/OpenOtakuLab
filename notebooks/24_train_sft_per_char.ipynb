{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8240ae82",
   "metadata": {},
   "source": [
    "# Per-Character Training Set Construction and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "# os.chdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29b9f1",
   "metadata": {},
   "source": [
    "## Training Set Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89233681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 定义路径 (基于当前工作目录: OtakuLab)\n",
    "input_file = \"data/neutral_sentences_with_CoT.jsonl\"\n",
    "output_dir = \"data/per_character_train\"\n",
    "dataset_info_path = os.path.join(output_dir, \"dataset_info.json\")\n",
    "\n",
    "# 角色英文名到中文名的映射\n",
    "EN_NAME_TO_ZH = {\n",
    "    \"Muice\": \"沐雪\",\n",
    "    \"Ayaka\": \"神里绫华\",\n",
    "    \"Zhongli\": \"钟离\",\n",
    "    \"Hutao\": \"胡桃\",\n",
    "    \"Haruhi\": \"凉宫春日\"\n",
    "}\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 读取数据并按角色分组\n",
    "character_data = {}\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        char = item['character']\n",
    "        \n",
    "        # 获取角色中文名，如果不在映射中则使用原名\n",
    "        char_zh = EN_NAME_TO_ZH.get(char, char)\n",
    "        \n",
    "        # 映射字段\n",
    "        neutral_sentence = item['neutral']\n",
    "        thinking_process = item['CoT']\n",
    "        output_sentence = item['original']\n",
    "        \n",
    "        # 构建 ShareGPT 格式\n",
    "        conversation = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"Neutral Content: {neutral_sentence}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": f\"{thinking_process}\\n\\n{output_sentence}\"\n",
    "                }\n",
    "            ],\n",
    "            \"system\": f\"You are a style transfer expert. Your task is to mimic the personality of {char_zh} and generate a new sentence that matches the (s)he style, based on the content of a neutral sentence.\"\n",
    "        }\n",
    "        \n",
    "        if char not in character_data:\n",
    "            character_data[char] = []\n",
    "        character_data[char].append(conversation)\n",
    "\n",
    "# 保存每个角色的数据集并构建 dataset_info.json\n",
    "dataset_info = {}\n",
    "\n",
    "for char, data in character_data.items():\n",
    "    # 处理文件名（避免非法字符）\n",
    "    safe_char_name = \"\".join([c for c in char if c.isalnum() or c in (' ', '_', '-')]).strip()\n",
    "    file_name = f\"{safe_char_name}_train.json\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    # 保存角色数据\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 添加到 dataset_info\n",
    "    dataset_info[f\"{safe_char_name}_style_train\"] = {\n",
    "        \"file_name\": file_name,\n",
    "        \"formatting\": \"sharegpt\",\n",
    "        \"columns\": {\n",
    "            \"messages\": \"conversations\",\n",
    "            \"system\": \"system\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 保存 dataset_info.json\n",
    "with open(dataset_info_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"处理完成！共处理 {len(character_data)} 个角色。\")\n",
    "print(f\"数据集已保存至: {os.path.abspath(output_dir)}\")\n",
    "print(f\"配置文件已保存至: {os.path.abspath(dataset_info_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcd970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Generated Training Commands ###\n",
      "\n",
      "# Character: Muice | Count: 2704 | Scheme: A\n",
      "llamafactory-cli train \\\n",
      "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\n",
      "    --template qwen3 \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --finetuning_type lora \\\n",
      "    --dataset Muice_style_train \\\n",
      "    --lora_rank 32 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --max_samples 10000 \\\n",
      "    --overwrite_cache \\\n",
      "    --per_device_train_batch_size 4 \\\n",
      "    --gradient_accumulation_steps 8 \\\n",
      "    --learning_rate 6e-4 \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --max_grad_norm 1.0 \\\n",
      "    --num_train_epochs 5 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --warmup_ratio 0.03 \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 300 \\\n",
      "    --plot_loss \\\n",
      "    --output_dir outputs/Per-Character/Muice \\\n",
      "    --overwrite_output_dir \\\n",
      "    --report_to none\n",
      "\n",
      "==================================================\n",
      "\n",
      "# Character: Ayaka | Count: 1416 | Scheme: B\n",
      "llamafactory-cli train \\\n",
      "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\n",
      "    --template qwen3 \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --finetuning_type lora \\\n",
      "    --dataset Ayaka_style_train \\\n",
      "    --lora_rank 32 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --max_samples 10000 \\\n",
      "    --overwrite_cache \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 4e-4 \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --max_grad_norm 1.0 \\\n",
      "    --num_train_epochs 5 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --warmup_ratio 0.05 \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 300 \\\n",
      "    --plot_loss \\\n",
      "    --output_dir outputs/Per-Character/Ayaka \\\n",
      "    --overwrite_output_dir \\\n",
      "    --report_to none\n",
      "\n",
      "==================================================\n",
      "\n",
      "# Character: Zhongli | Count: 511 | Scheme: C\n",
      "llamafactory-cli train \\\n",
      "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\n",
      "    --template qwen3 \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --finetuning_type lora \\\n",
      "    --dataset Zhongli_style_train \\\n",
      "    --lora_rank 32 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.1 \\\n",
      "    --max_samples 10000 \\\n",
      "    --overwrite_cache \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 2e-4 \\\n",
      "    --weight_decay 0.02 \\\n",
      "    --max_grad_norm 1.0 \\\n",
      "    --num_train_epochs 8 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --warmup_ratio 0.1 \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 300 \\\n",
      "    --plot_loss \\\n",
      "    --output_dir outputs/Per-Character/Zhongli \\\n",
      "    --overwrite_output_dir \\\n",
      "    --report_to none\n",
      "\n",
      "==================================================\n",
      "\n",
      "# Character: Hutao | Count: 838 | Scheme: B\n",
      "llamafactory-cli train \\\n",
      "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\n",
      "    --template qwen3 \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --finetuning_type lora \\\n",
      "    --dataset Hutao_style_train \\\n",
      "    --lora_rank 32 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --max_samples 10000 \\\n",
      "    --overwrite_cache \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 4e-4 \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --max_grad_norm 1.0 \\\n",
      "    --num_train_epochs 5 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --warmup_ratio 0.05 \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 300 \\\n",
      "    --plot_loss \\\n",
      "    --output_dir outputs/Per-Character/Hutao \\\n",
      "    --overwrite_output_dir \\\n",
      "    --report_to none\n",
      "\n",
      "==================================================\n",
      "\n",
      "# Character: Haruhi | Count: 1099 | Scheme: B\n",
      "llamafactory-cli train \\\n",
      "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\n",
      "    --template qwen3 \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --finetuning_type lora \\\n",
      "    --dataset Haruhi_style_train \\\n",
      "    --lora_rank 32 \\\n",
      "    --lora_alpha 16 \\\n",
      "    --lora_dropout 0.05 \\\n",
      "    --max_samples 10000 \\\n",
      "    --overwrite_cache \\\n",
      "    --per_device_train_batch_size 2 \\\n",
      "    --gradient_accumulation_steps 16 \\\n",
      "    --learning_rate 4e-4 \\\n",
      "    --weight_decay 0.01 \\\n",
      "    --max_grad_norm 1.0 \\\n",
      "    --num_train_epochs 5 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --warmup_ratio 0.05 \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 300 \\\n",
      "    --plot_loss \\\n",
      "    --output_dir outputs/Per-Character/Haruhi \\\n",
      "    --overwrite_output_dir \\\n",
      "    --report_to none\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据数据量生成训练脚本\n",
    "print(\"### Generated Training Commands ###\\n\")\n",
    "\n",
    "for char, data in character_data.items():\n",
    "    count = len(data)\n",
    "    safe_char_name = \"\".join([c for c in char if c.isalnum() or c in (' ', '_', '-')]).strip()\n",
    "    dataset_name = f\"{safe_char_name}_style_train\"\n",
    "    output_dir = f\"outputs/Per-Character/{safe_char_name}\"\n",
    "    \n",
    "    # 默认参数\n",
    "    params = {\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"learning_rate\": \"6e-4\",\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lora_dropout\": 0.05\n",
    "    }\n",
    "    \n",
    "    scheme = \"A\"\n",
    "    \n",
    "    # 根据数据量调整参数\n",
    "    if count < 600:\n",
    "        scheme = \"C\"\n",
    "        params.update({\n",
    "            \"per_device_train_batch_size\": 2,\n",
    "            \"gradient_accumulation_steps\": 16,\n",
    "            \"learning_rate\": \"2e-4\",\n",
    "            \"warmup_ratio\": 0.1,\n",
    "            \"num_train_epochs\": 8,\n",
    "            \"weight_decay\": 0.02,\n",
    "            \"lora_dropout\": 0.1\n",
    "        })\n",
    "    elif count < 1500:\n",
    "        scheme = \"B\"\n",
    "        params.update({\n",
    "            \"per_device_train_batch_size\": 2,\n",
    "            \"gradient_accumulation_steps\": 16,\n",
    "            \"learning_rate\": \"4e-4\",\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"num_train_epochs\": 5,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"lora_dropout\": 0.05\n",
    "        })\n",
    "    \n",
    "    print(f\"# Character: {char} | Count: {count} | Scheme: {scheme}\")\n",
    "    \n",
    "    command = f\"\"\"llamafactory-cli train \\\\\n",
    "    --model_name_or_path /root/autodl-tmp/Qwen3-4B/ \\\\\n",
    "    --template qwen3 \\\\\n",
    "    --stage sft \\\\\n",
    "    --do_train \\\\\n",
    "    --finetuning_type lora \\\\\n",
    "    --dataset {dataset_name} \\\\\n",
    "    --lora_rank 32 \\\\\n",
    "    --lora_alpha 16 \\\\\n",
    "    --lora_dropout {params['lora_dropout']} \\\\\n",
    "    --max_samples 10000 \\\\\n",
    "    --overwrite_cache \\\\\n",
    "    --per_device_train_batch_size {params['per_device_train_batch_size']} \\\\\n",
    "    --gradient_accumulation_steps {params['gradient_accumulation_steps']} \\\\\n",
    "    --learning_rate {params['learning_rate']} \\\\\n",
    "    --weight_decay {params['weight_decay']} \\\\\n",
    "    --max_grad_norm 1.0 \\\\\n",
    "    --num_train_epochs {params['num_train_epochs']} \\\\\n",
    "    --lr_scheduler_type cosine \\\\\n",
    "    --warmup_ratio {params['warmup_ratio']} \\\\\n",
    "    --logging_steps 10 \\\\\n",
    "    --save_steps 300 \\\\\n",
    "    --plot_loss \\\\\n",
    "    --output_dir {output_dir} \\\\\n",
    "    --overwrite_output_dir \\\\\n",
    "    --report_to none\"\"\"\n",
    "    \n",
    "    print(command)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
