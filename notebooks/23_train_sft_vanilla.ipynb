{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b28ddac3",
   "metadata": {},
   "source": [
    "# General Fine-tuning Model based on Qwen3-4B (Baseline C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Remote Jupyter Book.\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "# os.chdir(\"/root/OtakuLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e1a79",
   "metadata": {},
   "source": [
    "## Define Training Set Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "class InstructionComponents(TypedDict):\n",
    "    lexical_keywords: list[str]\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "class DatasetItem(TypedDict):\n",
    "    character: str\n",
    "    neutral_sentence: str\n",
    "    instruction_components: InstructionComponents | dict[str, list[str]|dict]\n",
    "    output: str\n",
    "\n",
    "class DatasetStorage:\n",
    "    def __init__(self) -> None:\n",
    "        self.items: dict[str, DatasetItem] = {}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def new_item(self, character: str, neutral_sentence: str, output: str):\n",
    "        item = DatasetItem(character=character,\n",
    "                           neutral_sentence=neutral_sentence,\n",
    "                           instruction_components={},\n",
    "                           output=output)\n",
    "        self.items[output] = item\n",
    "\n",
    "    def save_characters_keywords(self, character: str, lexical_keywords: list[str]):\n",
    "        saved = False\n",
    "        for item in self.items.values():\n",
    "            if item['character'] == character:\n",
    "                output = item['output']\n",
    "                self.items[output]['instruction_components']['lexical_keywords'] = lexical_keywords\n",
    "                saved = True\n",
    "        if not saved:\n",
    "            raise ValueError(f\"角色 {character} 未找到\")        \n",
    "\n",
    "    def save_component(self,\n",
    "                       output: str,\n",
    "                       pragmatic_styles: Optional[list[str]] = None):\n",
    "        if output not in self.items.keys():\n",
    "            raise ValueError(f\"风格句: {output} 似乎未加载\")\n",
    "        \n",
    "        if pragmatic_styles:\n",
    "            self.items[output]['instruction_components']['pragmatic_styles'] = pragmatic_styles\n",
    "\n",
    "    @staticmethod\n",
    "    def _verify_validity(item: DatasetItem):\n",
    "        if not all((item['character'],\n",
    "                    item['neutral_sentence'],\n",
    "                    item['output'],\n",
    "                    item['instruction_components'])):\n",
    "            return False\n",
    "        \n",
    "        instruction_components = item['instruction_components']\n",
    "        \n",
    "        return all((instruction_components.get('lexical_keywords', None),\n",
    "                    instruction_components.get('pragmatic_styles', None)))\n",
    "\n",
    "    def output(self, output_path: Path):\n",
    "        items: list[DatasetItem] = list(self.items.values())\n",
    "        vaild_items = []\n",
    "        vaild_items_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if not self._verify_validity(item):\n",
    "                continue\n",
    "\n",
    "            instr = item[\"instruction_components\"]\n",
    "            keywords = \", \".join(instr[\"lexical_keywords\"]) if instr[\"lexical_keywords\"] else \"None\"\n",
    "            pragmatic_styles = \", \".join(instr[\"pragmatic_styles\"]) if instr[\"pragmatic_styles\"] else \"None\"\n",
    "            user_prompt = (\n",
    "                f\"Target Character {item['character']}\\n\"\n",
    "                f\"Personality: {pragmatic_styles}\\n\"\n",
    "                f\"Keywords: {keywords}\\n\"\n",
    "                f\"Neutral Content: {item['neutral_sentence']}\\n\"\n",
    "            )\n",
    "\n",
    "            # 构建 ShareGPT 格式\n",
    "            conversation = {\n",
    "                \"conversations\": [\n",
    "                    {\n",
    "                        \"from\": \"human\",\n",
    "                        \"value\": user_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"from\": \"gpt\",\n",
    "                        \"value\": item['output']\n",
    "                    }\n",
    "                ],\n",
    "                \"system\": f\"You are a style transfer expert. Your task is to generate a new sentence that matches the target style, based on the content of a neutral sentence.\"\n",
    "            }\n",
    "\n",
    "            vaild_items.append(conversation)\n",
    "            vaild_items_count += 1\n",
    "\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vaild_items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"训练集已导出至 {output_path}, 总有效训练集数量: {len(vaild_items)}, 跳过数量: {len(items) - vaild_items_count}\")\n",
    "\n",
    "dataset_storage = DatasetStorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf54e41",
   "metadata": {},
   "source": [
    "## Load Neutral Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792711d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载了 6510 条训练集条目\n"
     ]
    }
   ],
   "source": [
    "neutral_sentences_jsonl_file = Path(\"./data/neutral_sentences_with_CoT.jsonl\")\n",
    "\n",
    "EN_NAME_TO_ZH = {\"Muice\": \"沐雪\", \"Ayaka\": \"神里绫华\", \"Zhongli\": \"钟离\", \"Hutao\": \"胡桃\", \"Haruhi\": \"凉宫春日\"}\n",
    "\n",
    "class NSFileItem(TypedDict):\n",
    "    character: str\n",
    "    original: str\n",
    "    neutral: str\n",
    "    CoT: str\n",
    "\n",
    "def load_jsonl_file(jsonl_file: Path):\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        jsonl_file_lines = f.readlines()\n",
    "\n",
    "    for line in jsonl_file_lines:\n",
    "        if line := line.rstrip():\n",
    "            item: NSFileItem = json.loads(line)\n",
    "            character = EN_NAME_TO_ZH.get(item[\"character\"], item[\"character\"])\n",
    "            neutral = item[\"neutral\"]\n",
    "            dataset_storage.new_item(character, neutral, item[\"original\"])\n",
    "\n",
    "load_jsonl_file(neutral_sentences_jsonl_file)\n",
    "print(f\"成功加载了 {len(dataset_storage)} 条训练集条目\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12675261",
   "metadata": {},
   "source": [
    "## Process Lexical Layer Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae650c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_keywords_from_file(file: Path, top_n: int = 25) -> list[str]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data: dict[str, float] = json.loads(f.read())\n",
    "    return list(data.keys())[:top_n]\n",
    "\n",
    "Lexical_Muice = get_lexical_keywords_from_file(Path(\"./outputs/pmi/muice_pmi_filtered.json\"))\n",
    "Lexical_Ayaka = get_lexical_keywords_from_file(Path(\"./outputs/pmi/ayaka_pmi_filtered.json\"))\n",
    "Lexical_Zhongli = get_lexical_keywords_from_file(Path(\"./outputs/pmi/zhongli_pmi_filtered.json\"))\n",
    "Lexical_Hutao = get_lexical_keywords_from_file(Path(\"./outputs/pmi/hutao_pmi_filtered.json\"))\n",
    "Lexical_Haruhi = get_lexical_keywords_from_file(Path(\"./outputs/pmi/haruhi_pmi_filtered.json\"))\n",
    "\n",
    "dataset_storage.save_characters_keywords(\"沐雪\", Lexical_Muice)\n",
    "dataset_storage.save_characters_keywords(\"神里绫华\", Lexical_Ayaka)\n",
    "dataset_storage.save_characters_keywords(\"钟离\", Lexical_Zhongli)\n",
    "dataset_storage.save_characters_keywords(\"胡桃\", Lexical_Hutao)\n",
    "dataset_storage.save_characters_keywords(\"凉宫春日\", Lexical_Haruhi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0419e42",
   "metadata": {},
   "source": [
    "## Process Pragmatic Style Layer Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24c094",
   "metadata": {},
   "source": [
    "### Read Pragmatic Style Files for Each Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059b9b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新了 7377 条训练集条目的风格向量，已跳过: 714 条\n"
     ]
    }
   ],
   "source": [
    "Pragmatic_Muice = \"./outputs/pragmatic/muice.jsonl\"\n",
    "Pragmatic_ayaka = \"./outputs/pragmatic/ayaka.jsonl\"\n",
    "Pragmatic_zhongli = \"./outputs/pragmatic/zhongli.jsonl\"\n",
    "Pragmatic_hutao = \"./outputs/pragmatic/hutao.jsonl\"\n",
    "Pragmatic_haruhi = \"./outputs/pragmatic/haruhi.jsonl\"\n",
    "\n",
    "class RawPCFGItem(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    pragmatic_styles: list[dict[str, float]]\n",
    "\n",
    "class PCFGItem(TypedDict):\n",
    "    response: str\n",
    "    pragmatic_styles: list[str]\n",
    "\n",
    "def read_pcfg_jsonl_file(jsonl_file: Path, threshold: Optional[float] = None) -> list[PCFGItem]:\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    raw_items: list[RawPCFGItem] = []\n",
    "    items: list[PCFGItem] = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line := line.strip():\n",
    "            raw_item: RawPCFGItem = json.loads(line)\n",
    "            raw_items.append(raw_item)\n",
    "\n",
    "    # list[dict[str, float]] -> dict[str, float] -> list[str]\n",
    "    for raw_item in raw_items:\n",
    "        raw_pragmatic_styles = raw_item[\"pragmatic_styles\"]\n",
    "        pragmatic_styles: dict[str, float] = {}\n",
    "\n",
    "        for vec in raw_pragmatic_styles:\n",
    "            pragmatic_styles.update(vec)\n",
    "\n",
    "        threshold = threshold or 0\n",
    "        final_styles: list[str] = []\n",
    "\n",
    "        for key, value in pragmatic_styles.items():\n",
    "            if value > threshold:\n",
    "                final_styles.append(key)\n",
    "        \n",
    "        item = PCFGItem(response=raw_item[\"response\"], pragmatic_styles=final_styles)\n",
    "        items.append(item)\n",
    "\n",
    "    return items\n",
    "\n",
    "pcfg_muice_items = read_pcfg_jsonl_file(Path(Pragmatic_Muice), 0.4)\n",
    "pcfg_ayaka_items = read_pcfg_jsonl_file(Path(Pragmatic_ayaka), 0.4)\n",
    "pcfg_zhongli_items = read_pcfg_jsonl_file(Path(Pragmatic_zhongli), 0.4)\n",
    "pcfg_hutao_items = read_pcfg_jsonl_file(Path(Pragmatic_hutao), 0.4)\n",
    "pcfg_haruhi_items = read_pcfg_jsonl_file(Path(Pragmatic_haruhi), 0.4)\n",
    "\n",
    "pcfg_import_items = pcfg_muice_items + pcfg_ayaka_items + pcfg_zhongli_items + pcfg_hutao_items + pcfg_haruhi_items\n",
    "\n",
    "skiped = 0\n",
    "for item in pcfg_import_items:\n",
    "    try:\n",
    "        dataset_storage.save_component(item[\"response\"], pragmatic_styles=item[\"pragmatic_styles\"])\n",
    "    except ValueError as e:\n",
    "        skiped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"更新了 {len(pcfg_import_items)} 条训练集条目的风格向量，已跳过: {skiped} 条\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb576b87",
   "metadata": {},
   "source": [
    "## Export Training Set File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e3e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集已导出至 evaluate\\dataset\\vanilla_llm_train.json, 总有效训练集数量: 5786, 跳过数量: 724\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = Path(\"./data/vanilla_llm_train.json\")\n",
    "\n",
    "dataset_storage.output(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
