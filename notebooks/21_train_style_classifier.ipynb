{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f000341a",
   "metadata": {},
   "source": [
    "# Style Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7766d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "BACKBONE_PATH = str(Path('../Models/chinese-roberta-wwm-ext').resolve())\n",
    "DATASET_JSON = Path('data/llm_train.json')\n",
    "OUTPUT_DIR = Path('outputs/style-classifier')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Backbone path:', BACKBONE_PATH)\n",
    "print('Dataset json:', DATASET_JSON.resolve())\n",
    "print('Output dir:', OUTPUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f8c7f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9327a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5699\n",
      "Class distribution:\n",
      " role\n",
      "沐雪      2119\n",
      "神里绫华    1358\n",
      "凉宫春日     933\n",
      "胡桃       816\n",
      "钟离       473\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train size: 4559 Val size: 1140\n"
     ]
    }
   ],
   "source": [
    "# Data loading: unified from JSON only\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_text_role_dataframe() -> pd.DataFrame:\n",
    "    path = DATASET_JSON\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"未找到 {path}. 请先在 LLMFine-tuning.ipynb 中运行“导出训练集文件”单元格生成 llm_train.json。\"\n",
    "        )\n",
    "    data: List[Dict[str, Any]] = json.loads(path.read_text(encoding='utf-8'))\n",
    "    rows = []\n",
    "    for it in data:\n",
    "        text = it.get('output')\n",
    "        role = it.get('character')\n",
    "        if not text or not role:\n",
    "            continue\n",
    "        rows.append({'text': text, 'role': role})\n",
    "    df = pd.DataFrame(rows).dropna().reset_index(drop=True)\n",
    "    # 基础清洗：去除极短文本\n",
    "    df = df[df['text'].str.len() > 5].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# 加载数据并预览\n",
    "full_df = load_text_role_dataframe()\n",
    "\n",
    "# Prepare train/val split from loaded DataFrame\n",
    "assert {'text', 'role'}.issubset(set(full_df.columns))\n",
    "print('Total samples:', len(full_df))\n",
    "print('Class distribution:\\n', full_df['role'].value_counts())\n",
    "\n",
    "# Stratified split\n",
    "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=SEED, stratify=full_df['role'])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print('\\nTrain size:', len(train_df), 'Val size:', len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68f03a",
   "metadata": {},
   "source": [
    "## Define Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5adb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset class\n",
    "class TextRoleDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], roles: List[int], tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.roles = roles\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        role = self.roles[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(role, dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "    \n",
    "# Model definition\n",
    "class CharacterStyleClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name: str, embed_dim: int = 768, proj_dim: int = 256, num_roles: int = 6, dropout: float = 0.4):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(backbone_name, output_hidden_states=False)\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "        assert self.hidden_size == embed_dim, f\"Backbone hidden_size {self.hidden_size} != embed_dim {embed_dim}\"\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.classifier = nn.Linear(proj_dim, num_roles)\n",
    "\n",
    "    def mean_pooling(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_hidden = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        mean_pooled = sum_hidden / sum_mask\n",
    "        return mean_pooled\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = out.last_hidden_state\n",
    "        sent_emb = self.mean_pooling(last_hidden, attention_mask)\n",
    "        proj = self.proj(sent_emb)\n",
    "        logits = self.classifier(proj)\n",
    "        return logits, proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0337fac",
   "metadata": {},
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilities: save/load, centers, style score\n",
    "\n",
    "def save_checkpoint(model: nn.Module, tokenizer, label_encoder: LabelEncoder, role_centers: Dict[str, np.ndarray], path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model.backbone.save_pretrained(path)  # type: ignore\n",
    "    torch.save({'proj_state': model.proj.state_dict(), 'classifier_state': model.classifier.state_dict()}, os.path.join(path, 'head.pt'))  # type: ignore\n",
    "    tokenizer.save_pretrained(path)\n",
    "    with open(os.path.join(path, 'label_encoder.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump({'classes': label_encoder.classes_.tolist()}, f, ensure_ascii=False)\n",
    "    np.savez(os.path.join(path, 'role_centers.npz'), **role_centers)\n",
    "    print(f\"Saved checkpoint to {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str, device='cpu') -> Tuple[nn.Module, 'Tokenizer', LabelEncoder, Dict[str, np.ndarray]]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    with open(os.path.join(path, 'label_encoder.json'), 'r', encoding='utf-8') as f:\n",
    "        le_json = json.load(f)\n",
    "    le = LabelEncoder()\n",
    "    le.classes_ = np.array(le_json['classes'])\n",
    "\n",
    "    # use the saved backbone\n",
    "    # 1. 从 config 获取 hidden_size\n",
    "    backbone_hidden = AutoModel.from_pretrained(path).config.hidden_size\n",
    "    # 2. 直接从 `path` 初始化模型，这将加载微调后的 backbone\n",
    "    model = CharacterStyleClassifier(path, embed_dim=backbone_hidden, proj_dim=256, num_roles=len(le.classes_))\n",
    "    # 3. 加载 head 权重\n",
    "    chk = torch.load(os.path.join(path, 'head.pt'), map_location=device)\n",
    "    model.proj.load_state_dict(chk['proj_state'])\n",
    "    model.classifier.load_state_dict(chk['classifier_state'])\n",
    "    # 4. 加载 centers\n",
    "    centers_npz = np.load(os.path.join(path, 'role_centers.npz'))\n",
    "    role_centers = {k: centers_npz[k] for k in centers_npz.files}\n",
    "    return model.to(device), tokenizer, le, role_centers\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_role_centers(model: nn.Module, dataloader: DataLoader, label_encoder: LabelEncoder, device='cpu') -> Dict[str, np.ndarray]:\n",
    "    model.eval()\n",
    "    accum: Dict[int, List[np.ndarray]] = {}\n",
    "    for batch in tqdm(dataloader, desc=\"Computing centers\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].cpu().numpy()\n",
    "        _, embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emb_np = embeddings.detach().cpu().numpy()\n",
    "        for lbl, e in zip(labels, emb_np):\n",
    "            accum.setdefault(int(lbl), []).append(e)\n",
    "    role_centers = {}\n",
    "    for lbl, vecs in accum.items():\n",
    "        avg = np.mean(np.stack(vecs, axis=0), axis=0)\n",
    "        role_name = label_encoder.inverse_transform([lbl])[0]\n",
    "        role_centers[role_name] = avg\n",
    "    return role_centers\n",
    "\n",
    "\n",
    "def get_style_score(model: nn.Module, tokenizer, text: str, role_center: np.ndarray, device='cpu', max_length=128) -> float:\n",
    "    model.eval()\n",
    "    enc = tokenizer(text, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')\n",
    "    input_ids = enc['input_ids'].to(device)\n",
    "    attention_mask = enc['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        _, emb = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        emb_np = emb.detach().cpu().numpy()[0]\n",
    "    num = float(np.dot(emb_np, role_center))\n",
    "    den = float(np.linalg.norm(emb_np) * np.linalg.norm(role_center) + 1e-9)\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05caee",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93469a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Evaluate\n",
    "import math\n",
    "from transformers import get_linear_schedule_with_warmup, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, label_encoder: LabelEncoder, device='cpu') -> Dict:\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            logits, emb = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            preds.extend(pred.tolist())\n",
    "            trues.extend(labels.tolist())\n",
    "            embeddings.extend(emb.detach().cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    macro_f1 = f1_score(trues, preds, average='macro')\n",
    "    micro_f1 = f1_score(trues, preds, average='micro')\n",
    "    report = classification_report(trues, preds, target_names=label_encoder.classes_, zero_division=0)\n",
    "    cm = confusion_matrix(trues, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'preds': preds,\n",
    "        'trues': trues,\n",
    "        'embeddings': embeddings\n",
    "    }\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    backbone_name: str = BACKBONE_PATH,\n",
    "    output_dir: str = str(OUTPUT_DIR),\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 2e-5,\n",
    "    weight_decay: float = 3e-4,\n",
    "    max_length: int = 128,\n",
    "    device: str | None = None,\n",
    "    patience: int = 3,\n",
    "    seed: int = SEED\n",
    "):\n",
    "    \"\"\"\n",
    "    仅负责训练/验证与早停：\n",
    "    - 不在训练过程中保存完整工件，避免与 save_checkpoint 重复。\n",
    "    - 在出现更优指标时，临时保存整模型 state_dict（best_model_state.pt）。\n",
    "    - 训练结束后，恢复至最佳权重，计算 centers，并返回 (model, tokenizer, le, role_centers)。\n",
    "    外部调用者随后统一调用 save_checkpoint() 进行一次性落盘。\n",
    "    \"\"\"\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df['role'].tolist() + val_df['role'].tolist())\n",
    "    num_roles = len(le.classes_)\n",
    "    print('Roles:', le.classes_)\n",
    "\n",
    "    print(\"Loading tokenizer and preparing datasets...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(backbone_name)\n",
    "\n",
    "    train_texts = train_df['text'].tolist()\n",
    "    train_labels = le.transform(train_df['role'].tolist()).tolist()  # type:ignore\n",
    "    val_texts = val_df['text'].tolist()\n",
    "    val_labels = le.transform(val_df['role'].tolist()).tolist()  # type:ignore\n",
    "\n",
    "    train_ds = TextRoleDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_ds = TextRoleDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    embed_dim = AutoConfig.from_pretrained(backbone_name).hidden_size\n",
    "    model = CharacterStyleClassifier(backbone_name, embed_dim=embed_dim, proj_dim=256, num_roles=num_roles, dropout=0.4)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=math.ceil(0.06 * total_steps), num_training_steps=total_steps)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_epoch = -1\n",
    "    bad_cnt = 0\n",
    "\n",
    "    # 临时保存最佳整模型权重（避免与最终 save_checkpoint 重复保存工件）\n",
    "    tmp_best_path = os.path.join(output_dir, 'best_model_state.pt')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch} training')\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=np.mean(losses))\n",
    "\n",
    "        # 验证\n",
    "        val_metrics = evaluate(model, val_loader, le, device=device)\n",
    "        val_f1 = val_metrics['macro_f1']\n",
    "        print(f\"Epoch {epoch} validation macro_f1: {val_f1:.4f}, acc: {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            bad_cnt = 0\n",
    "            # 仅保存一次整模型 state_dict（临时文件）\n",
    "            torch.save(model.state_dict(), tmp_best_path)\n",
    "            print(f\"Updated best model at epoch {epoch} (macro_f1={val_f1:.4f}).\")\n",
    "        else:\n",
    "            bad_cnt += 1\n",
    "            if bad_cnt >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}, best epoch {best_epoch}, best_macro_f1 {best_val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    # 恢复最佳权重\n",
    "    if os.path.exists(tmp_best_path):\n",
    "        best_state = torch.load(tmp_best_path, map_location=device)\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"Restored best model from {tmp_best_path}\")\n",
    "    else:\n",
    "        print(\"Warning: best_model_state.pt not found; using last epoch weights.\")\n",
    "\n",
    "    # 计算 centers（基于最佳权重）\n",
    "    full_train_loader = DataLoader(TextRoleDataset(train_texts, train_labels, tokenizer, max_length), batch_size=batch_size, shuffle=False)\n",
    "    role_centers = compute_role_centers(model, full_train_loader, le, device=device)\n",
    "\n",
    "    # 可选：清理临时文件\n",
    "    try:\n",
    "        if os.path.exists(tmp_best_path):\n",
    "            os.remove(tmp_best_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer, le, role_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d89bf2",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02658825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Roles: ['凉宫春日' '沐雪' '神里绫华' '胡桃' '钟离']\n",
      "Loading tokenizer and preparing datasets...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training: 100%|██████████| 143/143 [00:48<00:00,  2.94it/s, loss=0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation macro_f1: 0.8463, acc: 0.8868\n",
      "Updated best model at epoch 1 (macro_f1=0.8463).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 training: 100%|██████████| 143/143 [00:47<00:00,  3.02it/s, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation macro_f1: 0.8816, acc: 0.9132\n",
      "Updated best model at epoch 2 (macro_f1=0.8816).\n",
      "Restored best model from outputs\\style-classifier\\best_model_state.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing centers: 100%|██████████| 143/143 [00:23<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        凉宫春日       0.89      0.90      0.90       187\n",
      "          沐雪       0.98      0.96      0.97       424\n",
      "        神里绫华       0.86      0.93      0.89       272\n",
      "          胡桃       0.92      0.93      0.92       163\n",
      "          钟离       0.82      0.66      0.73        94\n",
      "\n",
      "    accuracy                           0.91      1140\n",
      "   macro avg       0.89      0.88      0.88      1140\n",
      "weighted avg       0.91      0.91      0.91      1140\n",
      "\n",
      "Saved checkpoint to outputs\\style-classifier\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LR = 2e-5\n",
    "\n",
    "model, tokenizer, le, role_centers = train(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    backbone_name=BACKBONE_PATH,\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_ds = TextRoleDataset(val_df['text'].tolist(), le.transform(val_df['role'].tolist()).tolist(), tokenizer)  # type:ignore\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "metrics = evaluate(model, val_loader, le, device=('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Validation report:\\n', metrics['report'])\n",
    "\n",
    "# Save full checkpoint\n",
    "save_checkpoint(model, tokenizer, le, role_centers, str(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e07c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample role: 凉宫春日\n",
      "Style score (cosine to center): 0.5417\n",
      "Text: 你们好!我前来征收一台电脑!...\n"
     ]
    }
   ],
   "source": [
    "# Inference demo: style score on one sample\n",
    "sample_idx = 0\n",
    "sample_text = val_df['text'].iloc[sample_idx]\n",
    "sample_role = val_df['role'].iloc[sample_idx]\n",
    "\n",
    "score = get_style_score(\n",
    "    model, tokenizer, sample_text, role_centers[sample_role],\n",
    "    device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "print(f\"Sample role: {sample_role}\\nStyle score (cosine to center): {score:.4f}\\nText: {sample_text[:120]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
